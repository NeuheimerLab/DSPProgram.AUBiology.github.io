[
  {
    "objectID": "DSPPH_SM_Responses.html",
    "href": "DSPPH_SM_Responses.html",
    "title": "Statistical modelling: Responses",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nDefine your research question and response variable (what variability are you trying to explain?)\nPresent the motivation for your research question (why is it worth explaining?)\nThe first step of the statistical modelling framework is to identify your response variable1.\nYour response variable is the observed variability you are trying to explain. As mentioned earlier, all science is explaining variability - why something you observe is changing. Your response variable is the “thing” you are trying to explain. It is sometimes called by other names such as “dependent variable” or “y variable”.\nBefore you can proceed with your hypothesis making and testing, you need to be clear about the variation you are trying to explain and how it was observed. What is making you curious?\nThese questions are called “research questions” and they identify your response variable (contrast this with your research hypothesis in an upcoming section).\nThough it is not necessary to be able to proceed with statistical modelling, it is useful at this point to stop and think about why you want to explain the variation in your response. Why is it important to explain different tree heights? Or fish abundance? Or hormone level? Being clear about what variation you are trying to explain (your response variable) and why it is important to explain that variation will make up a good portion of your introduction section to a report or paper - and help shape your discussion section as well."
  },
  {
    "objectID": "DSPPH_SM_Responses.html#footnotes",
    "href": "DSPPH_SM_Responses.html#footnotes",
    "title": "Statistical modelling: Responses",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice I write “Response(s)” in the title of this section - plural. It is possible to have multiple response variables and we will discuss this elsewhere in the handbook when we discuss multivariate data. For the focus of this handbook though, we will begin by working with one response variable↩︎"
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#motivation",
    "href": "philosophy.html#motivation",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#terminology",
    "href": "philosophy.html#terminology",
    "title": "DSP Program - Philosophy",
    "section": "Terminology",
    "text": "Terminology\nData skills: We use the term “data skills” to represent the quantitative and computing skills involved in many fields of research (including biological) and in demand from a range of employers. This includes data literacy (the collecting, management, archiving and wrangling of data), computational thinking (computational logic, problem solving, pattern identification, algorithms), analytical skills (collecting and considering information, making decisions), model building and hypothesis testing, and other quantitative skills. Other terms used in the literature, community and job market are “data science” and “digital competences”\nPortfolio: We use the term “portfolio” to represent the collection of data skills the student will acquire throughout the degree. Later modules in the program are tied to elective courses so that students will have portfolios that vary based on their experience. In all cases, the students will create an explicit Data Skills Portfolio (DSP) to develop their awareness and confidence in their skills, help clarify the applicability of their skills across disciplines, and more easily communicate their skills to future employers.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#core-competencies",
    "href": "philosophy.html#core-competencies",
    "title": "DSP Program - Philosophy",
    "section": "Core competencies",
    "text": "Core competencies\nSkills are identified through the following core competencies\n\ncritical thinking\ngeneral programming\ndata management\ndata visualization\nstatistical modelling\nproject management\nskills marketing",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#guiding-principles-of-the-dsp-program",
    "href": "philosophy.html#guiding-principles-of-the-dsp-program",
    "title": "DSP Program - Philosophy",
    "section": "Guiding principles of the DSP Program",
    "text": "Guiding principles of the DSP Program\n\nRelevance\nThe data skills taught will be relevant and state-of-the-art with respect to the current needs of both the biological research community and the greater job market. Skills will be taught in the context of current biological research.\n\n\nBest Practices\nCourse content and instruction will follow current best-practices for teaching data skills and for teaching to a diversity of students (diversity of backgrounds, learning styles). To this end, a common teaching strategy will be developed.\n\n\nCohesion\nCohesion throughout the DSP Program is necessary for student learning and mastering of skills. Program cohesion will be developed through repetition, consistency and clarity: Students will have a chance to apply skills repeatedly and regularly with DSP courses and modules positioned in as many semesters as possible. Skills will be taught with consistency with instructors using a common framework, syntax and terminology across DSP courses and modules. Learned skills will be made clear to the students as they will be explictly trained to communicate why and how they are applying skills to accomplish tasks through the development of their Data Skills Portfolio.\n\n\nResilience\nProgram development will support resilience of both the student and the program as a whole.\nStudent resilience will be nurtured by:\n\nencouraging an understanding of the “how” and “why” behind the data skills they are learning so they are aware of the general applicability of their skills.\nrepeated exposure to the training throughout their career so they have a number of opportunities to practice skills,\ndeveloping awareness of the skills they are learning through the building of their Data Skills Portfolio that follows them throughout their degree,\nfeedback opportunities where students are able to identify areas they find challenging so that swift interventions are made, and no student is left behind, and\naccessible tools: Where possible, open-source programs and languages will be taught to allow students uninterrupted access to tools after they leave their education.\n\nProgram resilience will be nurtured by:\n\nthe DSP Program being a shared goal & responsibility across sections: The program will be grounded by input from all Sections in the Department of Biology. Teaching responsibilities will shared by all Sections. The current make-up of the DSP Program Taskforce is available here.\nbuilding in redundancy in teaching responsibilities: Courses and modules will be team taught as much as possible to allow for consistency in the program in the face of staff availability changes, and\ndevelopment of maintainable online resources: Online resources (including this handbook) will be structured to maintanence as minimal as possible.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#program-assessment",
    "href": "philosophy.html#program-assessment",
    "title": "DSP Program - Philosophy",
    "section": "Program assessment",
    "text": "Program assessment\nPlans for the assessment of the program for scope and effectiveness are in development. These will include:\n\nCourse evaluations to assess new courses and modules\nMidterm and final evaluation of the DSP program with students\nFeedback from employers 1-2 years after the first cohort graduates after the DSP Program.\nEmployment statistics of our graduates including employment rates and areas of employment.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "Structure of the DSP Program",
    "section": "",
    "text": "The DSP Program consists of a series of modules intended to allow students to gain, practice, apply and communicate their data-skills.\nSemester:\n\n\n\n\n\n\n\n\n\n\n\n\nSemester\n2\n3\n4\n5 (in 2024)\n6 (in 2025)\nKandidat (in 2025)\n\n\n\n\n\nIntro Module:\nBFTP (Marshall)\nModule:\nMikrobiologi for biologer (Koren, Marzocchi)\nCourse:\nProgramming and Statistics for Biologists\nModule:\nAkvatisk Biologi (Neuheimer)\nModule:\nPlanters Økofysiologi (Eller, Sorrell, Neuheimer)\nExit Module:\nCareer (Taskforce)\n\n\n\n\nothers in development\n\nothers in development\nExit Module:\nResearch (Taskforce)\nothers in development\n\n\n\n\n\n\n\nothers in development",
    "crumbs": [
      "What is the DSP Program?",
      "Structure of the DSP Program"
    ]
  },
  {
    "objectID": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "href": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "title": "Frequently Asked Questions",
    "section": "How do I send my comments/questions feedback?",
    "text": "How do I send my comments/questions feedback?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "href": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "title": "Frequently Asked Questions",
    "section": "I want to include a DSP module in my course. Who should I contact?",
    "text": "I want to include a DSP module in my course. Who should I contact?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "handbookSMIntro.html",
    "href": "handbookSMIntro.html",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistics",
    "href": "handbookSMIntro.html#why-statistics",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistical-modelling",
    "href": "handbookSMIntro.html#why-statistical-modelling",
    "title": "Statistical Modelling Handbook",
    "section": "Why statistical modelling?",
    "text": "Why statistical modelling?\nYou can quantify how much variability you can explain with your research hypothesis through statistical modelling. Your statistical model represents your research hypothesis in a mathematical structure. This mathematical structure can be tested against your data to determine what evidence there is for your hypothesis:\n\ncan I explain the variability that I am seeing? (Can I reject my hypothesis?)\ngiven my hypothesis, how much variation in the observations can I explain?\ngiven my hypothesis, what would I expect (predict) to observe under different times or locations?\n\nYour job then is to explain observation variability in time and space by creating a “model” of what (you think) is going on - hence statistical modelling.\nIt is important to remember that any model is only an approximation of what is going on in the real world. As many have said before\n\nAll models are wrong but some are useful.\n\nWe will discuss how you can build useful models that you can use to test your hypotheses.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "href": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "title": "Statistical Modelling Handbook",
    "section": "Introducing a statistical modelling framework",
    "text": "Introducing a statistical modelling framework\n\nThe focus of this section of the handbook is statistics that can be applied to your work as a biologist. For that reason, the motivation for what we are going to do together comes directly from your biological research hypotheses. As a biologist, you have a research hypothesis that you want to test. The method presented in this handbook will help you test it. You will learn how to move from biological theory to hypothesis to the statistical modelling process you will use to test your hypothesis.\nYou will learn this process of statistical modelling by walking through a “Statistical Modelling Framework”. This is a set of steps that you can use to go from your research hypothesis to designing a model, testing your hypothesis and communicating the results.\nThis handbook will walk through the parts of this framework one by one. During DSP modules throughout your degree, you will do the same in class while you practice applying the framework to case studies. In this way, you should see how the framework is generally applicable but also flexible. And after you leave the course you will be able to apply the framework to help you in your statistical analysis in other courses, your thesis, and your future career - in every case, your process will take its starting point and focus from the hypothesis that is motivating you.\nAs we will discuss later in the handbook, working through this framework will also guide you in creating the “guts” of a paper or report. As well as clarifying how and why you made your analysis choices, it will guide you in describing your motivation behind your research question (why is it worthwhile to spend time explaining this variation?) as well as the mechanisms behind your research hypothesis (why do I think X is responsible for the variation I’m trying to explain?). Once through, you’ll have a solid draft that can be the basis of a report, thesis chapter or scientific paper. We will talk about how this works in section.\nA note about our Statistical Modelling Framework: The steps in the image to the right as a linear process but it is not actually a linear process. As you will see in the examples, sometimes you will need to make best guesses5 as to what model might be useful and only after confronting the model with your data will you know if your guesses were reasonable (and useful!) - i.e. the model is a valid one that you can use to test your hypothesis. We will talk about how to find a useful model, how to choose when there are multiple options, and how to communicate your choices.\n\nSteps in the statistical modelling framework:\n\n\nResponse(s)\nHere you will define your research question by identifying your response variable(s).\n\nWhat variability are you trying to explain?\nAnd why is it worth explaining? (your motivation)\n\nNote: we’ll begin by discussing how to model hypotheses with just one response variable before discussing multiple response variable(s).\n\n\nPredictor(s)\nHere you will choose your predictor variables.\n\nwhat could explain the variability in your response?\nwhat are the possible mechanisms behind your argument?\n\n\n\nHypothesis\nHere you will see how your response and predictor variables come together to define your research hypothesis. And we will discuss how to write this hypothesis to begin building your statistical modelling.\n\n\nStarting model\nHere you will choose and fit the starting model that will be used to test your hypothesis. You will do this by choosing and communicating two key assumptions that will help you pick a useful modelling starting point. Then you will fit your model to your data (i.e. confronting your model with your data).\n\n\nModel validation\nHere you will investigate whether your model will be a useful one to test your hypothesis. Your steps here will include considering if you have correlated predictors or problems with observation dependence.\nYou will also considering if your starting model assumptions were realistic. After this step, you will have a model that you can confidently use to test your hypothesis.\n\n\nHypothesis testing\nHere you will test your hypothesis by assessing the evidence supporting your model. We will discuss a number of different methods to do this, but will focus on the model selection method as a robust way to evaluate what your model is telling you about your hypothesis.\n\n\nReporting\nHere you will report the results of your hypothesis testing.\nYou will report:\n\nyour best-specified model identified in the hypothesis testing\nthe effects (patterns) described by your model (including visualizing your model effects)\nhow well your model explains variability in your response.\n\n\n\nPredicting\nHere you will use your model to make predictions of your response under different conditions (while considering prediction limits).\n\n\n\nWhere we will begin: generalized linear models (GLMs)\nTo begin with, we will be discussing generalized linear models (GLMs) as models that can be useful to test many different hypotheses. Also, understanding how GLMs can be used to test your hypothesis will help you understand other, including more advanced, statistical models (Pongpipat_et_al_PracticalExtensionStatisticsForPsychology).\nRemember: the model you choose is just an approximation of the real world. This means that often times alternative models would be possible (models like t-tests, ANOVAs, ANCOVAs, etc.6). In fact, you may be collaborating with someone who wants to model your hypothesis with a different method. In this handbook, we compare GLMs to alternative model types here. And remember: the steps in the statistical modelling framework are generally applicable. Regardless of the method you apply, you need to ground your choices in good biological and statistical theory.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#the-examples",
    "href": "handbookSMIntro.html#the-examples",
    "title": "Statistical Modelling Handbook",
    "section": "The examples",
    "text": "The examples\nHere we’ve gathered examples following our statistical modelling framework structure. You can request/contribute new examples here",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "href": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "title": "Statistical Modelling Handbook",
    "section": "From statistical modelling to scientific report writing",
    "text": "From statistical modelling to scientific report writing\nHere you can see how you can use the steps in the Statistical Modelling Framework to outline your communication of your hypothesis testing in reports and paper.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#where-to-from-here",
    "href": "handbookSMIntro.html#where-to-from-here",
    "title": "Statistical Modelling Handbook",
    "section": "Where to from here?",
    "text": "Where to from here?\nTBA: - boosted regression trees - Bayesian negative bionomial regression mixed models - nested design in mixed models - random forest vs. GAMs - meta-analysis - complex models - classification (categorical response) and regression (continous response) trees - boosted regression trees - boosted regression trees iteratively fit models gradually increasing emphasis on observations that were initially poorly fit - ADMB and TMB\n\nmultivariate statistics\nexploratoryordination/clustering\ncross validation\n\n[[Principle Component Analysis]] linearly transforms multivariate data into a new coordinate system where the majority of the variation in the data is captured with fewer dimensions than the initial data - Li et al. 2023\n[[Principle Component Analysis|PCA]] constructs a “map” of the samples where samples that are more similar are closer together\n\nAcknowledgements\nTBA",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#footnotes",
    "href": "handbookSMIntro.html#footnotes",
    "title": "Statistical Modelling Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhere, we can also use the term “research hypothesis” instead of explanation and you’ll see we quickly switch to using this term↩︎\nMath skills have often been underemphasized in many Biology educations. This has not been helpful or necessary. Biologists need math and are very capable at applying math to solve problems, but the math needs to be useful; that is, math that you can apply to your own needs as you research biology. The DSP Program aims at providing statistical modelling tools that will be useful to you as a biologist. It is a happy coincidence that the same skills can be applied to a lot of other situations as well. The programming and statistics skills you are learning here will be useful to you in the future - in your future courses, thesis-writing and a wide range of careers.↩︎\nmore on causal vs. correlative explanations coming soon↩︎\nuseful predictions will always include uncertainty↩︎\nand these will be educated guesses!↩︎\nDon’t worry if these terms don’t mean anything to you yet - more to come!↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html",
    "href": "handbookDAIntro.html",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.\n\n\n\n\n\nA bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.\n\n\n\nHere we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.\n\n\n\n\nHere we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-tools",
    "href": "handbookDAIntro.html#a-note-on-tools",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "href": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "A bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#the-basics",
    "href": "handbookDAIntro.html#the-basics",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#so-you-want-to",
    "href": "handbookDAIntro.html#so-you-want-to",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.\n\n\n\nThe Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions\n\n\n\n\n\n\n\nModules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#purpose",
    "href": "intro.html#purpose",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#interested-in-contributing-to-the-dsp-program",
    "href": "intro.html#interested-in-contributing-to-the-dsp-program",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "Modules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "DSPPH_DA_Merge.html",
    "href": "DSPPH_DA_Merge.html",
    "title": "So you want to: merge your data sets",
    "section": "",
    "text": "Often times we are interested in exploring connections among variables from different sources. Merging your data files can be a way of collecting all the variables of interest so you can explore research questions and hypotheses about the data.\nIn addition, merging can be used to label your observations. We go through examples below."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "href": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "title": "So you want to: merge your data sets",
    "section": "in the base package using merge()",
    "text": "in the base package using merge()\n\nmDat&lt;-merge(Dat1, # data frame to merge\n             Dat2, # other data frame to merge\n             by = c(\"ID\", \"Day\")) # merge by variable(s)\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n\n\nNote that you don’t need to have the same number of observations (rows) in your two data frames to merge. Merging can be a great way of labelling your data. Here’s an example:\nConsider a data frame with strain information for each of your organism IDs:\n\n## ID: organism ID\n## Strain: strain of organism\n\nstr(Dat3)\n\n'data.frame':   20 obs. of  2 variables:\n $ ID    : chr  \"id24\" \"id30\" \"id33\" \"id26\" ...\n $ Strain: chr  \"A\" \"C\" \"B\" \"C\" ...\n\n\nNote that Dat1 and Dat2 each contained 40 observations - one observation for each of 20 IDs made on each of 2 days.\nIn contrast Dat3 only has 20 observations - information about the strain for each of 20 IDs.\nBy using merge(), we can add the strain information to mDat:\n\nallDat &lt;- merge(mDat, # one data frame\n                Dat3, # the other data frame\n                by = \"ID\") # variables to merge by\n\nstr(allDat)\n\n'data.frame':   40 obs. of  5 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n $ Strain: chr  \"C\" \"C\" \"A\" \"A\" ...\n\n\nNow we have our observations labelled by the strain information!\nSome things to note:\n\nif you leave out the by = function totally, R will look for column names that are similar between the two data frames and use that for the merge.\nyou can designate that the “merge by” variables have different column names in the two data frames. This is done with the by.x = and by.y = arguments. Check ?merge for more.\nyou can control what happens to unmatched columns (e.g. if an ID appeared in only one of the two data frames). This is done with the all =, all.x =, and all.y = arguments. Check ?merge for more."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "href": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "title": "So you want to: merge your data sets",
    "section": "in the dplyr package using full_join()",
    "text": "in the dplyr package using full_join()\nThe dplyr package includes the full_join() function as another way to merge your data frames\n\nlibrary(dplyr) # load dplyr package\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmDat&lt;-full_join(Dat1, # data frame to merge\n                Dat2, # other data frame to merge\n                by = join_by(ID, Day)) # merge by column\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id29\" \"id30\" \"id22\" \"id31\" ...\n $ Day   : num  1 2 1 2 1 1 2 1 2 2 ...\n $ Length: num  150 164 110 134 155 125 144 144 139 167 ...\n $ Temp  : num  7.4 7.3 6.9 7.5 5.6 7.8 7.8 6.3 7.9 7.1 ...\n\n\nSome things to note:\n\nThe full_join() function keeps all observations appearing in either data frame.\nThe left_join() function keeps all observations in the first data frame (Dat1) but you will lose any unmatched observations in the second data frame (Dat2).\nThe right_join() function keeps all observations in the second data frame (Dat2) but you will lose any unmatched observations in the first data frame (Dat1)"
  },
  {
    "objectID": "handbookIntro.html",
    "href": "handbookIntro.html",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.\n\n\n\nThere are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.\n\n\n\nThe ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is.",
    "href": "handbookIntro.html#what-this-handbook-is.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is-not.",
    "href": "handbookIntro.html#what-this-handbook-is-not.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "There are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#dsp-program-tools",
    "href": "handbookIntro.html#dsp-program-tools",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#footnotes",
    "href": "handbookIntro.html#footnotes",
    "title": "Introduction to the Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComputational thinking includes skills in decomposition (breaking down tasks into small steps), pattern recognition (observing patterns in tasks and data), abstraction (identifying and extracting relevant information, ignoring or removing unnecessary information), algorithms (creating an ordered set of instructions for solving a problem), modelling and simulation (statistical modelling for hypothesis testing, imitating processes and problems), and evaluation (determining the effectiveness of a solution, generalizing to apply the solution to a new problem) - adapted from digitalcareers.csiro.au.↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Contacts and Feedback for the DSP Program",
    "section": "",
    "text": "The DSP Program is a joint venture by colleagues across AU’s Department of Biology.\nProgram development is led by the DSP Program Taskforce:\n\nAnna B. Neuheimer (Aquatic Biology, Taskforce head)\nRobert Buitenwerf (Ecoinformatics and Biodiversity)\nAlejandro Ordonez Gloria (Ecoinformatics and Biodiversity)\nTove Hedegaard Jørgensen (Genetics, Ecology and Evolution & Centre for Educational Development)\nIan Marshall (Microbiology)\nBirgit Olesen (Aquatic Biology & Arctic Research Centre)\nPeter Teglberg Madsen (Zoophysiology)\nJesper Givskov Sørensen (Genetics, Ecology and Evolution)\n\n\n\nSend your questions and feedback to the DSP Program Taskforce\nWe welcome your questions and feedback. Please submit them using this form or send an email to abneuheimer@bio.au.dk."
  },
  {
    "objectID": "handbookDCIntro.html",
    "href": "handbookDCIntro.html",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Data curation is the collection and on-going management of data.\nGood data curation helps maintain data quality, and allows you and others to discover, access, and re-use your data. Data curation skills then are essential for ethical, transparent and reproducible science, and are an important part of your contribution to the field of biology.\n\n\nDepending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.\n\n\n\nThe individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns (R can handle that)[DSPH_DA_DatesTimes.qmd], but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)\n\n\n\n\nFor more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "href": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Depending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#data-collection",
    "href": "handbookDCIntro.html#data-collection",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "The individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns (R can handle that)[DSPH_DA_DatesTimes.qmd], but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#where-to-go-from-here",
    "href": "handbookDCIntro.html#where-to-go-from-here",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "For more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#footnotes",
    "href": "handbookDCIntro.html#footnotes",
    "title": "Data Collection & Curation Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthough you may have projects where you are working with databases↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "Welcome to the Data Skills Portfolio Program!\nThe Data Skills Portfolio (DSP) Program is based at Aarhus University’s Department of Biology.\nThe DSP Program is designed to help our Biology students develop robust and useful quantitative skills for biological research and many fields beyond.\nHere you will find a description of the program along with DSP handbook materials.\nExplore and enjoy!\nAnd remember: your feedback is welcome!"
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html",
    "href": "DSPPH_DA_DatesTimes.html",
    "title": "So you want to: work with dates and times",
    "section": "",
    "text": "You will often have a time component to your biological research and research hypotheses - i.e. you might want to explain variability over time.. As mentioned in the Data Collection and Curation section, it is a good idea to record the time of your observation as separate columns of year, month, day, etc. But you will also need to work with data collected by others where date (and sometimes time) information is together one column (variable)."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "href": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "title": "So you want to: work with dates and times",
    "section": "In the base package:",
    "text": "In the base package:\nIn the base package which is installed along with R1, you can use the as.Date() function to format your data as a date:\nConsider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nmyDat$Time &lt;- as.Date(x = myDat$Time, # the date and time column\n                      format = \"%Y/%m/%d %H:%M\") # describing the format of the date and time column\n\nhead(myDat) # examine the first few rows\n\n        Time Value\n1 2024-10-03    36\n2 2020-06-09    35\n3 2020-10-07    57\n4 2021-10-09    43\n5 2020-06-04    44\n6 2021-12-02    36\n\n\nYou can learn more about the formatting syntax with ?strptime. Note that the code above replaces the myDat$Time column with the new, formatted date and time information.\n\nYou can extract parts of the date and time column with functions like months() for months, years() for years, etc.\n\nFor example:\n\nmyDat$Months &lt;- months(myDat$Time) # extract only the months\n\nstr(myDat) # structure of the data frame\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : Date, format: \"2024-10-03\" \"2020-06-09\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: chr  \"October\" \"June\" \"October\" \"October\" ..."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "href": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "title": "So you want to: work with dates and times",
    "section": "Using the lubridate package:",
    "text": "Using the lubridate package:\nThe lubridate package was created to make working with dates and times easier. There are still two steps to the process. You can repeat the steps above but now with the lubridate package.\nAgain, consider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nlibrary(lubridate) # load the lubridate package\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nmyDat$Time &lt;- ymd_hm(myDat$Time) # the date and time column\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  2 variables:\n $ Time : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value: int  36 35 57 43 44 36 47 39 38 50 ...\n\n\n\nYou can extract parts of the date and time column with functions like month() for months, year() for years, etc. Notice that the function names are not plural with the lubridate package.2\n\nFor example:\n\nmyDat$Months &lt;- month(myDat$Time) # extract only the months\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: num  10 6 10 10 6 12 3 4 11 11 ...\n\n\nMuch more is available in the lubridate packages, including determining durations and dealing with time-zones. Check the lubridate package “cheat sheet” for more information."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#footnotes",
    "href": "DSPPH_DA_DatesTimes.html#footnotes",
    "title": "So you want to: work with dates and times",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nno need to load a separate package↩︎\nBoth months() and month() will work with lubridate, but months() is also used to denote a duration of a month. See the cheat sheet for more.↩︎"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html",
    "href": "DSPPH_DA_MissingValues.html",
    "title": "So you want to: deal with missing values",
    "section": "",
    "text": "Missing values are important and interesting, and they can affect how functions can be used on your data."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Apply functions to data containing missing values",
    "text": "Apply functions to data containing missing values\nWe can apply functions to objects or their components when missing values are present by letting R know what we want to do with the missing values. For example, getting the overall mean of the chlorophyll column without telling R how to handle missing values:\n\nmean(ChlData$Chl)\n\n[1] NA\n\n\nvs. telling R to remove them with the na.rm = ... argument:\n\nmean(ChlData$Chl, na.rm = TRUE)\n\n[1] 32.42375\n\n\nNote that the data frame itself remains unchanged, but R ignores the NAs when calculating the mean. We can find out more about how a particular function handles missing values by looking at the function’s help file (e.g. ?mean)."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "href": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Locating missing values",
    "text": "Locating missing values\nWe can also find missing values using the is.na() function:\n\nis.na(ChlData$Chl)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nand identify locations of missing values with:\n\nwhich(is.na(ChlData$Chl) == TRUE)\n\n[1] 5 8 9"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Removing missing values",
    "text": "Removing missing values\nFinally, we can remove all rows that are incomplete (i.e. containing any missing values) with the na.omit() function:\n\nhead(ChlData) # Original data frame\n\n  Station Year Month   Chl\n1     HL2 2007     2 16.07\n2     S27 2005    10 31.31\n3     HL2 2002    NA 40.38\n4     HL2 2001     2 32.00\n5     HL2 2001    10    NA\n6     S27 2007    10 29.71\n\nChlData &lt;- na.omit(ChlData) # Remove the NAs\n\nhead(ChlData) # Data frame without NAs\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nNote that above I reassign the output of the na.omit() function back to the name ChlData. This replaces the original data frame with the new data frame without missing values. I could also save it as a new object (with a new name) so the original is not overwritten.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTake a look at the numbers that print out to the left of the data frame:\n\nhead(ChlData)\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nThese are row names that were assigned when the data were read in. We can ignore row names but I wanted to to explain them as they can be distracting when one starts manipulating data frames. Unless we specify otherwise, rows are named by their original position when the data are read in to R, e.g. initially row #3 was assigned the name “3”, and row #4 was assigned the name “4”, etc. Since we’ve removed some rows with na.omit() above, the row names now skip from e.g. 2 to 4 (row 3 has been removed), but note that the 3rd row in the data frame can still be accessed with:\n\nChlData[3,]\n\n  Station Year Month Chl\n4     HL2 2001     2  32\n\n\nYou can choose your own row names with the rownames() function (or column names with the colnames() function) as well as with arguments in e.g. read.csv()."
  }
]
[
  {
    "objectID": "DSPPH_SM_Responses.html",
    "href": "DSPPH_SM_Responses.html",
    "title": "Statistical modelling: Responses",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nDefine your research question and response variable (what variability are you trying to explain?)\nPresent the motivation for your research question (why is it worth explaining?)\nThe first step of the statistical modelling framework is to identify your response variable1.\nYour response variable is the observed variability you are trying to explain. As mentioned earlier, all science is explaining variability - why something you observe is changing. Your response variable is the “thing” you are trying to explain. It is sometimes called by other names such as “dependent variable” or “y variable”.\nBefore you can proceed with your hypothesis making and testing, you need to be clear about the variation you are trying to explain and how it was observed. What is making you curious?\nThese questions are called “research questions” and they identify your response variable (contrast this with your research hypothesis in an upcoming section).\nThough it is not necessary to be able to proceed with statistical modelling, it is useful at this point to stop and think about why you want to explain the variation in your response. Why is it important to explain different tree heights? Or fish abundance? Or hormone level? Being clear about what variation you are trying to explain (your response variable) and why it is important to explain that variation will make up a good portion of your introduction section to a report or paper - and help shape your discussion section as well."
  },
  {
    "objectID": "DSPPH_SM_Responses.html#footnotes",
    "href": "DSPPH_SM_Responses.html#footnotes",
    "title": "Statistical modelling: Responses",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice I write “Response(s)” in the title of this section - plural. It is possible to have multiple response variables and we will discuss this elsewhere in the handbook when we discuss multivariate data. For the focus of this handbook though, we will begin by working with one response variable↩︎"
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#motivation",
    "href": "philosophy.html#motivation",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#terminology",
    "href": "philosophy.html#terminology",
    "title": "DSP Program - Philosophy",
    "section": "Terminology",
    "text": "Terminology\nData skills: We use the term “data skills” to represent the quantitative and computing skills involved in many fields of research (including biological) and in demand from a range of employers. This includes data literacy (the collecting, management, archiving and wrangling of data), computational thinking (computational logic, problem solving, pattern identification, algorithms), analytical skills (collecting and considering information, making decisions), model building and hypothesis testing, and other quantitative skills. Other terms used in the literature, community and job market are “data science” and “digital competences”\nPortfolio: We use the term “portfolio” to represent the collection of data skills the student will acquire throughout the degree. Later modules in the program are tied to elective courses so that students will have portfolios that vary based on their experience. In all cases, the students will create an explicit Data Skills Portfolio (DSP) to develop their awareness and confidence in their skills, help clarify the applicability of their skills across disciplines, and more easily communicate their skills to future employers.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#core-competencies",
    "href": "philosophy.html#core-competencies",
    "title": "DSP Program - Philosophy",
    "section": "Core competencies",
    "text": "Core competencies\nSkills are identified through the following core competencies\n\ncritical thinking\ngeneral programming\ndata management\ndata visualization\nstatistical modelling\nproject management\nskills marketing",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#guiding-principles-of-the-dsp-program",
    "href": "philosophy.html#guiding-principles-of-the-dsp-program",
    "title": "DSP Program - Philosophy",
    "section": "Guiding principles of the DSP Program",
    "text": "Guiding principles of the DSP Program\n\nRelevance\nThe data skills taught will be relevant and state-of-the-art with respect to the current needs of both the biological research community and the greater job market. Skills will be taught in the context of current biological research.\n\n\nBest Practices\nCourse content and instruction will follow current best-practices for teaching data skills and for teaching to a diversity of students (diversity of backgrounds, learning styles). To this end, a common teaching strategy will be developed.\n\n\nCohesion\nCohesion throughout the DSP Program is necessary for student learning and mastering of skills. Program cohesion will be developed through repetition, consistency and clarity: Students will have a chance to apply skills repeatedly and regularly with DSP courses and modules positioned in as many semesters as possible. Skills will be taught with consistency with instructors using a common framework, syntax and terminology across DSP courses and modules. Learned skills will be made clear to the students as they will be explictly trained to communicate why and how they are applying skills to accomplish tasks through the development of their Data Skills Portfolio.\n\n\nResilience\nProgram development will support resilience of both the student and the program as a whole.\nStudent resilience will be nurtured by:\n\nencouraging an understanding of the “how” and “why” behind the data skills they are learning so they are aware of the general applicability of their skills.\nrepeated exposure to the training throughout their career so they have a number of opportunities to practice skills,\ndeveloping awareness of the skills they are learning through the building of their Data Skills Portfolio that follows them throughout their degree,\nfeedback opportunities where students are able to identify areas they find challenging so that swift interventions are made, and no student is left behind, and\naccessible tools: Where possible, open-source programs and languages will be taught to allow students uninterrupted access to tools after they leave their education.\n\nProgram resilience will be nurtured by:\n\nthe DSP Program being a shared goal & responsibility across sections: The program will be grounded by input from all Sections in the Department of Biology. Teaching responsibilities will shared by all Sections. The current make-up of the DSP Program Taskforce is available here.\nbuilding in redundancy in teaching responsibilities: Courses and modules will be team taught as much as possible to allow for consistency in the program in the face of staff availability changes, and\ndevelopment of maintainable online resources: Online resources (including this handbook) will be structured to maintanence as minimal as possible.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#program-assessment",
    "href": "philosophy.html#program-assessment",
    "title": "DSP Program - Philosophy",
    "section": "Program assessment",
    "text": "Program assessment\nPlans for the assessment of the program for scope and effectiveness are in development. These will include:\n\nCourse evaluations to assess new courses and modules\nMidterm and final evaluation of the DSP program with students\nFeedback from employers 1-2 years after the first cohort graduates after the DSP Program.\nEmployment statistics of our graduates including employment rates and areas of employment.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "Structure of the DSP Program",
    "section": "",
    "text": "The DSP Program consists of a series of modules intended to allow students to gain, practice, apply and communicate their data-skills.\nSemester:\n\n\n\n\n\n\n\n\n\n\n\n\nSemester\n2\n3\n4\n5 (in 2024)\n6 (in 2025)\nKandidat (in 2025)\n\n\n\n\n\nIntro Module:\nBFTP (Marshall)\nModule:\nMikrobiologi for biologer (Koren, Marzocchi)\nCourse:\nProgramming and Statistics for Biologists\nModule:\nAkvatisk Biologi (Neuheimer)\nModule:\nPlanters Økofysiologi (Eller, Sorrell, Neuheimer)\nExit Module:\nCareer (Taskforce)\n\n\n\n\nothers in development\n\nothers in development\nExit Module:\nResearch (Taskforce)\nothers in development\n\n\n\n\n\n\n\nothers in development",
    "crumbs": [
      "What is the DSP Program?",
      "Structure of the DSP Program"
    ]
  },
  {
    "objectID": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "href": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "title": "Frequently Asked Questions",
    "section": "How do I send my comments/questions feedback?",
    "text": "How do I send my comments/questions feedback?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "href": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "title": "Frequently Asked Questions",
    "section": "I want to include a DSP module in my course. Who should I contact?",
    "text": "I want to include a DSP module in my course. Who should I contact?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html",
    "href": "DSPPH_SM_StartingModel.html",
    "title": "Statistical Modelling: Starting Model",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nlearn why you need a starting model to test your hypothesis\nlearn what model fitting means\nchoose and fit a starting model to start testing your hypothesis\ntake a first look at your fitted starting model"
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-whyNeed",
    "href": "DSPPH_SM_StartingModel.html#sec-whyNeed",
    "title": "Statistical Modelling: Starting Model",
    "section": "Why do you need a starting model?",
    "text": "Why do you need a starting model?\n\nImagine your research hypothesis is:\n\nResponse ~ Predictor + 1\n\nwhere you are hypothesizing that\n\nvariability in Response is explained by variability in Predictor.\n\nThis can be tested by determining the evidence for an effect of Predictor on your Response. An effect means that we get a change in Response when we observe a change in Predictor.\nIn fitting your model, you will be estimating the effect of Predictor on Response - the magnitude and direction of the effect, as well as an estimate of the uncertainty (error) in the effect.\n\n\n\n\n\n\nEffects are coefficients\n\n\n\n\n\nWhen we say that a predictor has “an effect” on a response, we are saying that a change in the predictor leads to a change in the response.\nThis change in the response that comes from a unit change in the predictor is estimated as a coefficient.\nThe coefficient of a continuous predictor is the slope. The slope describes the change in response that you get from a unit change in the predictor. For example, if your hypothesis is Growth ~ Temperature + 1, (and Temperature is continuous), the slope (coefficient) for Temperature will tell you the change in Growth you expect for a 1˚C change in Temperature.\nThe coefficient of a categorical predictor tells you how much the response will change when the categorical predictor changes from one category (level) to another. For example, if your hypothesis is Growth ~ Species + 1, (and Species is categorical with “Species A” and “Species B”), the coefficient for Species will tell you the change in Growth you expect when you change from one Species to another (e.g. “Species A” to “Species B”).\n\n\n\nYour starting model will let you estimate this effect (coefficient) of your predictor on your response. Your starting model will also let you estimate the error (uncertainty) around this effect (coefficient).\nOnce you have these estimates, you can test your hypothesis to see if the effect of the predictor on your response is meaningful (i.e. is the coefficient significantly different than zero?1)."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-WhatMeanFitModel",
    "href": "DSPPH_SM_StartingModel.html#sec-WhatMeanFitModel",
    "title": "Statistical Modelling: Starting Model",
    "section": "What does it mean to “fit a model”?",
    "text": "What does it mean to “fit a model”?\nLet’s take a step back and start by looking at the structure of a statistical model.\nA statistical model is a model of your hypothesis where the coefficients of the model (e.g. slope, intercept) are estimated from your data.\nA statistical model is a model that include both a deterministic part and a stochastic part.\n\nThe deterministic part represents your research hypothesis in math form - this describes how the predictors and response are related.\nThe stochastic part represents the error in your model. This includes error due to all the other possible factors or predictors that you have not been included in your hypothesis (called “process error”) as well as any error made when you made your observations (called “measurement error”).\n\n\n\n\n\n\n\nMore on statistical model form\n\n\n\n\n\nWe can represent a general statistical model as\n\\(E(Y_i) = Function(Pred1_i,  Pred2_i, ...) \\tag{Deterministic part}\\)\n\\(Resp_i \\sim Distribution(E(Y_i)) \\tag{Stochastic part}\\)\nwhere \\(Resp_i\\) is your response, \\(Pred1_i, Pred2_i\\) are your predictors for observation \\(i\\), and \\(E(Y_i)\\) is the expected value of your response.\nHere is an example for a case where variability in your response is explained by a continuous predictor:\n\nNote here that i) the shape assumption (deterministic part) is that the effect of the predictor on the response is linear, and ii) the error distribution assumption (stochastic part) is that the error is normal, meaning that your observations should be assumed to be normally distributed around the fitted value (\\(\\mu_i\\)).\n\n\n\nSo to choose (and eventually) fit your starting model, you need to choose both deterministic and stochastic assumptions. Happily, how we make our choices lies back in the biological world."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#choosing-your-starting-model",
    "href": "DSPPH_SM_StartingModel.html#choosing-your-starting-model",
    "title": "Statistical Modelling: Starting Model",
    "section": "Choosing your starting model",
    "text": "Choosing your starting model\nIt is important to note that there is more than one model you could use to test your hypothesis. This is because each model is a simplification and approximation of the real world, and there are a number of mathematical ways one can simplify and approximate the processes involved in your research hypothesis.\nIn this Handbook, you will learn about some very useful models, and how to choose among them. We will also discuss alternatives - this will give you other options, but will also help you communicate to other researchers that may choose different methods for their hypothesis testing.\nDespite the fact that there is more than one valid starting model, not all models are useful starting models. Here we will focus on finding a useful starting model.\nA useful starting model is one that reflects the mechanistic2 understanding underlying your research hypothesis and one that reflects the nature of your data (observations).\nNote: you will first be able to assess if your starting model is a useful one AFTER you have fit the model to the data3. At this stage you just need to pick an intelligent starting point - but what does that mean? And how do you do it?\n\n1. Choosing your error distribution assumption:\nTo choose your starting model, start by choosing the stochastic part of your model by choosing an error distribution assumption.\nThis describes how the data should be assumed to be distributed around your model fit (e.g. how to model the scatter of the data around the line in the figure above). Note I use the word “assumption” here - we are picking an existing mathematical form (data distribution4) that can approximate the behaviour of our observations. This is an assumption that we will test in the Validate section to come.\nHow do you choose?\n\nThink about theory: Note that the error in your statistical model (scatter in the plot above) is error around the response variable (i.e. on the y-axis). The key to choosing an error distribution assumption is then to look at your response variable.\n\n\nCan your response be a decimal (continuous) and positive or negative? Choose a normal error distribution assumption.\n\nCan your response be a decimal (continuous) but only positive? Start by using a Gamma error distribution assumption.\n\nCan your response only be a positive integer? Try a poisson error distribution assumption.\n\nCan your response only be one of two values? Start with a binomial error distribution assumption.\n\nPlot your data: Plotting your response variable can also help you determine the data distribution that would be the best starting point for your error distribution assumption.\n\n\n2. Choosing your shape assumption:\nThe next step in choosing your starting model is to choose your shape assumption. Your shape assumption specifies how your predictor(s) and your response are related to one another. This represents the deterministic part of your model and answers the question “what shape do I expect the relationship between my response and predictor to be?”.\nYour choices are linear (where a unit change in the predictor always leads to the same change in the response) or non-linear (where the effect of the predictor on the response depends on the value of the predictor). Note that you need to make a shape assumption choice for each of your predictors in your research hypothesis.\nHow do you choose?\nThink about theory: The first thing to consider is the nature of your predictor variable. Is it categorical? If yes, choose a linear shape assumption, as a non-linear shape assumption does not make sense for a categorical predictor (e.g. species).\nIs your predictor continuous? Then you need to think a bit more about the relationship between your predictor and response. Do you expect the predictor to always affect your response in the same way (i.e. a unit change in your response for a unit change in your predictor is expected to be the same over the range in your predictor)? Or do you expect that relationship to change as your predictor changes?\n\nPlot your data: Plotting your response vs. your predictor is another good exercise to get you thinking about what shape assumption will be appropriate. The GGally package has some good options for quickly plotting your data:\n\nlibrary(palmerpenguins) # loading palmer penguins data\nlibrary(GGally) # loading GGally package\nlibrary(dplyr) # loading dplyr package for select() \n\nmyDat&lt;-select(penguins, bill_length_mm, body_mass_g, species) # select a subset of columns to plot.  These would be your response and predictor columns\n\nggpairs(data=myDat, # your data \n        mapping=aes(col=species), # ggpairs will plot all columns in myDat, so we only need to tell it here any grouping variables we also want to include\n        upper=\"blank\" # keep the upper triangle of plots blank for this simple example.  Check ?ggpairs for more options.\n        )\n\n\n\n\n\n\n\n\nIn this course, we will primarily focus on statistical models assuming a linear shape assumption. You will see how flexible these can be, but we will also discuss what you should do if you want to assume non-linear5 relationships between your response and predictors.\n\n\nCommunicating your starting model\nTo communicate your starting model, report your research hypothesis, your response and predictors (main effects and interactions) along with descriptions of your error distribution and shape assumptions.\nFor each, describe how you arrived at your choices (e.g. “a poisson error distribution assumption was chosen as the response variable is count data…”). We will go over examples of all this in class."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-fitting",
    "href": "DSPPH_SM_StartingModel.html#sec-fitting",
    "title": "Statistical Modelling: Starting Model",
    "section": "Fitting your starting model",
    "text": "Fitting your starting model\nOnce you have chosen your starting model, you will fit the model to your data so that it can be used to test your hypothesis.\n As mentioned above, fitting your model means that you are going to use your data to estimate the value of the coefficients in your model (e.g. slope, intercept).\nThe best choices for the coefficient values are the coefficient values that give you the highest probability of observing your data. In other words, the best choices for the coefficients are values that are most likely given the data you have6. Fitting models this way is done using a method called maximum likelihood.\nFor example, think about how we would draw a “best-fit” linear line through this relationship:\n\nWhat we do intuitively is to find a line that minimizes the error7 in the model (i.e. the average difference between each observation and the fitted line). The coefficient values of this “best-fit” line have the maximum likelihood given our data.\nThe math involved when fitting your model follows the same logic to find the most likely values for your model coefficients. The actual math used will vary based on the type of model you are fitting (i.e. based on your error and shape assumptions), but in general, finding the right coefficients can be illustrated like this “gradient descent” illustration:\n\nThe most likely coefficient values are found by choosing a starting point for coefficient values and fitting the model. Then, the coefficient values are changed slightly and a new fit is made. The new fit is compared to the old fit to determine if the fit improved (i.e. the average error around the model decreased). This procedure is repeated8 until the error is reduced as much as possible. The resulting coefficient values are the most likely coefficient values for your model given your data.\nHere is another illustration showing gradient descent for two coefficients (e.g. two slopes):\n\nThe mathematical methods involved to fit our model will vary depending on our error distribution assumption and shape assumption. The math methods are behind the functions you will use to fit our models in R. Let’s look at one example of this now.\n\nGeneralized Linear Models (GLMs)\nGeneralized Linear Models or GLMs are statistical models that have a linear shape assumption but allow for a wide range of error distribution assumptions. The “generalized” in generalized linear model refers to the fact that the methods used to fit a GLM were developed from methods used to fit models that were restricted to a normal error distribution assumption (e.g. simple linear models and ANOVAs)9:\n\nSo we can use a GLM as our starting model when we have a shape assumption that is linear, and one of a variety of error distribution assumptions - see a little further along to find out which error distribution assumptions are supported by GLMs.\n\nHow to fit a GLM to data in R\n\nChoosing a link function:\nTo fit a GLM, you need to let R know what error distribution assumption you are using so that the math governing the stochastic part of your model can be “linked” to the original math developed for a normal error distribution assumption.\nSo, we need to choose a “link function” to use when we fit our model to data. This is not hard: There are canonical (default) link functions associated with each of the error distribution assumptions for your GLM. You can find them in R with ?family which will open up the help window to show:\n\nIn general, start with the default link function that matches the error distribution assumption you chose for your starting model.\n\n\n\n\n\n\n\nTransformations vs. GLMs\n\n\n\n\n\nYou may be familiar with the idea of transforming your response variable to be normally distributed for use in linear model. This idea stems from a time when methods were limited to those requiring a normal error distribution assumption. GLMs mean transformations are often no longer necessary as you can now choose an error distribution that reflects the nature of your data instead.\nWhile the idea of transformations is similar to the GLM’s link function, they are not the same. Transformations transform the response variable itself in order to try to constrain it to a normal error distribution assumption. In contrast, link functions “transform” the expected (fitted) value of the model while also indicating an error distribution that matches the original data.\nWhen possible, using a GLM with a link function is preferable to transforming your response variable. This is because transformations:\n\nchange the response variable itself making interpretation of modelled effects more difficult\ncan introduce bias in the coefficients,\ncan result in models that make predictions that are impossible, and\ncan be tricky to find an appropriate transformation.\n\nThat said, there may be times when you want to use a transformation of your response - e.g. to follow a method that is already established in your field. For these cases, you can still fit the model with the GLM strategy we describe here.\n\n\n\n\n\nFitting your GLM to your data:\nThe function for fitting a GLM in R is (helpfully) glm() and it is already installed in base R (no need to load another package).\nTo fit your model to the data you need to tell R your hypothesis10, your data, and your error distribution assumption:\n\nstartMod &lt;- glm(formula = Resp ~ Pred + 1, # your hypothesis\n                data = myDat, # your data\n                family = ...) # your error distribution assumption\n\nFor example, a GLM fit with a Gamma error distribution assumption would be:\n\nstartMod &lt;- glm(formula = Resp ~ Pred + 1, # your hypothesis\n                data = myDat, # your data\n                family = Gamma(link=\"inverse\")) # your error distribution assumption, with the canonical link \n\nAnd that’s it! You are now ready to fit a GLM as a starting model on the road to testing your research hypothesis!\n\n\n\n\nYour GLM model object\nBefore you can use your model to test your hypothesis, you need to validate your starting model. This will be the focus of the next section of your statistical modelling framework.\nFor now though, let’s take a first look at your starting model.\nFitting your GLM will produce a model object (called startMod above) - let’s explore this object. If you print information about the object itself, you will get something like:\n\nThis output includes11:\n\nA: a description of your starting model: This gives back information on the starting model you fit.\nB: estimates of the coefficients of your model: This gives you estimates of each coefficient in your model. Remember (as mentioned above) that the coefficients tell you the direction and magnitude of the effects of your predictor on your response. Coefficients in this output can be hard to interpret (e.g. they may be influenced by the error distribution assumption, and/or be complicated by many factor levels in a categorical predictor). We will be discussing how to get estimates of your coefficients in meaningful ways.\nC: Measures of how well your model performs: This section includes:\n\ndegrees of freedom for the null model (“Total” or “Null”, assuming no effects of your predictors on your response) and your starting model (“Residual”). Degrees of freedom are a measure of how complicated your model is and how much data you have.\ndeviance for the null model (assuming no effects of your predictors on your response) and remaining deviance after your starting model is applied. Deviance represents the variability in your response variable. It is this variability we are trying to explain. Comparing the Residual Deviance (remaining variability in your response after starting model is applied) with Null Deviance (original variability in your response that you were trying to explain) tells you how your model performs (i.e. how much variability in your response did you manage to explain).\nAIC stands for Akaike Information Criterion. AIC is another way of indicating model performance. It balances the explained variation with how complicated your starting model is. Your model complexity relates to how many predictors (and individual terms) are in your model as well as the shape of your model. We will talk much more about AIC in the Hypothesis Testing section of your model framework.\n\n\nYou can get a little more information about your model using the summary() command. Using summary(startMod) will lead to something like:\n\nSimilar to the output above, there are three sections produced:\n\nA: a description of your starting model\nB: estimates of the coefficients of your model: Notice that here you get more information about your coefficients. The summary() output also gives you information about the error around your coefficients as well as a test of significance of the coefficient. This test is a kind of hypothesis test, but the meaning behind the test and result will vary with your starting model structure. We will discuss this more in the Hypothesis Testing section coming up.\nC: Measures of how well your model performs: This section gives you similar information to the output above, but also includes the “Number of Fisher iterations”. This relates to how the model was fit. See the section on “Fitting your starting model”.\n\nIf you’re interested, here are some examples of GLM model objects. And if you’re not, skip to A first look at your starting model. As mentioned above, we will come back to coefficients again in the Reporting section of the Statistical Modelling Framework.\n\n\n\n\n\n\nGLM model object examples\n\n\n\n\n\nLet’s take a look at an examples of a GLM object in R.\nNOTE: the descriptions here are relevant for models with a normal error distribution assumption (i.e. using an “identity” link). We will generally not be using information from these objects in our Statistical Modelling Framework as it is easy to misinterpret the information (e.g. with error distribution assumptions other than normal).\n\nExample 1: Resp ~ ContPred + 1\nThe first example fits a GLM to test the hypothesis that\nResp ~ ContPred + 1\nwhere\n\nResp is your response variable,\nContPred is a continuous predictor,\nand your error distribution assumption is normal.\n\n\nstartMod.1 &lt;- glm(Resp ~ ContPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.1 object gives you:\n\nsummary(startMod.1)\n\n\nCall:\nglm(formula = Resp ~ ContPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4645.2  -2072.8     60.6   2060.8   4885.7  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20871.686   2176.542   9.589  &lt; 2e-16 ***\nContPred      -14.143      2.713  -5.212 2.74e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5551537)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance: 2764665463  on 498  degrees of freedom\nAIC: 9187.7\n\nNumber of Fisher Scoring iterations: 2\n\n\nAt the top of the output is the “Call”- the model you fit with the glm() function:\n\nsummary(startMod.1)$call\n\nglm(formula = Resp ~ ContPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\n\nBelow this are the coefficients. Notice there are two coefficients, and for each coefficient, you can see four values:\n\nthe coefficient estimate (Estimate),\nuncertainty (as standard error, Std.Error),\na t-statistic (t value, based on the estimate and error associated with the coefficient)12,\nand probability associated with the t-statistic (Pr(&gt;|t|)):\n\n\nsummary(startMod.1)$coefficients\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 20871.68582 2176.541631  9.589380 4.247405e-20\nContPred      -14.14253    2.713359 -5.212184 2.739251e-07\n\n\nThe t-statistic is used to test the null hypothesis that an estimate (a coefficient in this case) is not different than 0. The probability gives us the probability that you would get a t-statistic at least as large as you did even though the null hypothesis is in fact true. If the probability is very low, it is more likely our estimate is different than 0.\n\nNote that you have two coefficient estimates (rows) in the table above:\n\nThe first row of the coefficient table gives us the coefficient estimates associated with the intercept (2.087169^{4})\nThe second row is the coefficient estimate associated with ContPred. Note that as ContPred is a continuous variable, this estimate represents the slope of the linear effect of ContPred on Resp, i.e. for every unit change in ContPred, you get a -14.14 change in Resp.\n\n\n\nExample 2: Resp ~ CatPred + 1\nThe second example fits a GLM to test the hypothesis that\nResp ~ CatPred + 1\nwhere\n\nResp is your response variable,\nCatPred is a categorical predictor with three levels (“North”, “South”, “Central”),\nand your error distribution assumption is normal.\n\n\nstartMod.2 &lt;- glm(Resp ~ CatPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.2 object gives you:\n\nsummary(startMod.2)\n\n\nCall:\nglm(formula = Resp ~ CatPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2169.85   -731.45    -53.57    712.81   2912.31  \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6737.60      79.63   84.61   &lt;2e-16 ***\nCatPredSouth    2596.95     110.40   23.52   &lt;2e-16 ***\nCatPredCentral  5406.34     108.61   49.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 976451.4)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance:  485296363  on 497  degrees of freedom\nAIC: 8319.8\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nNote that we now have three coefficients in the table above. Recall from the Why do you need a starting model section that, with a categorical Predictor, fitting a model finds the mean predicted value of the Response at each category (level) of the categorical Predictor. The three coefficients are:\n\nThe coefficient labelled (Intercept) gives us the fitted value of our Resp when CatPred is “North” (6737.6). When you have a categorical predictor, R uses the Intercept coefficient to represent the predicted value of the response at one of the categories (levels).13 By default it chooses the first of the categories (levels) of your predictor (in this case, “North”)14\nThe coefficient labelled CatPredSouth gives you the difference between the predicted value of the response when CatPred = \"South\" and when CatPred = \"North\". So if you want to know the predicted value of Resp when CatPred = \"South\" you need to calculate 6737.6 + 2596.95 = 9334.55.\nThe coefficient labelled CatPredCentral gives you the difference between the predicted value of the response when CatPred = \"Central\" and when CatPred = \"North\". So if you want to know the predicted value of Resp when CatPred = \"Central\" you need to calculate 6737.6 + 5406.34 = 1.214394^{4}.\n\nYes, this is tedious way to calculate the coefficients in your model! And that is why use a different way when we come to the Reporting section of the Statistical Modelling Framework.\n\n\nExample 3: Resp ~ ContPred + CatPred + ContPred:CatPred + 1\nThe third example fits a GLM to test the hypothesis that\nResp ~ ContPred + CatPred + ContPred:CatPred + 1\nwhere\n\nResp is your response variable,\nContPred is a continuous predictor,\nCatPred is a categorical predictor with three levels (“North”, “South”, “Central”),\nContPred:CatPred indicates that you are including an interaction term representing a two-way interaction between your predictors,\nand your error distribution assumption is normal.\n\n\nstartMod.3 &lt;- glm(Resp ~ ContPred + CatPred + ContPred:CatPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.3 gives you:\n\nsummary(startMod.3)\n\n\nCall:\nglm(formula = Resp ~ ContPred + CatPred + ContPred:CatPred + \n    1, family = gaussian(link = \"identity\"), data = myDF)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2204.12   -545.06     58.81    574.09   3003.42  \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             22335.803   1283.405  17.404  &lt; 2e-16 ***\nContPred                  -19.516      1.604 -12.169  &lt; 2e-16 ***\nCatPredSouth            -2665.492   1818.608  -1.466 0.143373    \nCatPredCentral           -964.233   1805.741  -0.534 0.593594    \nContPred:CatPredSouth       6.660      2.266   2.939 0.003444 ** \nContPred:CatPredCentral     7.985      2.255   3.541 0.000436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 638994)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance:  315663026  on 494  degrees of freedom\nAIC: 8110.7\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nNote that we now have six coefficients in the table above:\n\nThe coefficient labelled (Intercept) gives us the fitted value of our Resp when CatPred is “North” and ContPred is set to the mean value of ContPred.\nThe coefficient labelled ContPred gives us the coefficient (slope) associated with ContPred when CatPred = \"North\".\nThe coefficient labelled CatPredSouth gives you the difference between the predicted value of the response when CatPred = \"South\" and when CatPred = \"North\" (with ContPred is set to the mean value of ContPred). So, when CatPred = \"Central\", the coefficient (intercept) for CatPred is 2.23358^{4} + -2665.49 = 1.967031^{4}.\nThe coefficient labelled CatPredCentral gives you the difference between the predicted value of the response when CatPred = \"Central\" and when CatPred = \"North\" (with ContPred is set to the mean value of ContPred). So, when CatPred = \"Central\", the coefficient (intercept) for CatPred is 2.23358^{4} + -964.23 = 2.137157^{4}.\nThe coefficient labelled ContPred:CatPredSouth gives us the difference between coefficient (slope) associated with ContPred when CatPred = \"South\" vs. the ContPred slope when CatPred = \"North\". So, when CatPred = \"South\", the coefficient (slope) for ContPred is -19.52 + 6.66 = -12.86.\nThe coefficient labelled ContPred:CatPredCentral gives us the difference between coefficient (slope) associated with ContPred when CatPred = \"Central\" vs. the ContPred slope when CatPred = \"North\". So, when CatPred = \"Central\", the coefficient (slope) for ContPred is -19.52 + 7.99 = -11.53.\n\nYes, this is tedious way to calculate the coefficients in your model! And that is why use a different way when we come to the Reporting section of the Statistical Modelling Framework."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-firstLookModel",
    "href": "DSPPH_SM_StartingModel.html#sec-firstLookModel",
    "title": "Statistical Modelling: Starting Model",
    "section": "A first look at your starting model",
    "text": "A first look at your starting model\nYou can use the visreg package to quickly visualize your modelled effects\n\nlibrary(visreg) # load visreg package\nlibrary(ggplot2) # load ggplot2\n\nvisreg(startMod.3, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"ContPred\", # predictor on x-axis\n       by = \"CatPred\", # predictor plotted as colour\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  geom_point(data = myDF, # data\n             mapping = aes(x = ContPred, y = Resp, col = CatPred))+ # add your data to your plot\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cont, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nRemember though: before you explore these modelled effects too closely, you have to validate your model."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#up-next",
    "href": "DSPPH_SM_StartingModel.html#up-next",
    "title": "Statistical Modelling: Starting Model",
    "section": "Up next",
    "text": "Up next\nNext we will discuss how you can make validate your model (make sure your starting model can be used to test your hypothesis), and then test your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#footnotes",
    "href": "DSPPH_SM_StartingModel.html#footnotes",
    "title": "Statistical Modelling: Starting Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore on this to come!↩︎\nI will keep mentioning mechanisms. In our statistical model building, we keep our focus on biologically meaningful or mechanistic explanations of variability in our response. This is because i) explaining the world through mechanisms is necessary for true understanding and to be able to show this understanding through prediction (e.g. the difference between correlation and causation). And ii) this is where the joy of being a biologist lies!↩︎\nmore on this to come↩︎\nA good time to review your notes on data distributions from earlier↩︎\nwhich might be non-linear of known shape, or non-linear of unknown shape↩︎\nParaphrased from Crawley 2013 pg. 451↩︎\nhere shown as Root Mean Square Error which is a method used when one has a normal error distribution assumption and linear shape assumption↩︎\neach repetition is called “an iteration”↩︎\nWe will discuss these other model types in upcoming classes↩︎\nfor a binomial error distribution assumption, you might have your response as the # successes in # of trials. In such cases, you would present your hypothesis as cbind(Success, Trials) ~ Predictor + 1↩︎\nnote that we will be discussing this more in depth when we get to the Reporting section of the Statistical Modelling Framework↩︎\nnote that the type of statistic shown will depend on the structure of your model including the error distribution assumption↩︎\nThis is called “dummy level coding”. You can avoid this with “level means coding”.↩︎\nNote that you can control this if needed.↩︎"
  },
  {
    "objectID": "DSPPH_SM_Predictors.html",
    "href": "DSPPH_SM_Predictors.html",
    "title": "Statistical Modelling: Predictor(s)",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nDefine your predictor variables (what could explain the variability in your response?)\nPresent possible mechanisms behind your argument (why might your predictor variables explain variability in your response?\n\n\n\n\n\nOften very quickly, you will start to have ideas about what mechanistically might be responsible for the variation in your response. This is exciting! This is where you can apply biological theory to form your research hypothesis of why observations are as they appear. This step needs you to be curious, creative, and tap into your foundation of biological theory. And this step is where you identify the predictors in your statistical model.\nHere your focus is on the biological mechanisms (or processes) that you expect affect change in your response variable. An example of a mechanism affecting plant height might be temperature-dependent growth as temperature controls the rate of enzymatic reactions involved in plant growth.\nOnce you have identified a possible mechanism, you can identify a measurable factor that can be used to quantify that mechanism. This is a predictor. To follow our example, a corresponding predictor to measure an effect of temperature-dependent growth is ambient temperature.\nIt is necessary also to spend some time thinking about how you measure your predictor - what measure is relevant to your response variable? Here think about how measures of your predictor can be relevant to the time and space resolution of your response variable. To complete our example, you will want to measure ambient temperature quite close to each plant, and need to consider not just the temperature on the day the plant height was measured, but throughout the growing period of the plant (e.g. by considering average or integrated temperature measures).\nSo your response variable is the variability you are trying to explain, and your predictor(s) is what you think is causing the variability. In this course, we will use the term “predictors” but note that they are also known as “covariates”, “factors”, “independent variables”, “explanatory variables”, or “x variables”.\nIt is important that you let yourself think freely when you are considering what might by causing the variability in your response variable. At this early stage, do not restrict yourself to what you will be able to measure and test - let your curiosity and ideas range freely (called “blue-sky thinking”). Think first about all the mechanisms that may be responsible for the response variability. Then think about all the ways observations may be limited (e.g. limitations in our ability to measure certain variables or access data from certain places or times). And take lots of notes! As you move on in the framework, you will quickly simplify your hypothesis into what is measured and what is testable, but all your exciting ideas will be used in to communicate the scope of your study, motivate your predictor choice (in your Introduction and Methods), to put your results into context of greater biological theory, as well as direct future study efforts to focus on variation in your response that remains unexplained (in your Discussion section). Spending some time allowing yourself to brainstorm at this point is time well spent.\nFinally, notice that throughout this section, we emphasized mechanisms. You want to develop and test a hypothesis that is grounded in biological mechanisms."
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html",
    "href": "DSPPH_SM_Hypothesis.html",
    "title": "Statistical Modelling: Hypothesis",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nidentify your model terms (main effects and interactions)\npresent your research hypothesis in words and formula"
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html#hypotheses-with-more-than-one-predictor---including-interactions",
    "href": "DSPPH_SM_Hypothesis.html#hypotheses-with-more-than-one-predictor---including-interactions",
    "title": "Statistical Modelling: Hypothesis",
    "section": "Hypotheses with more than one predictor - including interactions",
    "text": "Hypotheses with more than one predictor - including interactions\nWhen you are considering more than one predictor, you need to consider the possibility of both main effects and interactions.\nA main effect represents the direct independent effect of the predictor on the response.\nAn interaction represents that the effect of one predictor on the response depends on the value of the other predictor.\nIf your research hypothesis states that\n\nVariability in the response (Resp) is explained by independent effects of two predictors (Pred1 and Pred2)1\n\nyour hypothesis in R syntax would be:\n\nResp ~ Pred1 + Pred2 + 1\n\nIf your research hypothesis states that\n\nVariability in the response (Resp) is explained by effects of two predictors (Pred1 and Pred2 with the effect of Pred1 depending on the value of Pred2)\n\nyour hypothesis in R syntax would be:\n\nResp ~ Pred1 + Pred2 + Pred1:Pred2 + 1\n\nNote that : is used to denote an interaction. In this case, it is a two-way interaction between Pred1 and Pred2.2\n\n\n\n\n\n\nTip\n\n\n\n\n\nR has a number of shortcuts for representing formulas in shortform. For example:\nResp ~ Pred1 + Pred2 + Pred1:Pred2 + 1\ncan be written as\nResp ~ Pred1*Pred2\ni.e. * means to include all main effects and all possible interactions.\nAnother example:\nResp ~ Pred1 + Pred2 + Pred3 + Pred1:Pred2 + Pred2:Pred3 + Pred1:Pred3 + 1\ncan be written as\nResp ~ (Pred1 + Pred2 + Pred3)^2\nwhich tells R to include all main effects and all possible two-way interactions between the three predictors."
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html#footnotes",
    "href": "DSPPH_SM_Hypothesis.html#footnotes",
    "title": "Statistical Modelling: Hypothesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome jargon here: when the effects of each predictor is independent of other predictors, you say the model is “additive”↩︎\nNote: to include an interaction all predictors involved in the interaction have to have a main effect also included in the model↩︎"
  },
  {
    "objectID": "handbookIntro.html",
    "href": "handbookIntro.html",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.\n\n\n\nThere are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.\n\n\n\nThe ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is.",
    "href": "handbookIntro.html#what-this-handbook-is.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is-not.",
    "href": "handbookIntro.html#what-this-handbook-is-not.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "There are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#dsp-program-tools",
    "href": "handbookIntro.html#dsp-program-tools",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#footnotes",
    "href": "handbookIntro.html#footnotes",
    "title": "Introduction to the Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComputational thinking includes skills in decomposition (breaking down tasks into small steps), pattern recognition (observing patterns in tasks and data), abstraction (identifying and extracting relevant information, ignoring or removing unnecessary information), algorithms (creating an ordered set of instructions for solving a problem), modelling and simulation (statistical modelling for hypothesis testing, imitating processes and problems), and evaluation (determining the effectiveness of a solution, generalizing to apply the solution to a new problem) - adapted from digitalcareers.csiro.au.↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "DSPPH_DA_Merge.html",
    "href": "DSPPH_DA_Merge.html",
    "title": "So you want to: merge your data sets",
    "section": "",
    "text": "Often times we are interested in exploring connections among variables from different sources. Merging your data files can be a way of collecting all the variables of interest so you can explore research questions and hypotheses about the data.\nIn addition, merging can be used to label your observations. We go through examples below."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "href": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "title": "So you want to: merge your data sets",
    "section": "in the base package using merge()",
    "text": "in the base package using merge()\n\nmDat&lt;-merge(Dat1, # data frame to merge\n             Dat2, # other data frame to merge\n             by = c(\"ID\", \"Day\")) # merge by variable(s)\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n\n\nNote that you don’t need to have the same number of observations (rows) in your two data frames to merge. Merging can be a great way of labelling your data. Here’s an example:\nConsider a data frame with strain information for each of your organism IDs:\n\n## ID: organism ID\n## Strain: strain of organism\n\nstr(Dat3)\n\n'data.frame':   20 obs. of  2 variables:\n $ ID    : chr  \"id24\" \"id30\" \"id33\" \"id26\" ...\n $ Strain: chr  \"A\" \"C\" \"B\" \"C\" ...\n\n\nNote that Dat1 and Dat2 each contained 40 observations - one observation for each of 20 IDs made on each of 2 days.\nIn contrast Dat3 only has 20 observations - information about the strain for each of 20 IDs.\nBy using merge(), we can add the strain information to mDat:\n\nallDat &lt;- merge(mDat, # one data frame\n                Dat3, # the other data frame\n                by = \"ID\") # variables to merge by\n\nstr(allDat)\n\n'data.frame':   40 obs. of  5 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n $ Strain: chr  \"C\" \"C\" \"A\" \"A\" ...\n\n\nNow we have our observations labelled by the strain information!\nSome things to note:\n\nif you leave out the by = function totally, R will look for column names that are similar between the two data frames and use that for the merge.\nyou can designate that the “merge by” variables have different column names in the two data frames. This is done with the by.x = and by.y = arguments. Check ?merge for more.\nyou can control what happens to unmatched columns (e.g. if an ID appeared in only one of the two data frames). This is done with the all =, all.x =, and all.y = arguments. Check ?merge for more."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "href": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "title": "So you want to: merge your data sets",
    "section": "in the dplyr package using full_join()",
    "text": "in the dplyr package using full_join()\nThe dplyr package includes the full_join() function as another way to merge your data frames\n\nlibrary(dplyr) # load dplyr package\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmDat&lt;-full_join(Dat1, # data frame to merge\n                Dat2, # other data frame to merge\n                by = join_by(ID, Day)) # merge by column\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id29\" \"id30\" \"id22\" \"id31\" ...\n $ Day   : num  1 2 1 2 1 1 2 1 2 2 ...\n $ Length: num  150 164 110 134 155 125 144 144 139 167 ...\n $ Temp  : num  7.4 7.3 6.9 7.5 5.6 7.8 7.8 6.3 7.9 7.1 ...\n\n\nSome things to note:\n\nThe full_join() function keeps all observations appearing in either data frame.\nThe left_join() function keeps all observations in the first data frame (Dat1) but you will lose any unmatched observations in the second data frame (Dat2).\nThe right_join() function keeps all observations in the second data frame (Dat2) but you will lose any unmatched observations in the first data frame (Dat1)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.\n\n\n\nThe Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions\n\n\n\n\n\n\n\nModules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#purpose",
    "href": "intro.html#purpose",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#interested-in-contributing-to-the-dsp-program",
    "href": "intro.html#interested-in-contributing-to-the-dsp-program",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "Modules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "handbookDAIntro.html",
    "href": "handbookDAIntro.html",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.\n\n\n\n\n\nA bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.\n\n\n\nHere we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.\n\n\n\n\nHere we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-tools",
    "href": "handbookDAIntro.html#a-note-on-tools",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "href": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "A bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#the-basics",
    "href": "handbookDAIntro.html#the-basics",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#so-you-want-to",
    "href": "handbookDAIntro.html#so-you-want-to",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Contacts and Feedback for the DSP Program",
    "section": "",
    "text": "The DSP Program is a joint venture by colleagues across AU’s Department of Biology.\nProgram development is led by the DSP Program Taskforce:\n\nAnna B. Neuheimer (Aquatic Biology, Taskforce head)\nRobert Buitenwerf (Ecoinformatics and Biodiversity)\nAlejandro Ordonez Gloria (Ecoinformatics and Biodiversity)\nTove Hedegaard Jørgensen (Genetics, Ecology and Evolution & Centre for Educational Development)\nIan Marshall (Microbiology)\nBirgit Olesen (Aquatic Biology & Arctic Research Centre)\nPeter Teglberg Madsen (Zoophysiology)\nJesper Givskov Sørensen (Genetics, Ecology and Evolution)\n\n\n\nSend your questions and feedback to the DSP Program Taskforce\nWe welcome your questions and feedback. Please submit them using this form or send an email to abneuheimer@bio.au.dk."
  },
  {
    "objectID": "handbookSMIntro.html",
    "href": "handbookSMIntro.html",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistics",
    "href": "handbookSMIntro.html#why-statistics",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistical-modelling",
    "href": "handbookSMIntro.html#why-statistical-modelling",
    "title": "Statistical Modelling Handbook",
    "section": "Why statistical modelling?",
    "text": "Why statistical modelling?\nYou can quantify how much variability you can explain with your research hypothesis through statistical modelling. Your statistical model represents your research hypothesis in a mathematical structure. This mathematical structure can be tested against your data to determine what evidence there is for your hypothesis:\n\ncan I explain the variability that I am seeing? (Can I reject my hypothesis?)\ngiven my hypothesis, how much variation in the observations can I explain?\ngiven my hypothesis, what would I expect (predict) to observe under different times or locations?\n\nYour job then is to explain observation variability in time and space by creating a “model” of what (you think) is going on - hence statistical modelling.\nIt is important to remember that any model is only an approximation of what is going on in the real world. As many have said before\n\nAll models are wrong but some are useful.\n\nWe will discuss how you can build useful models that you can use to test your hypotheses.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "href": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "title": "Statistical Modelling Handbook",
    "section": "Introducing a statistical modelling framework",
    "text": "Introducing a statistical modelling framework\n\nThe focus of this section of the handbook is statistics that can be applied to your work as a biologist. For that reason, the motivation for what we are going to do together comes directly from your biological research hypotheses. As a biologist, you have a research hypothesis that you want to test. The method presented in this handbook will help you test it. You will learn how to move from biological theory to hypothesis to the statistical modelling process you will use to test your hypothesis.\nYou will learn this process of statistical modelling by walking through a “Statistical Modelling Framework”. This is a set of steps that you can use to go from your research hypothesis to designing a model, testing your hypothesis and communicating the results.\nThis handbook will walk through the parts of this framework one by one. During DSP modules throughout your degree, you will do the same in class while you practice applying the framework to case studies. In this way, you should see how the framework is generally applicable but also flexible. And after you leave the course you will be able to apply the framework to help you in your statistical analysis in other courses, your thesis, and your future career - in every case, your process will take its starting point and focus from the hypothesis that is motivating you.\nAs we will discuss later in the handbook, working through this framework will also guide you in creating the “guts” of a paper or report. As well as clarifying how and why you made your analysis choices, it will guide you in describing your motivation behind your research question (why is it worthwhile to spend time explaining this variation?) as well as the mechanisms behind your research hypothesis (why do I think X is responsible for the variation I’m trying to explain?). Once through, you’ll have a solid draft that can be the basis of a report, thesis chapter or scientific paper. We will talk about how this works in section.\nA note about our Statistical Modelling Framework: The steps in the image to the right as a linear process but it is not actually a linear process. As you will see in the examples, sometimes you will need to make best guesses5 as to what model might be useful and only after confronting the model with your data will you know if your guesses were reasonable (and useful!) - i.e. the model is a valid one that you can use to test your hypothesis. We will talk about how to find a useful model, how to choose when there are multiple options, and how to communicate your choices.\n\nSteps in the statistical modelling framework:\n\n\nResponse(s)\nHere you will define your research question by identifying your response variable(s).\n\nWhat variability are you trying to explain?\nAnd why is it worth explaining? (your motivation)\n\nNote: we’ll begin by discussing how to model hypotheses with just one response variable before discussing multiple response variable(s).\n\n\nPredictor(s)\nHere you will choose your predictor variables.\n\nwhat could explain the variability in your response?\nwhat are the possible mechanisms behind your argument?\n\n\n\nHypothesis\nHere you will see how your response and predictor variables come together to define your research hypothesis. And we will discuss how to write this hypothesis to begin building your statistical modelling.\n\n\nStarting model\nHere you will choose and fit the starting model that will be used to test your hypothesis. You will do this by choosing and communicating two key assumptions that will help you pick a useful modelling starting point. Then you will fit your model to your data (i.e. confronting your model with your data).\n\n\nModel validation\nHere you will investigate whether your model will be a useful one to test your hypothesis. Your steps here will include considering if you have correlated predictors or problems with observation dependence.\nYou will also considering if your starting model assumptions were realistic. After this step, you will have a model that you can confidently use to test your hypothesis.\n\n\nHypothesis testing\nHere you will test your hypothesis by assessing the evidence supporting your model. We will discuss a number of different methods to do this, but will focus on the model selection method as a robust way to evaluate what your model is telling you about your hypothesis.\n\n\nReporting\nHere you will report the results of your hypothesis testing.\nYou will report:\n\nyour best-specified model identified in the hypothesis testing\nthe effects (patterns) described by your model (including visualizing your model effects)\nhow well your model explains variability in your response.\n\n\n\nPredicting\nHere you will use your model to make predictions of your response under different conditions (while considering prediction limits).\n\n\n\nWhere we will begin: generalized linear models (GLMs)\nTo begin with, we will be discussing generalized linear models (GLMs) as models that can be useful to test many different hypotheses. Also, understanding how GLMs can be used to test your hypothesis will help you understand other, including more advanced, statistical models (Pongpipat_et_al_PracticalExtensionStatisticsForPsychology).\nRemember: the model you choose is just an approximation of the real world. This means that often times alternative models would be possible (models like t-tests, ANOVAs, ANCOVAs, etc.6). In fact, you may be collaborating with someone who wants to model your hypothesis with a different method. In this handbook, we compare GLMs to alternative model types here. And remember: the steps in the statistical modelling framework are generally applicable. Regardless of the method you apply, you need to ground your choices in good biological and statistical theory.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#the-examples",
    "href": "handbookSMIntro.html#the-examples",
    "title": "Statistical Modelling Handbook",
    "section": "The examples",
    "text": "The examples\nHere we’ve gathered examples following our statistical modelling framework structure. You can request/contribute new examples here",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "href": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "title": "Statistical Modelling Handbook",
    "section": "From statistical modelling to scientific report writing",
    "text": "From statistical modelling to scientific report writing\nHere you can see how you can use the steps in the Statistical Modelling Framework to outline your communication of your hypothesis testing in reports and paper.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#where-to-from-here",
    "href": "handbookSMIntro.html#where-to-from-here",
    "title": "Statistical Modelling Handbook",
    "section": "Where to from here?",
    "text": "Where to from here?\nTBA: - boosted regression trees - Bayesian negative bionomial regression mixed models - nested design in mixed models - random forest vs. GAMs - meta-analysis - complex models - classification (categorical response) and regression (continous response) trees - boosted regression trees - boosted regression trees iteratively fit models gradually increasing emphasis on observations that were initially poorly fit - ADMB and TMB\n\nmultivariate statistics\nexploratoryordination/clustering\ncross validation\n\n[[Principle Component Analysis]] linearly transforms multivariate data into a new coordinate system where the majority of the variation in the data is captured with fewer dimensions than the initial data - Li et al. 2023\n[[Principle Component Analysis|PCA]] constructs a “map” of the samples where samples that are more similar are closer together\n\nAcknowledgements\nTBA",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#footnotes",
    "href": "handbookSMIntro.html#footnotes",
    "title": "Statistical Modelling Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhere, we can also use the term “research hypothesis” instead of explanation and you’ll see we quickly switch to using this term↩︎\nMath skills have often been underemphasized in many Biology educations. This has not been helpful or necessary. Biologists need math and are very capable at applying math to solve problems, but the math needs to be useful; that is, math that you can apply to your own needs as you research biology. The DSP Program aims at providing statistical modelling tools that will be useful to you as a biologist. It is a happy coincidence that the same skills can be applied to a lot of other situations as well. The programming and statistics skills you are learning here will be useful to you in the future - in your future courses, thesis-writing and a wide range of careers.↩︎\nmore on causal vs. correlative explanations coming soon↩︎\nuseful predictions will always include uncertainty↩︎\nand these will be educated guesses!↩︎\nDon’t worry if these terms don’t mean anything to you yet - more to come!↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html",
    "href": "handbookDCIntro.html",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Data curation is the collection and on-going management of data.\nGood data curation helps maintain data quality, and allows you and others to discover, access, and re-use your data. Data curation skills then are essential for ethical, transparent and reproducible science, and are an important part of your contribution to the field of biology.\n\n\nDepending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.\n\n\n\nThe individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns R can handle that, but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)\n\n\n\n\nFor more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "href": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Depending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#data-collection",
    "href": "handbookDCIntro.html#data-collection",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "The individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns R can handle that, but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#where-to-go-from-here",
    "href": "handbookDCIntro.html#where-to-go-from-here",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "For more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#footnotes",
    "href": "handbookDCIntro.html#footnotes",
    "title": "Data Collection & Curation Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthough you may have projects where you are working with databases↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "The Data Skills Portfolio (DSP) Program is based at Aarhus University’s Department of Biology.\nThe DSP Program is designed to help our Biology students develop robust and useful quantitative skills for biological research and many fields beyond.\nHere you will find a description of the program along with DSP handbook materials.\nExplore and enjoy!\nAnd remember: your feedback is welcome!\n\n\nto the Data Skills Portfolio Program Handbook for\n\nData Collection & Curation\nData Analysis\nStatistical Modelling"
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "to the Data Skills Portfolio Program Handbook for\n\nData Collection & Curation\nData Analysis\nStatistical Modelling"
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html",
    "href": "DSPPH_DA_DatesTimes.html",
    "title": "So you want to: work with dates and times",
    "section": "",
    "text": "You will often have a time component to your biological research and research hypotheses - i.e. you might want to explain variability over time.. As mentioned in the Data Collection and Curation section, it is a good idea to record the time of your observation as separate columns of year, month, day, etc. But you will also need to work with data collected by others where date (and sometimes time) information is together one column (variable)."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "href": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "title": "So you want to: work with dates and times",
    "section": "In the base package:",
    "text": "In the base package:\nIn the base package which is installed along with R1, you can use the as.Date() function to format your data as a date:\nConsider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nmyDat$Time &lt;- as.Date(x = myDat$Time, # the date and time column\n                      format = \"%Y/%m/%d %H:%M\") # describing the format of the date and time column\n\nhead(myDat) # examine the first few rows\n\n        Time Value\n1 2024-10-03    36\n2 2020-06-09    35\n3 2020-10-07    57\n4 2021-10-09    43\n5 2020-06-04    44\n6 2021-12-02    36\n\n\nYou can learn more about the formatting syntax with ?strptime. Note that the code above replaces the myDat$Time column with the new, formatted date and time information.\n\nYou can extract parts of the date and time column with functions like months() for months, years() for years, etc.\n\nFor example:\n\nmyDat$Months &lt;- months(myDat$Time) # extract only the months\n\nstr(myDat) # structure of the data frame\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : Date, format: \"2024-10-03\" \"2020-06-09\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: chr  \"October\" \"June\" \"October\" \"October\" ..."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "href": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "title": "So you want to: work with dates and times",
    "section": "Using the lubridate package:",
    "text": "Using the lubridate package:\nThe lubridate package was created to make working with dates and times easier. There are still two steps to the process. You can repeat the steps above but now with the lubridate package.\nAgain, consider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nlibrary(lubridate) # load the lubridate package\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nmyDat$Time &lt;- ymd_hm(myDat$Time) # the date and time column\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  2 variables:\n $ Time : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value: int  36 35 57 43 44 36 47 39 38 50 ...\n\n\n\nYou can extract parts of the date and time column with functions like month() for months, year() for years, etc. Notice that the function names are not plural with the lubridate package.2\n\nFor example:\n\nmyDat$Months &lt;- month(myDat$Time) # extract only the months\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: num  10 6 10 10 6 12 3 4 11 11 ...\n\n\nMuch more is available in the lubridate packages, including determining durations and dealing with time-zones. Check the lubridate package “cheat sheet” for more information."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#footnotes",
    "href": "DSPPH_DA_DatesTimes.html#footnotes",
    "title": "So you want to: work with dates and times",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nno need to load a separate package↩︎\nBoth months() and month() will work with lubridate, but months() is also used to denote a duration of a month. See the cheat sheet for more.↩︎"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html",
    "href": "DSPPH_DA_MissingValues.html",
    "title": "So you want to: deal with missing values",
    "section": "",
    "text": "Missing values are important and interesting, and they can affect how functions can be used on your data."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Apply functions to data containing missing values",
    "text": "Apply functions to data containing missing values\nWe can apply functions to objects or their components when missing values are present by letting R know what we want to do with the missing values. For example, getting the overall mean of the chlorophyll column without telling R how to handle missing values:\n\nmean(ChlData$Chl)\n\n[1] NA\n\n\nvs. telling R to remove them with the na.rm = ... argument:\n\nmean(ChlData$Chl, na.rm = TRUE)\n\n[1] 32.42375\n\n\nNote that the data frame itself remains unchanged, but R ignores the NAs when calculating the mean. We can find out more about how a particular function handles missing values by looking at the function’s help file (e.g. ?mean)."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "href": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Locating missing values",
    "text": "Locating missing values\nWe can also find missing values using the is.na() function:\n\nis.na(ChlData$Chl)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nand identify locations of missing values with:\n\nwhich(is.na(ChlData$Chl) == TRUE)\n\n[1] 5 8 9"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Removing missing values",
    "text": "Removing missing values\nFinally, we can remove all rows that are incomplete (i.e. containing any missing values) with the na.omit() function:\n\nhead(ChlData) # Original data frame\n\n  Station Year Month   Chl\n1     HL2 2007     2 16.07\n2     S27 2005    10 31.31\n3     HL2 2002    NA 40.38\n4     HL2 2001     2 32.00\n5     HL2 2001    10    NA\n6     S27 2007    10 29.71\n\nChlData &lt;- na.omit(ChlData) # Remove the NAs\n\nhead(ChlData) # Data frame without NAs\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nNote that above I reassign the output of the na.omit() function back to the name ChlData. This replaces the original data frame with the new data frame without missing values. I could also save it as a new object (with a new name) so the original is not overwritten.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTake a look at the numbers that print out to the left of the data frame:\n\nhead(ChlData)\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nThese are row names that were assigned when the data were read in. We can ignore row names but I wanted to to explain them as they can be distracting when one starts manipulating data frames. Unless we specify otherwise, rows are named by their original position when the data are read in to R, e.g. initially row #3 was assigned the name “3”, and row #4 was assigned the name “4”, etc. Since we’ve removed some rows with na.omit() above, the row names now skip from e.g. 2 to 4 (row 3 has been removed), but note that the 3rd row in the data frame can still be accessed with:\n\nChlData[3,]\n\n  Station Year Month Chl\n4     HL2 2001     2  32\n\n\nYou can choose your own row names with the rownames() function (or column names with the colnames() function) as well as with arguments in e.g. read.csv()."
  }
]
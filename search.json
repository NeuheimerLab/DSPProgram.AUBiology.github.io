[
  {
    "objectID": "DSPPH_SM_Predicting.html",
    "href": "DSPPH_SM_Predicting.html",
    "title": "Statistical Modelling: Predicting",
    "section": "",
    "text": "In this chapter you will learn:\n\n\n\n\n\n\nhow to use your statistical model to make a prediction"
  },
  {
    "objectID": "DSPPH_SM_Predicting.html#the-process",
    "href": "DSPPH_SM_Predicting.html#the-process",
    "title": "Statistical Modelling: Predicting",
    "section": "The process",
    "text": "The process\nHere we will go through the steps involved in using your model to make a prediction about your response, including (most importantly):\n\nStep Zero: consider your prediction limits\nBefore using your model to make a prediction, ask yourself: is it even logical to make this prediction?\nThings you should consider:\n\ndo I have a continuous predictor in my best-specified model?\n\nIf you have only categorical predictors in your model, you can not use your model to make a prediction outside the categories already in your model.\nLet’s bring in our Example 1 from the last chapter on Reporting to illustrate the prediction limits of models with only categorical predictors:\nResp1 ~ Cat1\n\n\n\n\n\n\n\n\n\nHere you can see the modelled Resp1 when Cat1 is Sp1, Sp2 or Sp3. You are not able to make a prediction for Cat1 = Sp4 or Cat1 = Sp63. This is limits of prediction limit when you have a categorical predictor. You are able to report the variability in Resp1 that is explained by Cat1 - this can give you an idea for how much Resp1 might vary if you study a new species.\n\nCan I assume that my model assumptions will hold under my prediction conditions?\n\nBy using your model to make a prediction, you are assuming that the same processes that govern the relationship(s) between your predictor(s) and response will hold at the new value of your predictor. For example, you might have used a linear shape assumption that was appropriate for modelling how growth varies due to temperature for your data, but now you want to estimate growth at an even higher temperature than you tested. Is this appropriate? Would it be possible that temperature has exceeded a tolerance limit and that growth starts to respond non-linearly with temperature?\n\nAs you can see, you need to consider prediction limits particularly when you are making a prediction using a value of your predictor outside the range in your predictor used to fit your model.\nYou also need to consider prediction limits when you are wanting to predict your response at a future time or different location.\nIn all cases: Think about the mechanisms underlying your hypothesis to consider if they are likely to be the same under your prediction conditions. This will help you decide if making a prediction is appropriate.\n\n\nStep One: Define the predictor values for your response prediction\nThe first step in using your model to make a response prediction is to define the values of your predictor(s) at which you want to make the response prediction. Note that you need to define a value for each of the predictors in your model, the defined values need to be in a data frame, and the columns should be given the same names as your predictors.\n\n\nStep Two: Make your response prediction\nYou can use the predict()2 function to predict the value of your response."
  },
  {
    "objectID": "DSPPH_SM_Predicting.html#examples",
    "href": "DSPPH_SM_Predicting.html#examples",
    "title": "Statistical Modelling: Predicting",
    "section": "Examples",
    "text": "Examples\nNote that Example 1 (Resp1 ~ Cat1) and Example 2 (Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1) from the previous chapter on Reporting only contained categorical predictors, so we will not explore them further here.\nExamples 3 (Resp3 ~ Cont4 + 1) and 4 (Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1) each contain a continuous predictor, so we will use those here as an example of how to use your models to make a prediction of your response.\n\nExample 3: Resp3 ~ Cont4 + 1\nHere’s a reminder of Example 3 (Resp3 ~ Cont4 + 1):\n\n\n\n\n\n\n\n\n\n\nWhat is the predicted response of Resp3 when Cont4 = 30?\n\n## Step One: Define the predictor values for your response prediction\nforPred &lt;- data.frame(Cont4 = 30) # the values of my predictor at which I want a response prediction\n\n## Step Two: Make your response prediction\nmyPred &lt;- predict(object = bestMod3, # your model\n                  newdata = forPred, # the values of the predictors at which to make the prediction\n                  type = \"response\", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption\n                  se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\n### This can be added to our forPred data frame as:\nforPred$fit &lt;- myPred$fit\nforPred$se &lt;- myPred$se.fit\n\nforPred\n\n  Cont4      fit       se\n1    30 7574.809 303.3246\n\n\nNote that you get the response prediction in myPred$fit = 7575 and an estimate of uncertainty around this prediction as standard error in myPred$se.fit = 303 (the last value myPred$residual.scale gives information on how the standard errors were estimated. See the help file for more.)\nSo the value of Resp3 when Cont4 = 30 units is 7575 ± 303 units.\nYou can add this to your plot with:\n\n# Set up your predictors for the visualized fit\nforCont4&lt;-seq(from = min(myDat3$Cont4), to = max(myDat3$Cont4), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  \n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod3, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat3, # add observations to your plot\n             mapping = aes(x = Cont4, y = Resp3)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont4, y = Fit),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont4, y = Upper),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont4, y = Lower),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ### NEW!  Adding your response prediction to the plot:\n  geom_point(data = forPred, # prediction data frame\n             mapping = aes(x = Cont4, y = fit), # coordinates of prediction\n             col = \"red\", # change point colour\n             size = 3) + # change point size\n  \n  ### NEW!  Adding errorbars for your response prediction to the plot:\n  geom_errorbar(data = forPred, # prediction data frame\n             mapping = aes(x = Cont4, ymax = fit+se, ymin = fit-se), # coordinates of prediction\n             width = 1, # the width of the error bars\n             col = \"red\") + # change point colour\n  \n  \n  ylab(\"Resp3, (units)\") + # change y-axis label\n  \n  xlab(\"Cont4, (units)\") + # change x-axis label\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\nWhat are the predicted responses of Resp3 when Cont4 = 9 and Cont4 = 22?\n\n## Step One: Define the predictor values for your response prediction\nforPred &lt;- data.frame(Cont4 = c(9,22)) # the values of my predictor at which I want a response prediction\n\n## Step Two: Make your response prediction\nmyPred &lt;- predict(object = bestMod3, # your model\n                  newdata = forPred, # the values of the predictors at which to make the prediction\n                  type = \"response\", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption\n                  se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\n### This can be added to our forPred data frame as:\nforPred$fit &lt;- myPred$fit\nforPred$se &lt;- myPred$se.fit\n\nforPred\n\n  Cont4      fit        se\n1     9 2113.604  82.54269\n2    22 5494.350 193.61366\n\n\nNote that you now get two predictions, one for Cont4 = 9 and one for Cont4 = 22:\n\nThe value of Resp3 when Cont4 = 9 units is 2114 ± 83 units.\nThe value of Resp3 when Cont4 = 22 units is 5494 ± 194 units.\n\nYou can add this to your plot with:\n\n# Set up your predictors for the visualized fit\nforCont4&lt;-seq(from = min(myDat3$Cont4), to = max(myDat3$Cont4), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  \n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod3, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat3, # add observations to your plot\n             mapping = aes(x = Cont4, y = Resp3)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont4, y = Fit),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont4, y = Upper),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont4, y = Lower),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ### NEW!  Adding your response prediction to the plot:\n  geom_point(data = forPred, # prediction data frame\n             mapping = aes(x = Cont4, y = fit), # coordinates of prediction\n             col = \"orange\", # change point colour\n             size = 3) + # change point size\n  \n  ### NEW!  Adding errorbars for your response prediction to the plot:\n  geom_errorbar(data = forPred, # prediction data frame\n             mapping = aes(x = Cont4, ymax = fit+se, ymin = fit-se), # coordinates of prediction\n             width = 1, # the width of the error bars\n             col = \"orange\") + # change point colour\n  \n  ylab(\"Resp3, (units)\") + # change y-axis label\n  \n  xlab(\"Cont4, (units)\") + # change x-axis label\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\n\nExample 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nHere’s a reminder of Example 4 (Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1):\n\n\n\n\n\n\n\n\n\n\nExample: What is the predicted response of Resp4 when Cont6 = 800 for Cat5 = Urban?\n\n## Step One: Define the predictor values for your response prediction\nforPred &lt;- data.frame(Cont6 = 800,\n                      Cat5 = \"Urban\") # the values of my predictor at which I want a response prediction\n\n## Step Two: Make your response prediction\nmyPred &lt;- predict(object = bestMod4, # your model\n                  newdata = forPred, # the values of the predictors at which to make the prediction\n                  type = \"response\", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption\n                  se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\n### This can be added to our forPred data frame as:\nforPred$fit &lt;- myPred$fit\nforPred$se &lt;- myPred$se.fit\n\nforPred\n\n  Cont6  Cat5      fit       se\n1   800 Urban 107.8597 1.247132\n\n\nSo the value of Resp4 when Cont6 = 800 units for Cat5 = Urban is 107.9 ± 1.2 units.\nYou can add this to your plot with:\n\n#### i) choosing the values of your predictors at which to make a prediction\n# Set up your predictors for the visualized fit\nforCat5&lt;-unique(myDat4$Cat5) # all levels of your categorical predictor\nforCont6&lt;-seq(from = min(myDat4$Cont6), to = max(myDat4$Cont6), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat5 = Cat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod4, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat4, # add observations to your plot\n             mapping = aes(x = Cont6, y = Resp4, col = Cat5)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont6, y = Fit, col = Cat5),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont6, y = Upper, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont6, y = Lower, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  \n    ### NEW!  Adding your response prediction to the plot:\n  geom_point(data = forPred, # prediction data frame\n             mapping = aes(x = Cont6, y = fit, col = Cat5), # coordinates of prediction\n             #col = \"black\", # change point colour\n             size = 3) + # change point size\n  \n  ### NEW!  Adding errorbars for your response prediction to the plot:\n  geom_errorbar(data = forPred, # prediction data frame\n             mapping = aes(x = Cont6, ymax = fit+se, ymin = fit-se,  col = Cat5), # coordinates of prediction\n             width = 20, # the width of the error bars\n             #col = \"black\"\n             ) + # change point colour\n  \n  ylab(\"Resp4, (units)\") + # change y-axis label\n  \n  xlab(\"Cont6, (units)\") + # change x-axis label\n  \n  labs(fill=\"Cat5, (units)\", col=\"Cat5, (units)\") + # change legend title\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\nExample: What is the predicted response of Resp4 when Cont6 = 800 for both Cat5 = Urban and Cat5 = Wild?\nNote the extra step with expand.grid() that allows you to quickly set up your predictor values for the prediction.\n\n## Step One: Define the predictor values for your response prediction\nforCat5 &lt;- c(\"Urban\", \"Wild\") # values of Cat5 for predictions\nforCont6 &lt;- 800 # value of Cont6 for prediction\n\nforPred &lt;- expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.\n\n## Step Two: Make your response prediction\nmyPred &lt;- predict(object = bestMod4, # your model\n                  newdata = forPred, # the values of the predictors at which to make the prediction\n                  type = \"response\", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption\n                  se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\n### This can be added to our forPred data frame as:\nforPred$fit &lt;- myPred$fit\nforPred$se &lt;- myPred$se.fit\n\nforPred\n\n   Cat5 Cont6      fit       se\n1 Urban   800 107.8597 1.247132\n2  Wild   800 119.8214 1.433306\n\n\nSo\n\nthe value of Resp4 when Cont6 = 800 units is both Cat5 = Urban is 107.9, 119.8 ± 1.2, 1.4 units.\nthe value of Resp4 when Cont6 = 800 units is both Cat5 = Wild is 107.86, 119.82 ± 1.25, 1.43 units.\n\nYou can add this to your plot with:\n\n#### i) choosing the values of your predictors at which to make a prediction\n# Set up your predictors for the visualized fit\nforCat5&lt;-unique(myDat4$Cat5) # all levels of your categorical predictor\nforCont6&lt;-seq(from = min(myDat4$Cont6), to = max(myDat4$Cont6), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod4, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat4, # add observations to your plot\n             mapping = aes(x = Cont6, y = Resp4, col = Cat5)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont6, y = Fit, col = Cat5),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont6, y = Upper, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont6, y = Lower, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  \n    ### NEW!  Adding your response prediction to the plot:\n  geom_point(data = forPred, # prediction data frame\n             mapping = aes(x = Cont6, y = fit, col = Cat5), # coordinates of prediction\n             #col = \"black\", # change point colour\n             size = 3) + # change point size\n  \n  ### NEW!  Adding errorbars for your response prediction to the plot:\n  geom_errorbar(data = forPred, # prediction data frame\n             mapping = aes(x = Cont6, ymax = fit+se, ymin = fit-se,  col = Cat5), # coordinates of prediction\n             width = 20, # the width of the error bars\n             #col = \"black\"\n             ) + # change point colour\n  \n  ylab(\"Resp4, (units)\") + # change y-axis label\n  \n  xlab(\"Cont6, (units)\") + # change x-axis label\n  \n  labs(fill=\"Cat5, (units)\", col=\"Cat5, (units)\") + # change legend title\n  \n  theme_bw() # change ggplot theme"
  },
  {
    "objectID": "DSPPH_SM_Predicting.html#inverse-predicting-for-models-with-a-binomial-error-distribution-assumption",
    "href": "DSPPH_SM_Predicting.html#inverse-predicting-for-models-with-a-binomial-error-distribution-assumption",
    "title": "Statistical Modelling: Predicting",
    "section": "Inverse predicting for models with a binomial error distribution assumption",
    "text": "Inverse predicting for models with a binomial error distribution assumption\nWhile the process (and code) above is identical regardless of your error distribution assumption, there is a second type of predicting one often uses when one has a model with a binomial error distribution assumption3.\nHere, you are “inverse predicting” to predict the value of your predictor that will lead to a certain probability of your response being 1.\nFor example, you might want to know the amount of a toxin (your predictor) that leads to a 25% probability of dying4, or the age (your predictor) at which a fish has a 50% probability of being mature5.\nOne method to do this would be to predict your response at values of your predictor in high enought resolution so that you can pick out the appropriate predictor value. Here is an example where you want to know the value of Cont4 that will give a 75% probability of Resp3.b being 1, given your bestmod3.b of Resp3.b ~ Cont4 + 1:\n\nforCont4&lt;-seq(from = min(myDat3.b$Cont4), to = max(myDat3.b$Cont4), by = 0.01)# a sequence of numbers in your continuous predictor range, notice that you are using a very high resolution series of your predictor here.\n\nforPred &lt;- expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.\n\n\n## Step Two: Make your response prediction\nmyPred &lt;- predict(object = bestMod3.b, # your model\n                  newdata = forPred, # the values of the predictors at which to make the prediction\n                  type = \"response\", # to make the prediction on the response scale. IMPORTANT if you have a non-normal error distributions assumption\n                  se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\nmyPredDF &lt;- data.frame(Cont4 = forCont4,\n                       fit = myPred$fit,\n                       se.fit = myPred$fit)\n\nmyPredDF$Diff &lt;- abs(myPred$fit - 0.75)\n\nmyInvPred &lt;- myPredDF[which(myPredDF$Diff == min(myPredDF$Diff)),]\n\nmyInvPred\n\n       Cont4       fit    se.fit         Diff\n5415 85.1791 0.7499013 0.7499013 9.868192e-05\n\n\nThe estimate of Cont4 that leads to a 75% probability of Resp3.b being 1 is 0.7 ± 0.7.\nAs you can see, this is a bit tedious. Another method of inverse predicting can be made with a function called dose.p() in the MASS package. Again, you might want to know the value of Cont4 that will give a 75% probability of Resp3.b being 1:\n\nsummary(bestMod3.b) # from this summary, you can see that the needed intercept is the first coefficient, and the slope is the second coefficient\n\n\nCall:\nglm(formula = Resp3.b ~ Cont4 + 1, family = binomial(link = \"logit\"), \n    data = myDat3.b)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5338  -0.4670   0.1316   0.4535   2.7394  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -8.1519     0.8916  -9.143   &lt;2e-16 ***\nCont4         0.1086     0.0116   9.359   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 415.77  on 299  degrees of freedom\nResidual deviance: 199.07  on 298  degrees of freedom\nAIC: 203.07\n\nNumber of Fisher Scoring iterations: 6\n\n#library(MASS) # load MASS package\n\n#myInvPred &lt;- dose.p(obj = bestMod3.b, # your model\n                    # cf = c(1,2), # the position in your coefficient list of the appropriate intercept and slope\n                    # p = 0.75) # the probability of Resp = 1 for your prediction\n\n#myInvPred\n\nREMOVED AFTER MASS ERROR The result is that we will have a 50% probability of Resp3.b being 1 if your Cont4 is r round(unname(myInvPred)[1],1) ± r as.numeric(round(attr(unname(myInvPred), “SE”),1)) units.\n\n\n\n\n\n\n\n\n\nThe dose.p() function is a nice one, but can get complicated when you have multiple predictors in your model. Tip: pay attention to your coefficients (the cf= arguement) if you do want to inverse predict with a model with more than one predictor."
  },
  {
    "objectID": "DSPPH_SM_Predicting.html#up-next",
    "href": "DSPPH_SM_Predicting.html#up-next",
    "title": "Statistical Modelling: Predicting",
    "section": "Up next",
    "text": "Up next\nYou did it! You made it through all the steps of our Statistical Modelling Framework. Nice work!\nUp next we will practice applying the framework to new examples, and learn how to communicate the whole process in reports and papers.\n- Use your model to make predictions (while considering prediction limits)\n“all data are wrong, but models can make data useful” Pershing"
  },
  {
    "objectID": "DSPPH_SM_Predicting.html#footnotes",
    "href": "DSPPH_SM_Predicting.html#footnotes",
    "title": "Statistical Modelling: Predicting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI realize that the term “prediction” and “predictor” can be confusing. Remember that another word for your model predictor is “covariate”.↩︎\nnote that when you hand a GLM object to predict(), R actually uses a function called predict.glm(). This does not affect what you need to do; it is just good to know in case you need help with the function↩︎\nrecall that this is also called “logistic regression”↩︎\nsometimes called \\(LD_{25}\\) where LD is lethal dose↩︎\nsometimes called the \\(A_{50}\\)↩︎"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html",
    "href": "DSPPH_DA_MissingValues.html",
    "title": "So you want to: deal with missing values",
    "section": "",
    "text": "Missing values are important and interesting, and they can affect how functions can be used on your data."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Apply functions to data containing missing values",
    "text": "Apply functions to data containing missing values\nWe can apply functions to objects or their components when missing values are present by letting R know what we want to do with the missing values. For example, getting the overall mean of the chlorophyll column without telling R how to handle missing values:\n\nmean(ChlData$Chl)\n\n[1] NA\n\n\nvs. telling R to remove them with the na.rm = ... argument:\n\nmean(ChlData$Chl, na.rm = TRUE)\n\n[1] 32.42375\n\n\nNote that the data frame itself remains unchanged, but R ignores the NAs when calculating the mean. We can find out more about how a particular function handles missing values by looking at the function’s help file (e.g. ?mean)."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "href": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Locating missing values",
    "text": "Locating missing values\nWe can also find missing values using the is.na() function:\n\nis.na(ChlData$Chl)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nand identify locations of missing values with:\n\nwhich(is.na(ChlData$Chl) == TRUE)\n\n[1] 5 8 9"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Removing missing values",
    "text": "Removing missing values\nFinally, we can remove all rows that are incomplete (i.e. containing any missing values) with the na.omit() function:\n\nhead(ChlData) # Original data frame\n\n  Station Year Month   Chl\n1     HL2 2007     2 16.07\n2     S27 2005    10 31.31\n3     HL2 2002    NA 40.38\n4     HL2 2001     2 32.00\n5     HL2 2001    10    NA\n6     S27 2007    10 29.71\n\nChlData &lt;- na.omit(ChlData) # Remove the NAs\n\nhead(ChlData) # Data frame without NAs\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nNote that above I reassign the output of the na.omit() function back to the name ChlData. This replaces the original data frame with the new data frame without missing values. I could also save it as a new object (with a new name) so the original is not overwritten.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTake a look at the numbers that print out to the left of the data frame:\n\nhead(ChlData)\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nThese are row names that were assigned when the data were read in. We can ignore row names but I wanted to to explain them as they can be distracting when one starts manipulating data frames. Unless we specify otherwise, rows are named by their original position when the data are read in to R, e.g. initially row #3 was assigned the name “3”, and row #4 was assigned the name “4”, etc. Since we’ve removed some rows with na.omit() above, the row names now skip from e.g. 2 to 4 (row 3 has been removed), but note that the 3rd row in the data frame can still be accessed with:\n\nChlData[3,]\n\n  Station Year Month Chl\n4     HL2 2001     2  32\n\n\nYou can choose your own row names with the rownames() function (or column names with the colnames() function) as well as with arguments in e.g. read.csv()."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html",
    "href": "DSPPH_DA_DatesTimes.html",
    "title": "So you want to: work with dates and times",
    "section": "",
    "text": "You will often have a time component to your biological research and research hypotheses - i.e. you might want to explain variability over time.. As mentioned in the Data Collection and Curation section, it is a good idea to record the time of your observation as separate columns of year, month, day, etc. But you will also need to work with data collected by others where date (and sometimes time) information is together one column (variable)."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "href": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "title": "So you want to: work with dates and times",
    "section": "In the base package:",
    "text": "In the base package:\nIn the base package which is installed along with R1, you can use the as.Date() function to format your data as a date:\nConsider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nmyDat$Time &lt;- as.Date(x = myDat$Time, # the date and time column\n                      format = \"%Y/%m/%d %H:%M\") # describing the format of the date and time column\n\nhead(myDat) # examine the first few rows\n\n        Time Value\n1 2024-10-03    36\n2 2020-06-09    35\n3 2020-10-07    57\n4 2021-10-09    43\n5 2020-06-04    44\n6 2021-12-02    36\n\n\nYou can learn more about the formatting syntax with ?strptime. Note that the code above replaces the myDat$Time column with the new, formatted date and time information.\n\nYou can extract parts of the date and time column with functions like months() for months, years() for years, etc.\n\nFor example:\n\nmyDat$Months &lt;- months(myDat$Time) # extract only the months\n\nstr(myDat) # structure of the data frame\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : Date, format: \"2024-10-03\" \"2020-06-09\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: chr  \"October\" \"June\" \"October\" \"October\" ..."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "href": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "title": "So you want to: work with dates and times",
    "section": "Using the lubridate package:",
    "text": "Using the lubridate package:\nThe lubridate package was created to make working with dates and times easier. There are still two steps to the process. You can repeat the steps above but now with the lubridate package.\nAgain, consider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nlibrary(lubridate) # load the lubridate package\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nmyDat$Time &lt;- ymd_hm(myDat$Time) # the date and time column\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  2 variables:\n $ Time : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value: int  36 35 57 43 44 36 47 39 38 50 ...\n\n\n\nYou can extract parts of the date and time column with functions like month() for months, year() for years, etc. Notice that the function names are not plural with the lubridate package.2\n\nFor example:\n\nmyDat$Months &lt;- month(myDat$Time) # extract only the months\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: num  10 6 10 10 6 12 3 4 11 11 ...\n\n\nMuch more is available in the lubridate packages, including determining durations and dealing with time-zones. Check the lubridate package “cheat sheet” for more information."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#footnotes",
    "href": "DSPPH_DA_DatesTimes.html#footnotes",
    "title": "So you want to: work with dates and times",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nno need to load a separate package↩︎\nBoth months() and month() will work with lubridate, but months() is also used to denote a duration of a month. See the cheat sheet for more.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "The Data Skills Portfolio (DSP) Program is based at Aarhus University’s Department of Biology.\nThe DSP Program is designed to help our Biology students develop robust and useful quantitative skills for biological research and many fields beyond.\nHere you will find a description of the program along with DSP handbook materials.\nExplore and enjoy!\nAnd remember: your feedback is welcome!\n\n\n\nto the Data Skills Portfolio Program Handbook for\n\nData Collection & Curation\nData Analysis\nStatistical Modelling"
  },
  {
    "objectID": "index.html#draft-website",
    "href": "index.html#draft-website",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "The Data Skills Portfolio (DSP) Program is based at Aarhus University’s Department of Biology.\nThe DSP Program is designed to help our Biology students develop robust and useful quantitative skills for biological research and many fields beyond.\nHere you will find a description of the program along with DSP handbook materials.\nExplore and enjoy!\nAnd remember: your feedback is welcome!"
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "to the Data Skills Portfolio Program Handbook for\n\nData Collection & Curation\nData Analysis\nStatistical Modelling"
  },
  {
    "objectID": "handbookDCIntro.html",
    "href": "handbookDCIntro.html",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Data curation is the collection and on-going management of data.\nGood data curation helps maintain data quality, and allows you and others to discover, access, and re-use your data. Data curation skills then are essential for ethical, transparent and reproducible science, and are an important part of your contribution to the field of biology.\n\n\nDepending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.\n\n\n\nThe individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns R can handle that, but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)\n\n\n\n\nFor more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "href": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Depending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#data-collection",
    "href": "handbookDCIntro.html#data-collection",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "The individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns R can handle that, but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#where-to-go-from-here",
    "href": "handbookDCIntro.html#where-to-go-from-here",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "For more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#footnotes",
    "href": "handbookDCIntro.html#footnotes",
    "title": "Data Collection & Curation Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthough you may have projects where you are working with databases↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html",
    "href": "DSPPH_SM_ModelValidation.html",
    "title": "Statistical Modelling: Validation",
    "section": "",
    "text": "In this section you will learn:\n\n\n\n\n\n\nwhy model validation is necessary\nhow to determine if your starting model can be used to test your hypothesis\nwhat to do if your model is misspecified"
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#why-do-you-need-to-validate-your-starting-model",
    "href": "DSPPH_SM_ModelValidation.html#why-do-you-need-to-validate-your-starting-model",
    "title": "Statistical Modelling: Validation",
    "section": "Why do you need to validate your starting model?",
    "text": "Why do you need to validate your starting model?\nAs we discussed before “All models are wrong, but some are useful”. So how do you tell if your starting model is a useful one; that is, one that can be used to test your hypothesis?\nAs discussed in the last section, when you choose your starting model you make an educated guess as to what a useful starting model might be, but you can only validate that it is a useful starting model after you have fit the model to your data.\nA useful model is one that reflects the mechanistic understanding of your research hypothesis (the deterministic part of your model - your shape assumption) as well as the nature of your observations (the stochastic part of your model - your error distribution assumption)1.\nIn addition, a useful model is one that conforms to the assumptions of the method you use to test your model (e.g. GLM). The assumptions of the GLM include that your predictors are not correlated with one another, and that your observations are independent of one another.\nIn this section, we will explore each of these assumptions by considering if your:\n\npredictors are correlated with one another,\nobservations are independent of one another,\nerror distribution assumption was adequate, and\nshape distribution assumption was adequate.\n\nBy considering these four points, you can determine if your model is “well-specified” to test your hypothesis.\nIn this section, we will go over tools that will help you determine if your model is well-specified and what to do if you find yourself with a misspecified starting model.\n\n\n\n\n\n\n“A well-specified model”\n\n\n\n\n\nNote that you are trying to find “a well-specified model”. This terminology reflects the fact that more than one model2 is likely appropriate to test your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#a-useful-residual---the-scaled-residual",
    "href": "DSPPH_SM_ModelValidation.html#a-useful-residual---the-scaled-residual",
    "title": "Statistical Modelling: Validation",
    "section": "A useful residual - the scaled residual",
    "text": "A useful residual - the scaled residual\nAs mentioned above, inspecting scaled residuals gives us a method of validating our model that is generally applicable - GREAT - but it also is a method that is intuitive: The scaled residual method checks to see if your model is useful (valid) by seeing if it can even produce data that looks like the data you used to fit your model (i.e. your observations). A model that is well-specified will be able to simulate data that looks like the data used to fix it.\nWe will estimate and explore scaled residuals using functions in the DHARMa package.\n\n\n\n\n\n\nDHARMa’s scaled residuals\n\n\n\n\n\n\nThe DHARMa package uses a simulation-based approach to estimate scaled residuals. These scaled residuals are standardized to a uniform distribution regardless of the model structure.\nHere’s how it works:\n\nDHARMa uses your starting model to simulate new response data for each observation (each row in your data frame). The default is that it simulates 250 new data sets from your model6.\nDHARMa uses the simulated values at each observation to calculate the empirical cumulative density function (ECDF) of the simulated values at that observation.\nThe scaled residual for observed data point i is then defined as the value of the ECDF at the value of the observed data i (see figure to the right).\nEstimated this way, if your model is wellspecified, the scaled residuals will always follow a uniform distribution, regardless of your starting model structure. Put another way: if the observed data were created from the same data-generating process of your starting model, all values of the cumulative distribution should appear with equal probability and the DHARMa residuals will be distributed uniformly.\n\nYou can explore more about the DHARMa package here."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#types-of-observation-dependence-and-what-you-can-do-about-them",
    "href": "DSPPH_SM_ModelValidation.html#types-of-observation-dependence-and-what-you-can-do-about-them",
    "title": "Statistical Modelling: Validation",
    "section": "Types of observation dependence and what you can do about them",
    "text": "Types of observation dependence and what you can do about them\nIt is relatively common to find violations of this assumption14. Violations can happen from:\n\ngrouping of your observations by a variable not in your hypothesis15\nobservations that are made at different times (temporal autocorrelation)\nobservations that are made at different locations (spatial autocorrelation)\n\n\nGrouping - a missing predictor\nGrouping occurs when you have observations dependent on one another due to some other variable not in your hypothesis. This may be a site variable (e.g. garden plots), or a variable that impacts your measurement (e.g. observer, or gear type), or could be related to repeated measurements (e.g. sampling the same individual multiple times).\n\nAn example\n\nAnother example: you might be exploring the effect of added fertilizer on plant height with your hypothesis being Height ~ Fertilizer, with Fertilizer being a categorical predictor with levels of “Fertilized” or “Control”. In making your observations, you measure 24 plant heights growing in a Fertilized or Control site. Oh, and these sites happened to be organized across six experimental plots.\nGiven your hypothesis that Height ~ Fertilizer, each measurement of plant height will be treated as an individual observation (replicate) with the assumption that, other than the Fertilizer treatment, these observations are independent of one another. However, this assumption is violated as the plants from the same plots may be more similar than plants from different plots (e.g. sunlight differences across plots may influence plant height, or water drainage differences across plots may influence the effect of the fertilizer). The Plot variable may be grouping your observations.\n\n#### How you know if you have a problem with observations dependent based on a grouping variable\nYou can find out if observation dependence is influencing your model by inspecting the residuals of your starting model. Here, you plot your residuals vs. variables not in your model that may be causing dependence in the observations. If you have a problem with observation dependence, there will be a pattern in your residuals when plotted against the offending variable.\nHere is an example of how to do this with our generic example started above. You can see how the residuals differ across levels of the Other variable also in our data set using violin plots:\n\nggplot()+ # start ggplot\n  geom_violin(data = myDat,\n              mapping = aes(x = Other, y = Resid))+ # add observations as a violin\n  geom_point(data = myDat,\n             mapping = aes(x = Other, y = Resid))+ # add observations as points\n  xlab(\"Other variable\")+ # y-axis label\n  ylab(\"Scaled residual\")+ # x-axis label\n  labs(caption = \"Figure 3: A comparison of model residuals vs. other variable\")+ # figure caption\n  theme_bw()+ # change theme of plot\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n\nor with a points plot, by colouring the residuals based on the level in Other16:\n\nggplot()+ # start ggplot\n  geom_point(data = myDat, # the data frame\n             mapping = aes(x = Cont1, y = Resid, col = Other), # add observations as points\n             size = 3)+ # change the size of the points\n  xlab(\"Cont1\")+ # y-axis label\n  ylab(\"Scaled residual\")+ # x-axis label\n  labs(caption = \"Figure 3: A comparison of model residuals vs. other variable\")+ # figure caption\n  theme_bw()+ # change theme of plot\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n\nNote that the spread of the residuals in each level (category) of Other is fairly equal in the violin plots and the colours are spread across the points plot. This indicates that Other is not causing much structure in the residuals, and likely is not causing the model to violate the assumption of independence.\nFinally, if you quantitative evidence that your observations are dependent on your grouping variable, you can test to see if residuals among the different groups have similar variances. This can be done with a Levene test for the homogeneity of variances through functions in the DHARMa package, e.g. \n\nlibrary(DHARMa) # load package\n\ntestCategorical(simOut, # the residuals \n                catPred = myDat$Other)$homogeneity # the grouping variable of concern\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   4  0.1112 0.9785\n      185               \n\nplotResiduals(simulationOutput = simOut, # compare simulated data to \n              form = myDat$Other, # our observations\n              asFactor = TRUE) # whether the variable plotted is a factor\n\n\n\n\n\n\n\n\nWhen your P-value is large (as it is here: P = 0.978), you would say that there is no evidence that the residual variance depends on the levels in Other (i.e. no concern for observations being dependent on Other).\n\n\nWhat can you do if your observations are grouped by something not in your hypothesis\nIf you find evidence of observation dependence, you can:\nAdd the grouping variable to your model as a (fixed effect) predictor: The assumption of observation independence means that your observations are independent of one another in all ways except for the predictors already included in your hypothesis. Therefore, an easy way to address observation dependence is to include the variable in your hypothesis, e.g. changing your original model:\nHeight ~ Fertilizer + 1\nto\nHeight ~ Fertilizer + Plot + Fertilizer:Plot + 1\nwould account for variability in plant height that was due to the plants growing in different plots (the main effect of Plot), as well as the influence of plot on the effect fertilizer has on the plants (the interaction Fertilizer:Plot).\nNote that adding Plot in this way is adding Plot as a “fixed effect” (vs. “random effect” - more on this below). In fact, every predictor we have been discussing up until now is in our model as something called a fixed effect.\nA couple of things to note when you add new fixed effects to your model:\n\nFixed effects influence the predicted mean of the model prediction, and so they are formerly part of your research hypothesis. This means that your hypothesis changes every time you add or remove a fixed effect. Your original hypothesis was\n\nHeight ~ Fertilizer + 1\nor that variability in plant height is explained by fertilizer addition. If you add Plot to your hypothesis to deal with observation dependence, your research hypothesis becomes:\nHeight ~ Fertilizer + Plot + Fertilizer:Plot + 1\nor that variability in plant height is explained by fertilizer addition, plot ID and the interaction between the two.\n\nWhen you add new fixed effects to your hypothesis, your model gets more “expensive” to fit. In the section on Hypothesis Testing, we will discuss how we can quantify the “benefit” of the model (increased explained deviance related to the likelihood of the model) vs. the cost of the model (how many coefficients have to be estimated). Adding new fixed effects increases the cost of fitting your model as more coefficients need to be estimated. This increased cost means you need bigger datasets (more observations) to fit the model. When you add a new continuous predictor to your hypothesis, there is one more coefficient to estimate (in a linear model, this is a slope). When you add a new categorical predictor to your hypothesis, you need to estimate another coefficient for each level in your new predictor. In our example, you would need to estimate 6 - 1 = 517 new coefficients to add the main effect of the Plot variable to the model, and another 6 - 1 = 5 coefficients to add the Fertilizer:Plot interaction to the model.\n\nBoth points above (that your hypothesis will change, and that your model will be more expensive to fit) means that just adding your grouping variable to deal with observation dependence is not always a great option. Instead you could,\nAdd the grouping variable to your model as a random effect using a mixed model: Many turn to mixed modelling as a way to deal with observation dependence. Mixed modelling18 is called “mixed” modelling because it includes a mix of both fixed effects (predictors that influence the model predicted mean) and random effects (predictors that influence the model predicted variance). Fitting mixed models is beyond the scope of this course, but I wanted you to have an idea of what mixed modelling is, and how to get started with mixed modelling, in case you want to use these methods in the future.\nTo fit mixed models in R, you need to adjust your hypothesis formula to tell R which predictors should be treated as fixed and which should be random. Recall that your model with only fixed effects:\nHeight ~ Fertilizer + Plot + Fertilizer:Plot + 1\nindicates that the predicted mean height is affected by fertilizer, plot ID and the interaction between the two, with separate coefficients associated with each plot ID. In mixed modelling, you could instead include Plot with:\nHeight ~ Fertilizer + (1|Plot) + (Fertilizer|Plot) + 1\nThe (1|Plot) term will estimate one (not 5) extra coefficient describing how the variance in average height (the intercept) varies when you move from plot to plot. The (Fertilizer|Plot) term will estimate one (not 5) extra coefficient describing how the variance in the effect of the fertilizer varies by plot.\nSo mixed modelling offers a way to address dependence in your observations that does not change your research hypothesis (as you are not adding fixed effects) and is cheaper (requiring less coefficient estimates as the random effects influence the modelled variance, not the mean).\nYou can fit mixed models in the lme4 package using the glmer() function (which stands for Generalized Linear Mixed Effects Models, or GLMM), with syntax that is very similar to what we have been using for GLMs, e.g.\n\nstartMod &lt;- glmer(formula = Height ~ Fertilizer + (1|Plot) + (Fertilizer|Plot) + 1,\n                  data = myDat,\n                  family = Gamma(link = \"inverse\"))\n\nWe won’t explore this further here.\nOne final thing to note is that random effects will always be categorical. If you have a continuous variable that is causing dependence in your observations, this variable can not be included as a random effect and must be included as a fixed effect.\nHere is a table if you are trying to determine if a variable causing dependence in your model should be included as a fixed or random effect:\n\n\n\n\n\n\n\nYour situation\nYour choice\n\n\n\n\n- the variable causing dependence is continuous- if you are interested in effect sizes of the variable on your response- if the factor levels are informative (vs. just numeric labels)\nfixed effect\n\n\n- the levels in the variable are only samples from a population of levels- if you have enough levels (at least 5) to estimate the variance of the effect due to your variable\nrandom effect\n\n\n\n(adapted from Crawley2013TheRBook)\n\n\n\nTemporal autocorrelation\nTemporal autocorrelation occurs when you measure your observations at different points in time.\nObservations collected closer together in time will be more similar than those collected further apart in time, and this could be happening independent of the mechanisms underlying your hypothesis. This topic is beyond the scope of this course, but I add a short description here so the idea is “on your radar” as you move forward in biology.\n\nAn example\nSuppose you want to test the hypothesis that variability in growth rate of newly hatched cod (torsk) in the Kattegat is explained by prey availability (Growth ~ Prey). The observations you are using to test this hypothesis have been collected over ~15 years, and it is likely that measurements made closer to each other in time are more similar than those made further apart from each other in time. This may be because the physical or biological environment was more similar in years that are close to each other in time (e.g. the parent population was similar, the temperature was similar) However, there is nothing in your hypothesis (Growth ~ Prey) to let R know which observations are close to each other in time. Thus, you violate the assumption of observation independence.\n\nWhen a model is fit to data and the observations are dependent on their sampling time, similar values closer in time are given too much weight on the model coefficients and the model fit is biased (compare lines in the plot on the right).\n\n\nHow you know if you have a problem with temporal autocorrelation\nYou can find out if observation dependence due to temporal autocorrelation is influencing your model by plotting your model residuals vs. time. If you have a problem with observation dependence, there will be a pattern in your residuals when plotted against the offending variable - in this case time.\nYou can also determine how large the problem of temporal autocorrelation is by estimating the autocorrelation function19 and the Durbin-Watson test. An easy way to test the latter is with the testTemporalAutocorrelation() function in the DHARMa package.\n\n\nWhat you do if your observations are influenced by temporal autocorrelation\nAs above, the assumption of observation independence means that your observations are independent of one another in all ways except for the predictors included in your hypothesis. We need to tell R about this temporal dependence in your model. This can be done by including time as a predictor in your model. Note that time will need to be a fixed effect as it is continuous, and will need to be modelled with a non-linear shape assumption20 to deal with the complicated form of the temporal autocorrelation.\nAlternatively, you can tell R about the correlation structure of the data directly (i.e. how similarity in observations changes as observations are further and further away from each other).\n### Spatial autocorrelation\n\nSimilar to the previous section, spatial autocorrelation describes the dependence among observations that are collected at different spatial locations. Observations measured close to each other in space are more (or less!) likely to be similar to one another than those measured further apart. In the plot on the right you can see examples of observation dependence on space in the dispersed and clustered drawings. In the dispersed example, observations closer to each other in space are less likely to resemble each other than one would expect if observations were distributed randomly in space (random example). In the clustered example, observations closer to each other in space are more likely to resemble each other than one would expect if observations were distributed randomly in space.\n\n#### An example\nFor example, you might be interested in how abundance of a species changes with mean environmental temperature and intend on testing the hypothesis that Abundance ~ Temperature. Measuring abundance over a large spatial area, you find that observations made closer to each other in space are more similar than those measured farther apart and part of this is due to effects other than temperature (e.g. other aspects of the environment such as food availability are more similar to each other for sites that are closer together). Without telling R information about where in space the observations were collected (remember, the hypothesis only includes Temperature), you violate the assumption of observation independence.\n\n\nHow you know if you have a problem with spatial autocorrelation\nYou can find out if observation dependence due to spatial autocorrelation is influencing your modelling by plotting your model residuals vs. location. As location is measured in two dimensions, you could try a bubble plot21 or variogram22 to help you look for patterns in your residuals with space. You can also estimate how big the problem of spatial autocorrelation is with Moran’s I test. The last can be done with the testSpatialAutocorrelation() function in the DHARMa package.\n\n\nWhat you do if your observations are influenced by spatial autocorrelation\nSimilarly to our discussion about temporal autocorrelation, the location of the observations (e.g. latitude and longitude) can be included in your model to account for the spatial autocorrelation23. Alternatively, the dependence among observations due to proximity can be included in the model as a spatial autocorrelation structure. \n\n\n\nA final point about observation independence\nRemember that you only have to worry about your observations being dependent based on variables not already in your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#footnotes",
    "href": "DSPPH_SM_ModelValidation.html#footnotes",
    "title": "Statistical Modelling: Validation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee course notes from last week if this is unclear↩︎\nbut not all models↩︎\nnotice I say “can be defined as…”. This is because there are many different definitions of a residual that have been made to deal with different model structures. We are going to use a very useful one - the scaled residuals - that will let you explore residuals for a wide range of models. More on this below!↩︎\nincluding those predictors inside your model, and possible predictors not included in your model. More on this to come!↩︎\nincluding those predictors inside your model, and possible predictors not included in your model. More on this to come!↩︎\nthis can be adjusted if needed↩︎\nsometimes also called multicollinearity↩︎\nremember, these are the modelled effects of your predictors on your response↩︎\nyour predictors are so correlated, in fact, we say they are “aliased” with one another↩︎\nthe error (variance) around the coefficient estimates is getting bigger (inflation)↩︎\nthe function we will use to calculate this will choose the correct estimate - either VIF or GVIF - depending on your model.↩︎\nthe model can not have interactions when estimating VIFs because an interaction term will always be correlated with the main effect predictor terms involved in the interaction. Ask in class if you have questions about this.↩︎\nin your own work, you will choose the predictor to remove based on your study goals and particular situation↩︎\nalso called pseudoreplication↩︎\nalso called nested sampling↩︎\nhere I plot the residuals vs. Cont1 but it could also be vs. the fitted values↩︎\nremember with categorical predictors, one factor level is included in the intercept↩︎\nalso called “multi-level modelling” or “hierarchical modelling”↩︎\ne.g. using the acf() function in the base stats package in R↩︎\nwe’ll talk about non-linear models in a couple of weeks↩︎\ncheck the sp package for more↩︎\ncheck the sgeotest package for more↩︎\nthis would be done with a non-linear model↩︎\nA quantile defines a particular part of a data set, e.g. the 90% quantile indicates the value where 90% of the values are less than the 90% quantile value. See your notes DSPH_SM_DataDistributions.html for more↩︎\n“Deviation n.s.” on the plot means that there is no significant deviation from expected↩︎\nNote that zero-inflated data (more zeroes than expected) appears similarly↩︎\nfrom the DHARMA package vignette↩︎"
  },
  {
    "objectID": "DSPPH_SM_Predictors.html",
    "href": "DSPPH_SM_Predictors.html",
    "title": "Statistical Modelling: Predictor(s)",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nDefine your predictor variables (what could explain the variability in your response?)\nPresent possible mechanisms behind your argument (why might your predictor variables explain variability in your response?\n\n\n\n\n\nOften very quickly, you will start to have ideas about what mechanistically might be responsible for the variation in your response. This is exciting! This is where you can apply biological theory to form your research hypothesis of why observations are as they appear. This step needs you to be curious, creative, and tap into your foundation of biological theory. And this step is where you identify the predictors in your statistical model.\nHere your focus is on the biological mechanisms (or processes) that you expect affect change in your response variable. An example of a mechanism affecting plant height might be temperature-dependent growth as temperature controls the rate of enzymatic reactions involved in plant growth.\nOnce you have identified a possible mechanism, you can identify a measurable factor that can be used to quantify that mechanism. This is a predictor. To follow our example, a corresponding predictor to measure an effect of temperature-dependent growth is ambient temperature.\nIt is necessary also to spend some time thinking about how you measure your predictor - what measure is relevant to your response variable? Here think about how measures of your predictor can be relevant to the time and space resolution of your response variable. To complete our example, you will want to measure ambient temperature quite close to each plant, and need to consider not just the temperature on the day the plant height was measured, but throughout the growing period of the plant (e.g. by considering average or integrated temperature measures).\nSo your response variable is the variability you are trying to explain, and your predictor(s) is what you think is causing the variability. In this course, we will use the term “predictors” but note that they are also known as “covariates”, “factors”, “independent variables”, “explanatory variables”, or “x variables”.\nIt is important that you let yourself think freely when you are considering what might by causing the variability in your response variable. At this early stage, do not restrict yourself to what you will be able to measure and test - let your curiosity and ideas range freely (called “blue-sky thinking”). Think first about all the mechanisms that may be responsible for the response variability. Then think about all the ways observations may be limited (e.g. limitations in our ability to measure certain variables or access data from certain places or times). And take lots of notes! As you move on in the framework, you will quickly simplify your hypothesis into what is measured and what is testable, but all your exciting ideas will be used in to communicate the scope of your study, motivate your predictor choice (in your Introduction and Methods), to put your results into context of greater biological theory, as well as direct future study efforts to focus on variation in your response that remains unexplained (in your Discussion section). Spending some time allowing yourself to brainstorm at this point is time well spent.\nFinally, notice that throughout this section, we emphasized mechanisms. You want to develop and test a hypothesis that is grounded in biological mechanisms."
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html",
    "href": "DSPPH_SM_Hypothesis.html",
    "title": "Statistical Modelling: Hypothesis",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nidentify your model terms (main effects and interactions)\npresent your research hypothesis in words and formula"
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html#hypotheses-with-more-than-one-predictor---including-interactions",
    "href": "DSPPH_SM_Hypothesis.html#hypotheses-with-more-than-one-predictor---including-interactions",
    "title": "Statistical Modelling: Hypothesis",
    "section": "Hypotheses with more than one predictor - including interactions",
    "text": "Hypotheses with more than one predictor - including interactions\nWhen you are considering more than one predictor, you need to consider the possibility of both main effects and interactions.\nA main effect represents the direct independent effect of the predictor on the response.\nAn interaction represents that the effect of one predictor on the response depends on the value of the other predictor.\nIf your research hypothesis states that\n\nVariability in the response (Resp) is explained by independent effects of two predictors (Pred1 and Pred2)1\n\nyour hypothesis in R syntax would be:\n\nResp ~ Pred1 + Pred2 + 1\n\nIf your research hypothesis states that\n\nVariability in the response (Resp) is explained by effects of two predictors (Pred1 and Pred2 with the effect of Pred1 depending on the value of Pred2)\n\nyour hypothesis in R syntax would be:\n\nResp ~ Pred1 + Pred2 + Pred1:Pred2 + 1\n\nNote that : is used to denote an interaction. In this case, it is a two-way interaction between Pred1 and Pred2.2\n\n\n\n\n\n\nTip\n\n\n\n\n\nR has a number of shortcuts for representing formulas in shortform. For example:\nResp ~ Pred1 + Pred2 + Pred1:Pred2 + 1\ncan be written as\nResp ~ Pred1*Pred2\ni.e. * means to include all main effects and all possible interactions.\nAnother example:\nResp ~ Pred1 + Pred2 + Pred3 + Pred1:Pred2 + Pred2:Pred3 + Pred1:Pred3 + 1\ncan be written as\nResp ~ (Pred1 + Pred2 + Pred3)^2\nwhich tells R to include all main effects and all possible two-way interactions between the three predictors."
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html#footnotes",
    "href": "DSPPH_SM_Hypothesis.html#footnotes",
    "title": "Statistical Modelling: Hypothesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome jargon here: when the effects of each predictor is independent of other predictors, you say the model is “additive”↩︎\nNote: to include an interaction all predictors involved in the interaction have to have a main effect also included in the model↩︎"
  },
  {
    "objectID": "handbookDAIntro.html",
    "href": "handbookDAIntro.html",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.\n\n\n\n\n\nA bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.\n\n\n\nHere we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.\n\n\n\n\nHere we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-tools",
    "href": "handbookDAIntro.html#a-note-on-tools",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "href": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "A bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#the-basics",
    "href": "handbookDAIntro.html#the-basics",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#so-you-want-to",
    "href": "handbookDAIntro.html#so-you-want-to",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "DSPPH_SM_Reporting.html",
    "href": "DSPPH_SM_Reporting.html",
    "title": "From statistical modelling to scientific report writing",
    "section": "",
    "text": "In this chapter you will learn:\n\n\n\n\n\n\nhow to report your hypothesis testing results (including visualizations) to communicate what your model says about your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#how-much-variation-in-your-response-is-explained-by-your-model",
    "href": "DSPPH_SM_Reporting.html#how-much-variation-in-your-response-is-explained-by-your-model",
    "title": "From statistical modelling to scientific report writing",
    "section": "how much variation in your response is explained by your model",
    "text": "how much variation in your response is explained by your model\nIf you will recall, your whole motivation for pursuing statistical modelling was to explain variation in your response. Thus, it is important that you quantify how much variation in your response you are able to explain by your model.\nNote that we will discuss this as “explained deviance” rather than “explained variation”. This is because the term “variance” is associated with models where the error distribution assumption is normal, whereas deviance is a more universal term.\nWhen you have a normal error distribution assumption and linear shape assumption, you can capture the amount of explained deviance simply by comparing the variance in your response (i.e. the starting variation before you fit your model - the null variation) vs. the variance in your model residuals (i.e. the remaining variation after you fit your model - the residual variation) as the \\(R^2\\):\n\\(R^2 = 1 - \\frac{residual variation}{null variation}\\)\nFrom this equation, you can see how, if your model is able to explain all the variation in the response, the residual variation will be zero, and \\(R^2 = 1\\). Alternatively, if the model explains no variation in the response the residual variation equals the null variation and \\(R^2 = 0\\).\nFor models with other error distribution and shape assumptions, you need another way of estimating the goodness of fit of your model. You can do this through a pseudo \\(R^2\\).\nOne useful pseudo \\(R^2\\) is called the Likelihood Ratio \\(R^2\\) or \\(R^2_{LR}\\). The \\(R^2_{LR}\\) compares the likelihood of your best-specified model to the likelihood of the null model:\n\\(R^2_{LR} = 1-exp(-\\frac{2}{n}(log𝓛(model)-log𝓛(null)))\\)\nwhere \\(n\\) is the number of observations, \\(log𝓛(model)\\) is the log-likelihood of your model, and \\(log𝓛(null)\\) is the log-likelihood of the null model. The \\(R^2_{LR}\\) is the type of pseudo \\(R^2\\) that shows up in your dredge() output when you add extra = \"R^2\" to the dredge() call. You can calculate \\(R^2_{LR}\\) by hand, read it from our dredge() output, or estimate it using r.squaredLR() from the MuMIn package:\n\nr.squaredLR(bestMod1)\n\n[1] 0.2644618\nattr(,\"adj.r.squared\")\n[1] 0.2646806\n\n\nNote here that two values of \\(R^2_{LR}\\) are reported. The adjusted pseudo \\(R^2\\) given here under attr(,\"adj.r.squared\") has been scaled so that \\(R^2_{LR}\\) can reach a maximum of 1, similar to a regular \\(R^2\\)1.\nLet’s compare this to the traditional \\(R^2\\):\n\n1-summary(bestMod1)$deviance/summary(bestMod1)$null.deviance\n\n[1] 0.2644618\n\n\nNote that the two estimates are similar: One nice feature of the \\(R^2_{LR}\\) is that it is equivalent to the regular \\(R^2\\) when our model assumes a normal error distribution assumption and linear shape assumption, so you can use \\(R^2_{LR}\\) for any of the models that we are discussing in class."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#how-important-each-of-your-predictors-is-in-explaining-that-variation",
    "href": "DSPPH_SM_Reporting.html#how-important-each-of-your-predictors-is-in-explaining-that-variation",
    "title": "From statistical modelling to scientific report writing",
    "section": "how important each of your predictors is in explaining that variation",
    "text": "how important each of your predictors is in explaining that variation\nWhen you have more than one predictor in your model. you may also want to report how relatively important each predictor is to explaining deviance in your response. This is also called “partitioning the explained deviance” to the predictors.\n\n\n\n\n\n\nAside: what we won’t be doing\n\n\n\n\n\nTo get an estimate of how much deviance in your response one particular predictor explains, you may be tempted to compute the explained deviance (\\(R^2\\)) estimates of models fit to data with and without that particular predictor. Let’s try with our Example 4:\n\ndredgeOut4\n\nGlobal model call: glm(formula = Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1, family = gaussian(link = \"identity\"), \n    data = myDat4)\n---\nModel selection table \n   (Int) Ct5       Cn6 Ct5:Cn6    R^2 df   logLik  AICc  delta weight\n8  98.35   + -0.002931       + 0.8557  7 -234.930 485.1   0.00      1\n4  92.25   +  0.009650         0.8166  5 -246.915 504.5  19.39      0\n2  96.93   +                   0.7923  4 -253.133 514.7  29.61      0\n3  94.92      0.017780         0.0848  3 -327.278 660.8 175.73      0\n1 104.00                       0.0000  2 -331.708 667.5 182.46      0\nModels ranked by AICc(x) \n\n\nIf you want to get an estimate as to how much response deviance the Cont6 predictor explains, you might compare the \\(R^2\\) of a model with and without the Cont6 predictor.\nLet’s compare comparing model #4 (that includes Cont6) and model #2 (that doesn’t include Cont6):\n\nR2.mod4 &lt;- (dredgeOut4$`R^2`[2]) # model #4 is the second row in dredgeOut4\nR2.mod2 &lt;- (dredgeOut4$`R^2`[3]) # model #2 is the third row in dredgeOut4\n\ndiffR2 &lt;- R2.mod4 - R2.mod2 # find estimated contribution of Cont to explained deviance\n\ndiffR2\n\n[1] 0.02429225\n\n\nSo it looks like 2.4% of the variability in Resp4 is explained by Cont6.\nBut what if instead you chose to compare model #3 (that includes Cont6) and model #1 (that doesn’t include Cont6):\n\nR2.mod3 &lt;- (dredgeOut4$`R^2`[4]) # model #3 is the fourth row in dredgeOut4\nR2.mod1 &lt;- (dredgeOut4$`R^2`[5]) # model #1 is the fifth row in dredgeOut4\n\ndiffR2 &lt;- R2.mod3 - R2.mod1 # find estimated contribution of Cont to explained deviance\n\ndiffR2\n\n[1] 0.08480009\n\n\nNow it looks like 8.5% of the variability in Resp4 is explained by Cont6! Quite a different answer! Your estimates of the contribution of Cont6 to explaining the response deviation don’t agree because of collinearity among our predictors2.\n\n\n\nThere are a few options you can use to get a robust estimate of how much each predictor is contributing to explained deviance in your response.\nOne option for partitioning the explained deviance when you have collinearity among your predictors is hierarchical partitioning. Hierarchical partitioning estimates the average independent contribution of each predictor to the total explained variance by considering all models in the candidate model set3. This method is beyond the scope of the course but see the rdacca.hp package for an example of how to do this.\nAnother method (that we will be using) for estimating the importance of each term (predictor or interaction) in your model is by again looking at the candidate model set ranking made by dredge(). Here you can measure the importance of a predictor by summing up the Akaike weights for any model that includes a particular predictor. The Akaike weight of a model compares the likelihood of the model scaled to the total likelihood of all the models in the candidate model set. The sum of Akaike weights for models including a particular predictor tells you how important the predictor is in explaining the deviance in your response. You can calculate the sum of Akaike weights directly with the sw() function in the MuMIn package:\n\nsw(dredgeOut4)\n\n                     Cat5 Cont6 Cat5:Cont6\nSum of weights:      1    1     1         \nN containing models: 3    3     1         \n\n\nHere we can see that all model terms (the predictors Cat5 and Cont6 as well as the interaction) are equally important in explaining the deviance in Resp4 (they appear in all models that have any Akaike weight).\nLet’s look at these two steps\n\nhow much deviance in your response is explained by your model\nhow important each of your predictors is in explaining that deviance\n\nwith our examples:\n\n\n\n\n\n\nExample 1: Resp1 ~ Cat1 + 1\n\n\n\n\n\nExample 1: Resp1 ~ Cat1 + 1\n\nhow much deviance in your response is explained by your model\n\n\nR2 &lt;- r.squaredLR(bestMod1)\n\nR2\n\n[1] 0.2644618\nattr(,\"adj.r.squared\")\n[1] 0.2646806\n\n\nThe best-specified model explains 26.4% of the deviance in Resp1.\n\nhow important each of your predictors is in explaining that deviance\n\n\ndredgeOut2\n\nGlobal model call: glm(formula = Resp2 ~ Cat2 + Cat3 + Cat2:Cat3, family = gaussian(link = \"identity\"), \n    data = myDat2)\n---\nModel selection table \n  (Int) Ct2 Ct3 Ct2:Ct3     R^2 df   logLik   AICc delta weight\n8 364.9   +   +       + 0.69800 13 -553.634 1137.5  0.00  0.964\n4 354.9   +   +         0.62540  7 -564.419 1144.1  6.55  0.036\n2 388.5   +             0.55300  5 -573.243 1157.1 19.63  0.000\n1 348.3                 0.00000  2 -613.508 1231.1 93.64  0.000\n3 331.1       +         0.03933  4 -611.502 1231.4 93.92  0.000\nModels ranked by AICc(x) \n\n\n\nsw(dredgeOut1)\n\n                     Cat1\nSum of weights:      1   \nN containing models: 1   \n\n\nWith Example 1, you have only one predictor (Cat1) and so this predictor is responsible for explaining all of the variability in your response (Resp1). You can see that it appears in all models with any weight with your sw() function from the MuMIn package.\n\n\n\n\n\n\n\n\n\nExample 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1\n\n\n\n\n\nExample 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1\n\nhow much deviance in your response is explained by your model\n\n\nR2 &lt;- r.squaredLR(bestMod2)\n\nR2\n\n[1] 0.6980474\nattr(,\"adj.r.squared\")\n[1] 0.6980507\n\n\nThe best-specified model explains 69.8% of the deviance in Resp2.\n\nhow important each of your predictors is in explaining that deviance\n\n\ndredgeOut2\n\nGlobal model call: glm(formula = Resp2 ~ Cat2 + Cat3 + Cat2:Cat3, family = gaussian(link = \"identity\"), \n    data = myDat2)\n---\nModel selection table \n  (Int) Ct2 Ct3 Ct2:Ct3     R^2 df   logLik   AICc delta weight\n8 364.9   +   +       + 0.69800 13 -553.634 1137.5  0.00  0.964\n4 354.9   +   +         0.62540  7 -564.419 1144.1  6.55  0.036\n2 388.5   +             0.55300  5 -573.243 1157.1 19.63  0.000\n1 348.3                 0.00000  2 -613.508 1231.1 93.64  0.000\n3 331.1       +         0.03933  4 -611.502 1231.4 93.92  0.000\nModels ranked by AICc(x) \n\n\n\nsw(dredgeOut2)\n\n                     Cat2 Cat3 Cat2:Cat3\nSum of weights:      1.00 1.00 0.96     \nN containing models:    3    3    1     \n\n\nHere you can see that Cat2 and Cat3 are equally important in explaining the deviance in Resp2 (they appear in all models that have any Akaike weight), while the interaction term between Cat2 and Cat3 is less important (only appearing in one model with Akaike weight, though this is the top model).\n\n\n\n\n\n\n\n\n\nExample 3: Resp3 ~ Cont4 + 1\n\n\n\n\n\nExample 3: Resp3 ~ Cont4 + 1\n\nhow much deviance in your response is explained by your model\n\n\nR2 &lt;- r.squaredLR(bestMod3)\n\nR2\n\n[1] 0.764852\nattr(,\"adj.r.squared\")\n[1] 0.764852\n\n\nThe best-specified model explains 76.5% of the deviance in Resp3.\n\nhow important each of your predictors is in explaining that deviance\n\n\ndredgeOut3\n\nGlobal model call: glm(formula = Resp3 ~ Cont4 + 1, family = gaussian(link = \"identity\"), \n    data = myDat3)\n---\nModel selection table \n  (Intrc) Cont4 df   logLik   AICc  delta weight\n2  -226.9 260.1  3 -811.080 1628.4   0.00      1\n1  2358.0        2 -883.457 1771.0 142.63      0\nModels ranked by AICc(x) \n\n\n\nsw(dredgeOut3)\n\n                     Cont4\nSum of weights:      1    \nN containing models: 1    \n\n\nWith Example 3, you have only one predictor (Cont6) and so this predictor is responsible for explaining all of the variability in your response (Resp3). You can see that it appears in all models with any weight with your sw() function from the MuMIn package.\n\n\n\n\n\n\n\n\n\nExample 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\n\n\n\n\n\nExample 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\n\nhow much deviance in your response is explained by your model\n\n\nR2 &lt;- r.squaredLR(bestMod4)\n\nR2\n\n[1] 0.855657\nattr(,\"adj.r.squared\")\n[1] 0.8567834\n\n\nThe best-specified model explains 85.6% of the deviance in Resp2.\n\nhow important each of your predictors is in explaining that deviance\n\n\ndredgeOut4\n\nGlobal model call: glm(formula = Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1, family = gaussian(link = \"identity\"), \n    data = myDat4)\n---\nModel selection table \n   (Int) Ct5       Cn6 Ct5:Cn6    R^2 df   logLik  AICc  delta weight\n8  98.35   + -0.002931       + 0.8557  7 -234.930 485.1   0.00      1\n4  92.25   +  0.009650         0.8166  5 -246.915 504.5  19.39      0\n2  96.93   +                   0.7923  4 -253.133 514.7  29.61      0\n3  94.92      0.017780         0.0848  3 -327.278 660.8 175.73      0\n1 104.00                       0.0000  2 -331.708 667.5 182.46      0\nModels ranked by AICc(x) \n\n\n\nsw(dredgeOut4)\n\n                     Cat5 Cont6 Cat5:Cont6\nSum of weights:      1    1     1         \nN containing models: 3    3     1         \n\n\nHere you can see that Cat5, Cont6 and the interaction Cat5:Cont6 are all equally important in explaining the deviance in Resp4 (they appear in all models that have any Akaike weight)."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#visualizing-your-model-effects",
    "href": "DSPPH_SM_Reporting.html#visualizing-your-model-effects",
    "title": "From statistical modelling to scientific report writing",
    "section": "Visualizing your model effects",
    "text": "Visualizing your model effects\n\nThis section is similar regardless of your model structure (e.g. error distribution assumption). The examples below all assume a normal error distribution assumption but you can use the same process for models of any structure.\n\nHere, you will visualize how each predictor is affecting the response by drawing how your modelled fitted values (the estimates of your response made by the model) change with your predictors, along with a measure of uncertainty around those fitted values. You also want to include your data (actual observations of your response and predictors) so you can start to communicate how well your model captures your hypothesis, and what amount of deviance in your response is explained by your model.\nThere are a lot of different R packages that make it easy to quickly visualize your model. We will focus on two methods that will allow you to make quick visualizations of your model and/or customize the figures as you would like.\n\nvisualizing using the visreg package: This will let you get a quick look at your model object with a limited amount of customization.\nvizualizing “by hand”: Here, “by hand” is a bit of a silly description as R will be doing the work for you. What I mean by “by hand” is that you will be building the plot yourself by querying your model object. This method is very flexible and will lead you to a fully customizable visualization. This process involves i) choosing the values of your predictors at which to make estimates of your fitted values, ii) using predict() to use your model to estimate your response variable at those values of your predictors, and iii) use the model estimates to plot your model fit. Aside: this is similar to what we will be doing when we learn about model predictions in the next chapter of these notes.\n\nBelow are visualizations for each of our examples.\n\n\n\n\n\n\nExample 1: Resp1 ~ Cat1 + 1\n\n\n\n\n\nExample 1: Resp1 ~ Cat1 + 1\nVisualizing with the visreg package\n\nlibrary (visreg) # load visreg package for visualization\n\nlibrary(ggplot2) # load ggplot2 package for visualization\n\nvisreg(bestMod1, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cat1\", # predictor on x-axis\n       #by = ..., # if you want to include a 2nd predictor plotted as colour\n       #breaks = ..., # if you want to control how the colour predictor is plotted\n       #cond = , # if you want to include a 3rd predictor\n       #overlay = TRUE, # to plot as overlay or panels, when there is \n       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals\n       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat1, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nUnfortunately due to limitations of the visreg package, you can not easily add you observations onto plots where the x-axis is a categorical predictor. But that’s ok, because there are other options…\nVisualizing by hand\nTo plot by hand, you will\n\nfirst make a data frame containing the value of your predictors at which you want to plot effects on the response:\n\n\n# Set up your predictors for the visualized fit\nforCat1&lt;-unique(myDat1$Cat1) # every value of your categorical predictor\n\n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat1=forCat1) # expand.grid() function makes sure you have all combinations of predictors\n\n\nNext, you will use the predict() function4 to get the model estimates of your response variable at those values of your predictors:\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod1, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n\nFinally, you will use these model estimates to make your plot:\n\n\nlibrary(ggplot2)\n\nggplot() + # start ggplot\n  geom_point(data = myDat1, # add observations to your plot\n             mapping = aes(x = Cat1, y = Resp1), \n             position=position_jitter(width=0.1)) + # control position of data points so they are easier to see on the plot\n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat1, y = Fit, ymin = Lower, ymax = Upper),\n              linewidth=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat1, y = Fit), \n             shape = 22, # shape of point\n             size = 3, # size of point\n             fill = \"white\", # fill colour for plot\n             col = 'black') + # outline colour for plot\n  ylab(\"Resp1, (units)\")+ # change y-axis label\n  xlab(\"Cat1, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1\n\n\n\n\n\nExample 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1\nVisualizing with the visreg package\nAs you have more than one predictor in Example 2, there are a number of ways you can visualize your effects.\nHere is an example of a visualization with Cat2 on the x-axis and the effects of Cat3 plotted as separate panels:\n\nlibrary (visreg) # load visreg package for visualization\n\nlibrary(ggplot2) # load ggplot2 package for visualization\n\nvisreg(bestMod2, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cat2\", # predictor on x-axis\n       by = \"Cat3\", # if you want to include a 2nd predictor plotted as colour\n       #breaks = ..., # if you want to control how the colour predictor is plotted\n       #cond = , # if you want to include a 3rd predictor\n       overlay = FALSE, # to plot as overlay or panels, when there is \n       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals\n       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat2, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n# tip: we can't include our observations on this plot due to the limitations of the visreg package when plotting with a categorical predictor on the x-axis\n\nAnd here is an example of a visualization with Cat2 on the x-axis and the effects of Cat3 overlayed on the plot as different colours for each category (level) of Cat3:\n\nvisreg(bestMod2, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cat2\", # predictor on x-axis\n       by = \"Cat3\", # if you want to include a 2nd predictor plotted as colour\n       #breaks = ..., # if you want to control how the colour predictor is plotted\n       #cond = , # if you want to include a 3rd predictor\n       overlay = TRUE, # to plot as overlay or panels, when there is \n       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals\n       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat2, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n# tip: we can't include our observations on this plot due to the limitations of the visreg package when plotting with a categorical predictor on the x-axis\n\nUnfortunately due to limitations of the visreg package, you can not easily add you observations onto plots where the x-axis is a categorical predictor. But that’s ok, because there are other options…\nVisualizing by hand\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n# Set up your predictors for the visualized fit\nforCat2&lt;-unique(myDat2$Cat2) # every level of your categorical predictor\nforCat3&lt;-unique(myDat2$Cat3) # every level of your categorical predictor\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat2 = forCat2, Cat3 = forCat3) # expand.grid() function makes sure you have all combinations of predictors\n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod2, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  geom_point(data = myDat2, # add observations to your plot\n             mapping = aes(x = Cat2, y = Resp2, col = Cat3), \n             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot\n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat2, y = Fit, ymin = Lower, ymax = Upper, col = Cat3),\n              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot\n              size=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat2, y = Fit, fill = Cat3), \n             shape = 22, # shape of point\n             size=3, # size of point\n             col = 'black', # outline colour for point\n             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot\n  ylab(\"Resp2, (units)\")+ # change y-axis label\n  xlab(\"Cat2, (units)\")+ # change x-axis label\n  labs(fill=\"Cat3, (units)\", col=\"Cat3, (units)\") + # change legend title\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3: Resp3 ~ Cont4 + 1\n\n\n\n\n\nExample 3: Resp3 ~ Cont4 + 1\nVisualizing with the visreg package\nNotice how you can include the gg = TRUE argument to plot this as a ggplot type figure. This allows you to add your data onto the visualization of your model.\n\nlibrary (visreg) # load visreg package for visualization\n\nlibrary(ggplot2) # load ggplot2 package for visualization\n\nvisreg(bestMod3, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cont4\", # predictor on x-axis\n       #by = ..., # if you want to include a 2nd predictor plotted as colour\n       #breaks = ..., # if you want to control how the colour predictor is plotted\n       #cond = , # if you want to include a 3rd predictor\n       #overlay = TRUE, # to plot as overlay or panels, when there is \n       #rug = FALSE, # to turn off the rug. The rug shows you where you have positive (appearing on the top axis) and negative (appearing on the bottom axis) residuals\n       gg = TRUE)+ # to plot as a ggplot, with ggplot, you will have more control over the look of the plot.\n  geom_point(data = myDat3,\n             mapping = aes(x = Cont4, y = Resp3))+\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cont4, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n# Note: you CAN include your observations on your plot with visreg when you have a continuous predictor on the x-axis\n\nVisualizing by hand\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n\n# Set up your predictors for the visualized fit\nforCont4&lt;-seq(from = min(myDat3$Cont4), to = max(myDat3$Cont4), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod3, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat3, # add observations to your plot\n             mapping = aes(x = Cont4, y = Resp3)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont4, y = Fit),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont4, y = Upper),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont4, y = Lower),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ylab(\"Resp3, (units)\") + # change y-axis label\n  \n  xlab(\"Cont4, (units)\") + # change x-axis label\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\n\n\n\n\n\nExample 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nVisualizing with the visreg package\nAs you will see in @sec_quantifying, there is no evidence of an effect of Cont6 on Resp3 when Cat5 = Farm - i.e. the coefficient (slope) is not different from zero. In such cases, you would typically not include the effect on the plot (red line in the next figure), but I include it here to illustrate the visualization techniques.\nA plot with Cont6 on the x-axis:\n\nlibrary (visreg) # load visreg package for visualization\n\nlibrary(ggplot2) # load ggplot2 package for visualization\n\nvisreg(bestMod, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cont6\", # predictor on x-axis\n       by = \"Cat5\", # predictor plotted as colour\n       #breaks = 3, # if you want to control how the colour predictor is plotted\n       #cond = , # if you want to include a 4th predictor\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  geom_point(data = myDat4, # data\n             mapping = aes(x = Cont6, y = Resp4, col = Cat5))+ # add data to your plot\n  #ylim(0, 60)+ # adjust the y-axis units\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cont6, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nA plot with Cat5 on the x-axis: Notice how the continuous predictor is represented by different levels in the colours on the plot. Here you’ve asked for three levels with breaks = 3. Note that you can not include your observations on the visreg plot when the x-axis predictor is a category\n\nvisreg(bestMod, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cat5\", # predictor on x-axis\n       by = \"Cont6\", # predictor plotted as colour\n       breaks = 3, # if you want to control how the colour predictor is plotted\n       #cond = , # if you want to include a 4th predictor\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat5, (units)\")+ # change x-axis label\n  labs(fill=\"Cont6, (units)\", col=\"Cont6, (units)\") + # change legend title\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nYou can also specify at which levels the breaks should occur with the breaks = ... argument. For example, you can ask visreg to plot the modelled effects when Cont6 = 400 and Cont6 = 600:\n\nvisreg(bestMod, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cat5\", # predictor on x-axis\n       by = \"Cont6\", # predictor plotted as colour\n       breaks = c(400,600), # if you want to control how the colour predictor is plotted\n       #cond = , # if you want to include a 4th predictor\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat5, (units)\")+ # change x-axis label\n  labs(fill=\"Cont6, (units)\", col=\"Cont6, (units)\") + # change legend title\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nNote that you can not include your observations on the visreg plot when the x-axis predictor is a category.\nVisualizing by hand\nHere’s an example with Cont6 on the x-axis:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n\n# Set up your predictors for the visualized fit\nforCat5&lt;-unique(myDat4$Cat5) # all levels of your categorical predictor\nforCont6&lt;-seq(from = min(myDat4$Cont6), to = max(myDat4$Cont6), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod4, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat4, # add observations to your plot\n             mapping = aes(x = Cont6, y = Resp4, col = Cat5)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont6, y = Fit, col = Cat5),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont6, y = Upper, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont6, y = Lower, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ylab(\"Resp4, (units)\") + # change y-axis label\n  \n  xlab(\"Cont6, (units)\") + # change x-axis label\n  \n  labs(fill=\"Cat5, (units)\", col=\"Cat5, (units)\") + # change legend title\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nHere’s an example with Cat5 on the x-axis:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n# Set up your predictors for the visualized fit\nforCat5&lt;-unique(myDat4$Cat5) # all levels of your categorical predictor\nforCont6&lt;-c(400, 600) # a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod4, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat4, # add observations to your plot\n             mapping = aes(x = Cat5, y = Resp4, col = Cont6), \n             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot\n  \n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat5, y = Fit, ymin = Lower, ymax = Upper, col = Cont6),\n              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot\n              size=1.2) + # control thickness of errorbar line\n  \n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat5, y = Fit, fill = Cont6), \n             shape = 22, # shape of point\n             size=3, # size of point\n             col = 'black', # outline colour for point\n             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot\n  \n  ylab(\"Resp4, (units)\")+ # change y-axis label\n  \n  xlab(\"Cat5, (units)\")+ # change x-axis label\n  \n  labs(fill=\"Cont6, (units)\", col=\"Cont6, (units)\") + # change legend title\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside: Plotting in 3D\n\n\n\n\n\nYou might notice that many of the plots above are communicating 3-dimensions (one response + two predictors) in a 2-dimensional plot. There are other ways of making 3-dimensional plots in R, e.g. with the visreg package using the visreg2d() function in the visreg package:\n\nvisreg2d(fit = bestMod4, # your model\n         xvar = \"Cont6\", # what to plot on the x-axis\n         yvar = \"Cat5\", # what to plot on the y-axis\n         scale = \"response\") # make sure fitted values (colours) are on the scale of the response\n\n\n\n\n\n\n\n\nor “by hand” using the geom_raster() function in the ggplot2 package:\n\n# Set up your predictors for the visualized fit\nforCont6&lt;-seq(from = min(myDat4$Cont6), \n             to = max(myDat4$Cont6), \n             by = 0.1) # e.g. a sequence of Cont values\nforCat5&lt;-unique(myDat4$Cat5) # every value of your categorical predictor\n\n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cont6=forCont6, Cat5=forCat5) # expand.grid() function makes sure you have all combinations of predictors\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod4, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n# create your plot\nggplot() + # start your ggplot\n  geom_raster(data = forVis, aes(x = Cont6, y = Cat5, fill = Fit))+ # add the 3 dimensions as a raster\n  geom_point(data = myDat4, aes(x = Cont6, y = Cat5, colour = Resp)) # add your data\n\n\n\n\n\n\n\n\nEXTRA: As you can see, these plots represent the 3rd dimension by using colour. You can also make actual 3 dimensional plots in R with the plotly package. These plots are interactive which makes them more useful than static 3d plots. Click on the plot and move your mouse to rotate the plot!\n\nlibrary(plotly) # load the plotly package\n\nplot_ly(data = forVis, # the data with your model predictions (made above)\n        x = ~Cont6, # what to plot on the x axis\n        y = ~Cat5, # what to plot on the y axis\n        z = ~Fit, # what to plot on the z axis\n        type=\"scatter3d\", # type of plot\n        mode=\"markers\") %&gt;% # type of plot\n  add_markers(data = myDat4, x = ~Cont6, y = ~Cat5, z = ~Resp4) # add your data"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#sec_quantifying",
    "href": "DSPPH_SM_Reporting.html#sec_quantifying",
    "title": "From statistical modelling to scientific report writing",
    "section": "Quantifying your model effects",
    "text": "Quantifying your model effects\n\nHow you quantify your model effects varies with your model structure. If it is your first time reading this, read through the examples that present models assuming a normal error distribution assumption. This will help you understand why we are reporting modelled effects in this way. Then, (if relevant) look at the section on the error distribution assumption you are interested in for your model.\n\nIn this section, you will learn how to report your modelled effects in numbers. In general, this section will involve answering:\n\nWhat are your modelled effects (with uncertainty)?\n\nIf you also have a categorical predictor with more than two levels, you will also want to answer:\n\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\n\nLet’s examine each of these in turn:\n\nWhat are your modelled effects (with uncertainty)?\nRecall from the text above that your model effects describe the change in your response given a change in your predictor.\nWhen you have a continuous predictor, this effect is the the amount of change in the response that is caused by a unit change in your continuous predictor. Information about this effect is captured in the coefficient (slope) of the linear fit. Expand the text box below to see why.\n\n\n\n\n\n\nthe math\n\n\n\n\n\nHere’s a model equation for a generalized linear model:\n\\[\n\\begin{align}\ng(\\mu_i|Pred_i) &= \\beta_1 \\cdot Pred_i + \\beta_0 \\\\\nResp_i &\\sim F(\\mu_i)\n\\end{align}\n\\]\nwhere * \\(g()\\) is the link function * \\(\\mu_i\\) is the mean fitted value \\(\\mu_i\\) dependent on \\(Pred_i\\) * \\(\\beta_1\\) is the coefficient of \\(Pred_i\\) * \\(Pred_i\\) is the value of your predictor * \\(\\beta_0\\) is the intercept * \\(Resp_i\\) is the value of the response variable * \\(F\\) represents some error distribution assumption\nAs above, the effect of a continuous predictor on your response is the change in your fitted value (\\(g(\\mu_i)\\)) for a unit change in your predictor (\\(Pred_i\\)):\n\\[\n\\begin{align}\ng(\\mu|Pred+1) - g(\\mu|Pred) &= (\\beta_1 \\cdot (Pred+1) + \\beta_0) - (\\beta_1 \\cdot Pred_i + \\beta_0) \\\\\n&=\\beta_1 \\cdot Pred  + \\beta_1 + \\beta_0 - \\beta_1 \\cdot Pred_i - \\beta_0 \\\\\n&=\\beta_1\n\\end{align}\n\\] For linear models with a normal error distribution assumption (i.e. when you are using link = \"identity\"), the link function \\(g()\\) is not there. For this reason, you can read the effects of your model directly from the model coefficients (\\(\\beta_1\\), \\(\\beta_0\\)) when you have a normal error distribution assumption. And for this reason, you will need to convert your model coefficients to quantify your modelled effects when you have an error distribution assumption that is not normal/uses a link function that isn’t “identity”.\n\nThis is because of the difference between the “link” scale and “response” scale for generalized linear models.\nThe link scale is where the model fitting happens, and where the evidence for your modelled effects is gathered.\nThe response scale is back in the real world - these are the actual effects in your response you would expect to see with a change in your predictor. \nRecall that you can see the default link functions associated with each error distribution function with ?family:\n\n\n\n\nWhen you have a categorical predictor, the coefficient represents the intercept of the linear fit and how that intercept changes as you move from one category to another. So if you have a categorical predictor with 3 categories (levels), you will have three coefficients that describe how the mean of your response changes as you move from category to category.\nWhen you have an interaction among your predictors, you will have one coefficient for each combination in your interaction.\nFinally, you need to report the uncertainty around your modelled effects so your peers know how much evidence there is for these modelled effects. There are a number of ways to do this. Two that we will use are i) reporting standard errors (SE) which are a measure of uncertainty of the average modelled effect, and ii) the 95% confidence intervals around your modelled effects, which tell you the range your modelled effects would be expected to fall into (with a 95% probability) if you redid your experiment.\nThese principles are true regardless of your model structure BUT our interpretation of what the effects mean in the real world will vary based on e.g. your error distribution assumptions (see the text box above for an explanation). In the examples below, you will learn how to quantify your modelled effects and uncertainty estimates for models with a normal error distribution assumption. There are sections below to show you how to do the same for models with other error distribution assumptions.\nFor a quick summary, here is how the method depends on your modelled error distribution assumption:\n\n\n\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors)\nWhen you have a categorical predictor in your best-specified model, you know there is evidence of an effect for at least one of your levels (categories). This step will tell you where you have evidence that the effects among your factor levels differ. The processes outlined below are the same for any model structure.\n\n\nModels with a normal error distribution assumption:\n\n\n\n\n\n\nExample 1: Resp1 ~ Cat1 + 1\n\n\n\n\n\nExample 1: Resp1 ~ Cat1 + 1\nRecall that Example 1 contains only one predictor and it is categorical:\nResp1 ~ Cat1 + 1\nWhat are your modelled effects (with uncertainty)?\nIn the chapter on your Starting Model, you found your coefficients in the “Estimate” column of the summary() output of your model:\n\ncoef(summary(bestMod1)) # extract the coefficients from summary()\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 20.954839   1.316231 15.920341 8.313153e-29\nCat1Sp2      5.876740   1.773637  3.313384 1.296521e-03\nCat1Sp3     -4.509677   1.861431 -2.422694 1.726174e-02\n\n\nInterpreting the coefficients from this output takes practice - especially for categorical predictors because of the way that R treats categorical predictors in regression. With R’s “dummy coding”, one level of the predictor (here Sp1) is incorporated into the intercept estimate (21) as the reference level. The other coefficients in the Estimate column represent the change in modelled response when you move from the reference level (Sp1) to another level.\nThe modelled response when Cat1 = Sp2 is (5.9) units higher than this reference level = 21 + 5.9 = 11.8 units.\nThe modelled Resp1 when Cat1 = Sp3 is -4.5 units lower than the reference level = 21 + 5.9 = 1.4 units.5\nSo all the information we need is in this summary() output, but not easy to see immediately. An easier way is to use the emmeans6 package which helps you by reporting the coefficients directly. In our case, you use the emmeans package to get the mean value of the response at each level of the predictor. For categorical predictors, you do this with the emmeans() function:\n\nlibrary(emmeans) # load the emmeans package\n\nemmOut &lt;- emmeans(object = bestMod1, # your model\n            specs = ~ Cat1, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat1 emmean   SE df lower.CL upper.CL\n Sp1    21.0 1.32 97     18.3     23.6\n Sp2    26.8 1.19 97     24.5     29.2\n Sp3    16.4 1.32 97     13.8     19.1\n\nConfidence level used: 0.95 \n\n\nSo now you have a modelled value of your response for each level of your categorical predictor - this captures the effect of your categorical predictor on your response. You also need to report uncertainty around this effect, and you can do this by reporting the standard error (SE) also reported by the emmeans() function.\nWhen Cat1 is Sp1, Resp1 is estimated to be 21 ± 1.3 units.\nWhen Cat1 is Sp2, Resp1 is estimated to be 26.8 ± 1.2 units.\nWhen Cat1 is Sp3, Resp1 is estimated to be 16.4 ± 1.3 units.\nNote that this is the same information in the summary() output just easier to read.7\nNote also that two types of uncertainty are measured here. SE stands for the standard error around the prediction, and is a measure of uncertainty of the average modelled effect. The lower.CL and upper.CL represent the 95% confidence limits of the prediction - so if I observed a new Resp1 at a particular Cat1, there would be a 95% chance it would lie between the bounds of the confidence limits.\nFinally, note that you can also get a quick plot of the effects by handing the emmeans() output to plot().\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nSince you have a categorical predictor with more than two levels, you will ask if all levels of Cat1 affect Resp1 in the same way - i.e. are the coefficients across factor levels significantly different from one another?\nTo get evidence about how each level affects your response, you need to test which effects differ from one another among the categories (levels) of your categorical predictor. This is done using multiple comparisons, i.e. you will compare the modelled effect of each level of your categorical predictor vs. each other level of your categorical predictor to determine which are different from each other (called pairwise testing).\nThis is done by testing the null hypothesis that the modelled effects of each pair are similar to one another, typically rejecting the null hypothesis when P &lt; 0.05. Remember that the P-value is the probability that you observe a value at least as big as the one you observed even though our null hypothesis is true. In this case, you are looking at the value you are observing is the difference between coefficients estimated for two levels of your predictor. The P-value is the probability of getting a difference at least as big as the one you observed even though there is actually no difference between the coefficients (the null hypothesis is true).\nA couple of things to note about multiple comparison testing:\n\nMultiple comparison testing on a categorical predictor should only be done after your hypothesis testing has given you evidence that the predictor has an effect on your response. That is, you should only do a multiple comparison test on a predictor if that predictor is in your best-specified model. As this is a test done after your hypothesis testing, it is called a post-hoc8 test.\nMultiple comparison testing can be a problem because you are essentially repeating a hypothesis test many times on the same data (i.e. are the effects of Sp1 different than those of Sp2? are the effects of Sp1 different than those of Sp3? are the effects of Sp2 different than those of Sp3?…). These repeated tests mean there is a high chance that you will find a difference in one test purely due to random chance, vs. due to there being an actual difference. To account for this, the multiple comparison tests you will perform have been formulated to correct for this increased error.\n\nMultiple comparison testing is very simple with the emmeans package. It just requires you to hand the output from emmeans() to a function called pairs():\n\nforComp &lt;- pairs(emmOut)\n\nforComp\n\n contrast  estimate   SE df t.ratio p.value\n Sp1 - Sp2    -5.88 1.77 97  -3.313  0.0037\n Sp1 - Sp3     4.51 1.86 97   2.423  0.0451\n Sp2 - Sp3    10.39 1.77 97   5.856  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThe output shows the results of the multiple comparison (pairwise) testing. The values in the p.value column tell you the results of the hypothesis test comparing the coefficients between the two levels of your categorical predictor. For example, for Sp1 vs. Sp2, there is a 0.37% (p = 0.0037) probability of getting a difference in coefficients at least as big as 26.8 - 21 = r 5.8 even though the null hypothesis (no difference) is true. This value 0.37% (P = 0.0037) is small enough that you can say you have evidence that the effect of Cat1 on Resp1 is different for these two different levels (Sp1andSp2`).\nBased on a threshold p-value of 0.05, we can see that:\nThere is evidence that the value of Resp1 when Cat1 is Sp1 is different (lower) than that when Cat1 is Sp2 as P = 0.0037 is less than P = 0.05.\nThere is a little evidence that the value of Resp1 when Cat1 is Sp1 is different (higher) than that when Cat1 is Sp3 as P = 0.0451 is less than P = 0.05 (but it is close!). There is some evidence that the value of Resp1 when Cat1 is Sp2 is different (higher) than that when Cat1 is Sp3 as P &lt; 0.00019 is smaller than P = 0.05.10.\nNote that the p-values have been adjusted via the Tukey method which adjusts the difference that the two coefficients need to have to allow for the fact that we are making multiple comparisons.11\nNote that you can also get the results from the pairwise testing visually by handing the output of pairs() to plot().\nNote that the difference between Resp1 when Cat1 is Sp1 vs. Sp3 is so close to overlapping zero. This is reflected in the high P value.\n\n\n\n\n\n\n\n\n\nExample 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1\n\n\n\n\n\nExample 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1\nRecall that Example 2 contains two predictors and both are categorical:\nResp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1\nWhat are your modelled effects (with uncertainty)?\nAgain, for categorical predictors, there is a coefficient estimated for each level of the predictor. If there is one or more interactions among predictors, there will be one coefficient for each combination of levels among predictors. Let’s look at the summary() output of your model to understand better:\n\ncoef(summary(bestMod2)) # extract the coefficients from summary()\n\n                        Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)            364.90879   24.73850 14.7506420 1.629062e-25\nCat2TypeB               37.37325   32.98467  1.1330491 2.602712e-01\nCat2TypeC              -75.94350   33.87459 -2.2419017 2.748208e-02\nCat2TypeD             -141.96558   38.32472 -3.7042819 3.695537e-04\nCat3Treat2             -43.62974   36.41409 -1.1981554 2.340735e-01\nCat3Control             64.11573   30.29835  2.1161457 3.715672e-02\nCat2TypeB:Cat3Treat2    42.70879   53.60009  0.7968045 4.277091e-01\nCat2TypeC:Cat3Treat2    73.60837   54.15228  1.3592849 1.775295e-01\nCat2TypeD:Cat3Treat2    66.02110   55.13228  1.1975037 2.343260e-01\nCat2TypeB:Cat3Control   81.25030   43.24327  1.8789121 6.356678e-02\nCat2TypeC:Cat3Control  -19.07730   41.29748 -0.4619483 6.452584e-01\nCat2TypeD:Cat3Control  -71.72444   46.17117 -1.5534463 1.239057e-01\n\n\nComparing this output to that of Example 1 shows many more estimated coefficients for Example 2. This is because we have one coefficient for each level of each predictor, as well as coefficients for each combination (the interaction term) of levels of the predictors.\nAgain, it takes a little practice to read the coefficients from the summary() output. For Example 2:\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat1 is 365 units (the intercept).\nThe modelled prediction for Resp2 when Cat2 is TypeB and Cat3 is Treat1 is 365 + 37 = 402 units.\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat2 is 365 - 44 = 321 units.\nThe modelled prediction for Resp2 when Cat2 is TypeC and Cat3 is Treat2 is 365 - 76 - 44 + 74 = 319 units.\nAs above, we can use the emmeans package to more easily see these coefficients:\n\nemmOut &lt;- emmeans(object = bestMod2, # your model\n            specs = ~ Cat2 + Cat3 + Cat2:Cat3, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat2  Cat3    emmean   SE df lower.CL upper.CL\n TypeA Treat1     365 24.7 88      316      414\n TypeB Treat1     402 21.8 88      359      446\n TypeC Treat1     289 23.1 88      243      335\n TypeD Treat1     223 29.3 88      165      281\n TypeA Treat2     321 26.7 88      268      374\n TypeB Treat2     401 32.7 88      336      466\n TypeC Treat2     319 32.7 88      254      384\n TypeD Treat2     245 29.3 88      187      304\n TypeA Control    429 17.5 88      394      464\n TypeB Control    548 21.8 88      504      591\n TypeC Control    334 15.9 88      302      366\n TypeD Control    215 18.9 88      178      253\n\nConfidence level used: 0.95 \n\n\nSo now we have a modelled value of our response for each level of our categorical predictor. For example:\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat1 is 365 +/- 25 units.\nThe modelled prediction for Resp2 when Cat2 is TypeB and Cat3 is Treat1 is 402 +/- 22 units.\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat2 is 321 +/- 27 units.\nThe modelled prediction for Resp2 when Cat2 is TypeC and Cat3 is Treat2 is 319 +/- 33 units.\nFinally, note that you can also get a quick plot of the effects by handing the emmeans() output to plot().\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nAs with Example 1, you can also find out which combinations of predictor levels are leading to statistically different model predictions in Resp2:\n\nforComp &lt;- pairs(emmOut)\n\nforComp\n\n contrast                      estimate   SE df t.ratio p.value\n TypeA Treat1 - TypeB Treat1    -37.373 33.0 88  -1.133  0.9923\n TypeA Treat1 - TypeC Treat1     75.944 33.9 88   2.242  0.5248\n TypeA Treat1 - TypeD Treat1    141.966 38.3 88   3.704  0.0180\n TypeA Treat1 - TypeA Treat2     43.630 36.4 88   1.198  0.9878\n TypeA Treat1 - TypeB Treat2    -36.452 41.0 88  -0.889  0.9991\n TypeA Treat1 - TypeC Treat2     45.965 41.0 88   1.120  0.9929\n TypeA Treat1 - TypeD Treat2    119.574 38.3 88   3.120  0.0940\n TypeA Treat1 - TypeA Control   -64.116 30.3 88  -2.116  0.6129\n TypeA Treat1 - TypeB Control  -182.739 33.0 88  -5.540  &lt;.0001\n TypeA Treat1 - TypeC Control    30.905 29.4 88   1.051  0.9959\n TypeA Treat1 - TypeD Control   149.574 31.1 88   4.805  0.0004\n TypeB Treat1 - TypeC Treat1    113.317 31.8 88   3.563  0.0277\n TypeB Treat1 - TypeD Treat1    179.339 36.5 88   4.912  0.0002\n TypeB Treat1 - TypeA Treat2     81.003 34.5 88   2.348  0.4517\n TypeB Treat1 - TypeB Treat2      0.921 39.3 88   0.023  1.0000\n TypeB Treat1 - TypeC Treat2     83.338 39.3 88   2.119  0.6110\n TypeB Treat1 - TypeD Treat2    156.947 36.5 88   4.299  0.0025\n TypeB Treat1 - TypeA Control   -26.742 28.0 88  -0.956  0.9982\n TypeB Treat1 - TypeB Control  -145.366 30.9 88  -4.711  0.0005\n TypeB Treat1 - TypeC Control    68.278 27.0 88   2.531  0.3355\n TypeB Treat1 - TypeD Control   186.947 28.9 88   6.477  &lt;.0001\n TypeC Treat1 - TypeD Treat1     66.022 37.3 88   1.769  0.8297\n TypeC Treat1 - TypeA Treat2    -32.314 35.3 88  -0.914  0.9988\n TypeC Treat1 - TypeB Treat2   -112.396 40.1 88  -2.804  0.1965\n TypeC Treat1 - TypeC Treat2    -29.979 40.1 88  -0.748  0.9998\n TypeC Treat1 - TypeD Treat2     43.631 37.3 88   1.169  0.9900\n TypeC Treat1 - TypeA Control  -140.059 29.0 88  -4.828  0.0003\n TypeC Treat1 - TypeB Control  -258.683 31.8 88  -8.134  &lt;.0001\n TypeC Treat1 - TypeC Control   -45.038 28.1 88  -1.605  0.9027\n TypeC Treat1 - TypeD Control    73.631 29.9 88   2.465  0.3757\n TypeD Treat1 - TypeA Treat2    -98.336 39.6 88  -2.481  0.3654\n TypeD Treat1 - TypeB Treat2   -178.418 43.9 88  -4.064  0.0056\n TypeD Treat1 - TypeC Treat2    -96.001 43.9 88  -2.186  0.5636\n TypeD Treat1 - TypeD Treat2    -22.391 41.4 88  -0.541  1.0000\n TypeD Treat1 - TypeA Control  -206.081 34.1 88  -6.043  &lt;.0001\n TypeD Treat1 - TypeB Control  -324.705 36.5 88  -8.894  &lt;.0001\n TypeD Treat1 - TypeC Control  -111.061 33.3 88  -3.335  0.0532\n TypeD Treat1 - TypeD Control     7.609 34.8 88   0.218  1.0000\n TypeA Treat2 - TypeB Treat2    -80.082 42.2 88  -1.895  0.7587\n TypeA Treat2 - TypeC Treat2      2.335 42.2 88   0.055  1.0000\n TypeA Treat2 - TypeD Treat2     75.945 39.6 88   1.916  0.7460\n TypeA Treat2 - TypeA Control  -107.746 31.9 88  -3.374  0.0478\n TypeA Treat2 - TypeB Control  -226.369 34.5 88  -6.562  &lt;.0001\n TypeA Treat2 - TypeC Control   -12.725 31.1 88  -0.409  1.0000\n TypeA Treat2 - TypeD Control   105.945 32.7 88   3.237  0.0693\n TypeB Treat2 - TypeC Treat2     82.417 46.3 88   1.781  0.8238\n TypeB Treat2 - TypeD Treat2    156.026 43.9 88   3.554  0.0285\n TypeB Treat2 - TypeA Control   -27.663 37.1 88  -0.745  0.9998\n TypeB Treat2 - TypeB Control  -146.287 39.3 88  -3.719  0.0172\n TypeB Treat2 - TypeC Control    67.357 36.4 88   1.852  0.7846\n TypeB Treat2 - TypeD Control   186.027 37.8 88   4.923  0.0002\n TypeC Treat2 - TypeD Treat2     73.609 43.9 88   1.677  0.8739\n TypeC Treat2 - TypeA Control  -110.081 37.1 88  -2.967  0.1366\n TypeC Treat2 - TypeB Control  -228.704 39.3 88  -5.815  &lt;.0001\n TypeC Treat2 - TypeC Control   -15.060 36.4 88  -0.414  1.0000\n TypeC Treat2 - TypeD Control   103.609 37.8 88   2.742  0.2240\n TypeD Treat2 - TypeA Control  -183.690 34.1 88  -5.387  &lt;.0001\n TypeD Treat2 - TypeB Control  -302.313 36.5 88  -8.281  &lt;.0001\n TypeD Treat2 - TypeC Control   -88.669 33.3 88  -2.663  0.2624\n TypeD Treat2 - TypeD Control    30.000 34.8 88   0.861  0.9993\n TypeA Control - TypeB Control -118.624 28.0 88  -4.242  0.0030\n TypeA Control - TypeC Control   95.021 23.6 88   4.023  0.0064\n TypeA Control - TypeD Control  213.690 25.7 88   8.299  &lt;.0001\n TypeB Control - TypeC Control  213.644 27.0 88   7.918  &lt;.0001\n TypeB Control - TypeD Control  332.314 28.9 88  11.514  &lt;.0001\n TypeC Control - TypeD Control  118.669 24.7 88   4.809  0.0004\n\nP value adjustment: tukey method for comparing a family of 12 estimates \n\n\nThis is a hard table to navigate as every possible combination of the levels across predictors is compared. This may not be what you want. Instead, you can look for differences of one predictor based on a value of another. For example:\n\nforComp &lt;- pairs(emmOut, # emmeans output\n                 simple = \"Cat2\") # contrasts within this categorical predictor\n\nforComp\n\nCat3 = Treat1:\n contrast      estimate   SE df t.ratio p.value\n TypeA - TypeB   -37.37 33.0 88  -1.133  0.6702\n TypeA - TypeC    75.94 33.9 88   2.242  0.1201\n TypeA - TypeD   141.97 38.3 88   3.704  0.0021\n TypeB - TypeC   113.32 31.8 88   3.563  0.0033\n TypeB - TypeD   179.34 36.5 88   4.912  &lt;.0001\n TypeC - TypeD    66.02 37.3 88   1.769  0.2949\n\nCat3 = Treat2:\n contrast      estimate   SE df t.ratio p.value\n TypeA - TypeB   -80.08 42.2 88  -1.895  0.2375\n TypeA - TypeC     2.34 42.2 88   0.055  0.9999\n TypeA - TypeD    75.94 39.6 88   1.916  0.2288\n TypeB - TypeC    82.42 46.3 88   1.781  0.2894\n TypeB - TypeD   156.03 43.9 88   3.554  0.0034\n TypeC - TypeD    73.61 43.9 88   1.677  0.3422\n\nCat3 = Control:\n contrast      estimate   SE df t.ratio p.value\n TypeA - TypeB  -118.62 28.0 88  -4.242  0.0003\n TypeA - TypeC    95.02 23.6 88   4.023  0.0007\n TypeA - TypeD   213.69 25.7 88   8.299  &lt;.0001\n TypeB - TypeC   213.64 27.0 88   7.918  &lt;.0001\n TypeB - TypeD   332.31 28.9 88  11.514  &lt;.0001\n TypeC - TypeD   118.67 24.7 88   4.809  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nHere you can more easily see the contrasts, and the adjustment to the P-value will be limited to what you actually need. Based on a threshold P-value of 0.05, you can see that the effects at some combinations of your predictors are not statistically different from each other.\nFor example, the coefficient (predicted Resp2) when Cat3 = Treat1 and Cat2 = TypeA is 365 and this is 37 lower than the coefficient when Cat3 = Treat1 and Cat2 = TypeB, but there is no evidence that the difference has any meaning as P = 0.67 for this comparison.\nOn the other hand, some combinations of your predictors are statistically different from each other. For example, a comparison of modelled Resp2 when Cat3 = Control and Cat2 = TypeB (548) vs. Cat3 = Control and Cat2 = TypeD (215) shows that they differ by 333 and that there is strong evidence that this difference is meaningful as P &lt; 0.001.\nNote that you could also organize this with contrasts by Cat3 instead:\n\nforComp &lt;- pairs(emmOut, # emmeans output\n                 simple = \"Cat3\") # contrasts within this categorical predictor\n\nforComp\n\nCat2 = TypeA:\n contrast         estimate   SE df t.ratio p.value\n Treat1 - Treat2    43.630 36.4 88   1.198  0.4574\n Treat1 - Control  -64.116 30.3 88  -2.116  0.0924\n Treat2 - Control -107.745 31.9 88  -3.374  0.0031\n\nCat2 = TypeB:\n contrast         estimate   SE df t.ratio p.value\n Treat1 - Treat2     0.921 39.3 88   0.023  0.9997\n Treat1 - Control -145.366 30.9 88  -4.711  &lt;.0001\n Treat2 - Control -146.287 39.3 88  -3.719  0.0010\n\nCat2 = TypeC:\n contrast         estimate   SE df t.ratio p.value\n Treat1 - Treat2   -29.979 40.1 88  -0.748  0.7357\n Treat1 - Control  -45.038 28.1 88  -1.605  0.2489\n Treat2 - Control  -15.060 36.4 88  -0.414  0.9099\n\nCat2 = TypeD:\n contrast         estimate   SE df t.ratio p.value\n Treat1 - Treat2   -22.391 41.4 88  -0.541  0.8514\n Treat1 - Control    7.609 34.8 88   0.218  0.9741\n Treat2 - Control   30.000 34.8 88   0.861  0.6661\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nNote that you can also get the results from the pairwise testing visually by handing the output of pairs() to plot().\n\n\n\n\n\n\n\n\n\nExample 3: Resp3 ~ Cont4 + 1\n\n\n\n\n\nExample 3: Resp3 ~ Cont4 + 1\nRecall that Example 3 contains one predictor and the predictor is continuous:\nResp3 ~ Cont4 + 1\nWhat are your modelled effects (with uncertainty)?\nFor a continuous predictor, the effect is captured in one coefficient that describes the change in the response for a unit change in the continuous predictor. For models with a normal error distribution assumption, this is communicated as the slope.\nLet’s look at the summary() output for Example 3:\n\ncoef(summary(bestMod3)) # extract the coefficients from summary()\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -226.9131  166.09360 -1.366176 1.750105e-01\nCont4        260.0574   14.56593 17.853818 1.438227e-32\n\n\nThis tells you that for a unit change in Cont4, you get a 260 ± 1512.\nNote that this same information is in the emmeans package. Because Cont4 is a continuous predictor, you need the emtrends() function, which provides the 95% confidence interval on the estimate:\n\ntrendsOut &lt;- emtrends(bestMod3,  # your model\n                      specs = ~ Cont4 + 1, # your predictors\n                      var = \"Cont4\") # your continuous predictors\n\ntrendsOut\n\n Cont4 Cont4.trend   SE df lower.CL upper.CL\n  9.94         260 14.6 98      231      289\n\nConfidence level used: 0.95 \n\n\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nThis question is only applies to categorical predictors of which there are none in Example 3.\n\n\n\n\n\n\n\n\n\nExample 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\n\n\n\n\n\nExample 4: Resp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nRecall that Example 4 contains two predictors, one is categorical and one is continuous:\nResp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nWhat are your modelled effects (with uncertainty)?\nThe effects estimated for this model will include values of the response (Resp4) at each level of the categorical predictor (Cat5) as well as slopes describing the change in the response when the continuous predictor (Cont6) changes, and a different slope will be estimated for each level in Cat5 (this is because of the interaction in the model).\nAs above, you can take a look at your modelled coefficients with the summary() output:\n\ncoef(summary(bestMod4))\n\n                   Estimate  Std. Error     t value     Pr(&gt;|t|)\n(Intercept)     98.34679337 1.868900753 52.62280151 1.536692e-71\nCat5Urban       -0.23642403 2.895404981 -0.08165491 9.350947e-01\nCat5Wild        -0.86733554 3.238562391 -0.26781498 7.894285e-01\nCont6           -0.00293078 0.003740830 -0.78345704 4.353283e-01\nCat5Urban:Cont6  0.01511745 0.005611511  2.69400763 8.361918e-03\nCat5Wild:Cont6   0.03085821 0.006183238  4.99062262 2.756559e-06\n\n\nThis shows:\nThe model prediction of Resp4 when Cat5 is Farm and Cont6 is 0 is 98.34 units13 (the intercept).\nThe model prediction of Resp4 when Cat5 is Urban and Cont6 is 0 is 98.34 - 0.24 = 98.1 units.\nThe model prediction of Resp4 when Cat5 is Wild and Cont6 is 0 is 98.34 - 0.87 = 97.5 units.\nThe slope of the relationship between Cont6 and Resp4 when Cat5 is Farm is -0.0029.14\nThe slope of the relationship between Cont6 and Resp4 when Cat5 is Urban is -0.0029 + 0.015 = 0.0121.\nThe slope of the relationship between Cont6 and Resp4 when Cat5 is Wild is -0.0029 + 0.031 = 0.0281.\nAgain, interpreting the coefficients from the summary() output is tedious and not necessary: You can use the emmeans package to give you the modelled response for each level of the categorical predictor (Cat5) directly:\n\nemmOut &lt;- emmeans(object = bestMod4, # your model\n            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat5  Cont6 emmean    SE df lower.CL upper.CL\n Farm    510   96.9 0.458 94     95.9     97.8\n Urban   510  104.3 0.421 94    103.5    105.2\n Wild    510  111.7 0.511 94    110.7    112.7\n\nConfidence level used: 0.95 \n\n\nNote that emmeans() sets our continuous predictor (Cont6) to the mean value of Cont6 (510 units). We can also control this with the at = argument:\n\nemmOut &lt;- emmeans(object = bestMod4, # your model\n            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors\n            type = \"response\", # report coefficients on the response scale\n            at = list(Cont6 = 0)) # control the value of your continuous predictor at which to make the coefficient estimates\n\n\nemmOut\n\n Cat5  Cont6 emmean   SE df lower.CL upper.CL\n Farm      0   98.3 1.87 94     94.6      102\n Urban     0   98.1 2.21 94     93.7      103\n Wild      0   97.5 2.64 94     92.2      103\n\nConfidence level used: 0.95 \n\n\nBy setting at = 0, you get the intercept - i.e. the modelled Resp4 when Cont6 = 0 for each level of Cat5, and this is what is reported in the summary() output.\nSimilarly, you can get the emmeans package to give you the slope coefficients for the continuous predictor (Cont6) using the emtrends() function, rather than interpreting them from the summary() output:\n\ntrendsOut &lt;- emtrends(bestMod4,  # your model\n                      specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors\n                      var = \"Cont6\") # your continuous predictors\n\n\ntrendsOut\n\n Cat5  Cont6 Cont6.trend      SE df lower.CL upper.CL\n Farm    510    -0.00293 0.00374 94 -0.01036   0.0045\n Urban   510     0.01219 0.00418 94  0.00388   0.0205\n Wild    510     0.02793 0.00492 94  0.01815   0.0377\n\nConfidence level used: 0.95 \n\n\nNote that the 95% confidence limits for the modelled effect for Cont6 when Cat5 = Farm includes zero. We can also see this with the test() function.\n\ntest(trendsOut) # get test if coefficient is different than zero.\n\n Cat5  Cont6 Cont6.trend      SE df t.ratio p.value\n Farm    510    -0.00293 0.00374 94  -0.783  0.4353\n Urban   510     0.01219 0.00418 94   2.914  0.0045\n Wild    510     0.02793 0.00492 94   5.673  &lt;.0001\n\n\ncombining the two output, we can report that:\n\nthere is no evidence on an effect on Resp4 of Cont6 when Cat5 = Farm (p = 0.44).\nthere is evidence for a positive effect of Cont6 on Resp4 when Cat5 = Urban. The effect is 0.01215 with a 95% confidence interval of 0.0039 - 0.020516.\nthere is evidence for a positive effect of Cont6 on Resp4 when Cat5 = Wild. The effect is 0.02817 with a 95% confidence interval of 0.0182 - 0.037718.\n\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\n\nSince you have a categorical predictor with more than two levels, you will want to report where the effects among levels differ from one another.\nNote that the effects in Example 4 include slope coefficients (associated with the continuous predictor) and intercept coefficients (associated with the categorical predictor). Because the slope and intercept are dependent on one another (when the slope changes, the intercept will naturally change), you need to first check if the slope coefficients vary across your categorical levels. Only if they don’t vary (i.e. the modelled effects produce lines that are parallel across your categorical levels) would you go on to test if the intercept coefficiens vary across your categorical levels.\n\nYou can find out which slopes (i.e. the effect of Cont6 on Resp4) are different across the levels of Cat5 using emtrends() with a request for pairwise testing:\n\nforCompSlope &lt;- pairs(trendsOut)\n\nforCompSlope\n\n contrast                               estimate      SE df t.ratio p.value\n Farm Cont6509.767 - Urban Cont6509.767  -0.0151 0.00561 94  -2.694  0.0226\n Farm Cont6509.767 - Wild Cont6509.767   -0.0309 0.00618 94  -4.991  &lt;.0001\n Urban Cont6509.767 - Wild Cont6509.767  -0.0157 0.00646 94  -2.437  0.0437\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nHere you see that the slope estimates (the effect of Cont6 on Resp4) for all levels of Cat5 are significantly different from one another (0.0001 ≤ P ≤ 0.044).\nNote that you can also get the results from the pairwise testing visually by handing the output of pairs() to plot().\nAgain: only if all the slopes were similar, would you want to test if the levels of your categorical predictor (Cat4) have significantly different coefficients (intercepts) from each other. You would do this with pairs() on the output from emmeans() (the emmOut object).\n\n\n\n\n\n\nModels with a poisson error distribution assumption:\nBackground\nFor models with a poisson error distribution assumption, you will typically start with a link function that is the natural logarithm or link = \"log\". You need to take this link function into consideration when you report your modelled effects.\n\nIf you are using link = \"log\" (as with a poisson error distribution, and sometimes also a Gamma error distribution - more on this to come), you get your coefficient on the response scale by taking e to the coefficient on the link scale (with the R function exp()). This coefficient is called the rate ratio19 and it tells you the % change in the response for a unit change in your predictor.\n\nFor a continuous predictor, the effect is captured in one coefficient that describes the change in the response for a unit change in the continuous predictor.\nFor models with a normal error distribution assumption, this was simply a slope.\nFor models using a log link (including many models with a poisson error distribution assumption), this is communicated as the rate ratio or the % change in the response for a unit change in your predictor. This allows you to communicate the effect of the continuous predictor while accounting for the curve in the relationship (see visualization of model effects above). This curve occurs because the model is not allowed to go below zero to be consistent with a poisson error distribution assumption.\n\n\n\n\n\n\nthe math\n\n\n\n\n\n\nBut why do we convert coefficients from the link to response scale with \\(e^x\\)20 when link = \"log\"?\nA reminder that we can present our linear model like this:\n\\[\n\\begin{align}\ng(\\mu_i|Pred_i) &= \\beta_1 \\cdot Pred_i + \\beta_0 \\\\\nResp_i &\\sim F(\\mu_i)\n\\end{align}\n\\]\nwhere * \\(g()\\) is the link function * \\(\\mu_i\\) is the mean fitted value \\(\\mu_i\\) dependent on \\(Pred_i\\) * \\(\\beta_1\\) is the coefficient of \\(Pred_i\\) * \\(Pred_i\\) is the value of your predictor * \\(\\beta_0\\) is the intercept * \\(Resp_i\\) is the value of the response variable * \\(F\\) represents some error distribution assumption\nIf you have an error distribution assumption (\\(F\\)), the link function \\(g()\\) is \\(log_e\\)21:\n\\[\n\\begin{align}\nlog_e(\\mu_i|Pred_i) &= \\beta_1 \\cdot Pred_i + \\beta_0 \\\\\nResp_i &\\sim poisson(\\mu_i)\n\\end{align}\n\\]\nThen the rate ratio (RR), or % change in the response for a unit change in the predictor becomes,\n\\[\n\\begin{align}\nRR&=\\frac{(\\mu_i|Pred = a+1)}{(\\mu_i|Pred = a)}\\\\[2em]\nlog_e(RR)&=log_e(\\frac{(\\mu_i|Pred = a+1)}{(\\mu_i|Pred = a)})\\\\[2em]\nlog_e(RR)&=log_e(\\mu_i|Pred = a+1)-log_e(\\mu_i|Pred = a)\\\\[2em]\nlog_e(RR)&=(\\beta_1\\cdot (a+1)+\\beta_0)-(\\beta_1\\cdot (a)+\\beta_0)\\\\[2em]\nlog_e(RR)&=\\beta_1\\cdot a + \\beta_1+\\beta_0-\\beta_1\\cdot a-\\beta_0\\\\[2em]\nlog_e(RR)&=\\beta_1\\\\[2em]\nRR &=e^{\\beta_1}\n\\end{align}\n\\]\n\n\n\nOne thing to note:\nThe emmeans() function will convert the coefficients (intercepts) from the link to the response scale. You can ask for this with the type = \"response\" argument.\nIn contrast, the emtrends() function does not convert the coefficients (slopes) to represent effects on the response scale. This is because emtrends() is reporting the slope of a straight line - the trend line on the link scale. But the line isn’t straight on the response scale.\nWe need to convert from the link to the response by hand when we use emtrends()^[remember, the coefficients for categorical predictors are also being converted from the link to the response scale. It is just that R is doing it for us in the emmeans() function when we add the argument `type = “response”). How we make the conversion, and interpret the result, will depend on our error distribution assumption:\n\nHere we will go through examples of reporting for models with a poisson error distribution assumption. If you want more details on why you are using the methods below, check the section on “Models with a normal error distribution assumption” (Section 5.2.3) for context.\nExamples\nHere I have made new examples where the error distribution assumption is poisson. This changes the response variables, indicated by Resp1.p, Resp2.p, etc. to be integers ≥ 0.\n\n\n\n\n\n\nExample 1: Resp1.p ~ Cat1 + 1\n\n\n\n\n\nExample 1: Resp1.p ~ Cat1 + 1\nRecall that Example 1 contains only one predictor and it is categorical:\nResp1.p ~ Cat1 + 1\nThe best-specified model (now with poisson error distribution assumption) is:\n\nbestMod1.p\n\n\nCall:  glm(formula = Resp1.p ~ Cat1 + 1, family = poisson(link = \"log\"), \n    data = Dat)\n\nCoefficients:\n(Intercept)      Cat1StB      Cat1StC  \n    6.15524     -0.04869      0.09948  \n\nDegrees of Freedom: 49 Total (i.e. Null);  47 Residual\nNull Deviance:      139 \nResidual Deviance: 40.01    AIC: 446\n\n\nthat was fit to data in myDat1.p:\n\nstr(myDat1.p)\n\n'data.frame':   50 obs. of  2 variables:\n $ Cat1   : Factor w/ 3 levels \"StA\",\"StB\",\"StC\": 2 2 2 1 3 2 3 1 3 1 ...\n $ Resp1.p: int  434 476 443 473 516 449 529 462 512 429 ...\n\n\nand the dredge() table you used to pick your bestMod1.p is in dredgeOut1.p\n\ndredgeOut1.p\n\nGlobal model call: glm(formula = Resp1.p ~ Cat1 + 1, family = poisson(link = \"log\"), \n    data = Dat)\n---\nModel selection table \n  (Intrc) Cat1 df   logLik  AICc delta weight\n2   6.155    +  3 -219.983 446.5  0.00      1\n1   6.164       1 -269.477 541.0 94.55      0\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n# Set up your predictors for the visualized fit\nforCat1&lt;-unique(myDat1.p$Cat1) # every value of your categorical predictor\n\n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat1=forCat1) # expand.grid() function makes sure you have all combinations of predictors\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod1.p, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n\nlibrary(ggplot2)\n\nggplot() + # start ggplot\n  geom_point(data = myDat1.p, # add observations to your plot\n             mapping = aes(x = Cat1, y = Resp1.p), \n             position=position_jitter(width=0.1)) + # control position of data points so they are easier to see on the plot\n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat1, y = Fit, ymin = Lower, ymax = Upper),\n              linewidth=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat1, y = Fit), \n             shape = 22, # shape of point\n             size = 3, # size of point\n             fill = \"white\", # fill colour for plot\n             col = 'black') + # outline colour for plot\n  ylab(\"Resp1.p, (units)\")+ # change y-axis label\n  xlab(\"Cat1, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nWhat are your modelled effects (with uncertainty)?\nFor categorical predictors, you can get the coefficients on the response scale directly with the emmeans() function22 when you specify type = “response”:\n\nlibrary(emmeans) # load the emmeans package\n\nemmOut &lt;- emmeans(object = bestMod1.p, # your model\n            specs = ~ Cat1 + 1, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat1 rate   SE  df asymp.LCL asymp.UCL\n StA   471 6.54 Inf       459       484\n StB   449 4.32 Inf       440       457\n StC   520 5.89 Inf       509       532\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nNotice that the column reported is “rate”: these are the coefficients converted back onto the response scale.\nNote also that two types of uncertainty are measured here. SE stands for the standard error around the prediction, and is a measure of uncertainty of the average modelled effect. The lower.CL and upper.CL represent the 95% confidence limits of the prediction - so if I observed a new Resp1.p at a particular Cat1, there would be a 95% chance it would lie between the bounds of the confidence limits.\nFrom this output you see:\nWhen Cat1 is SpA, Resp1.p is estimated to be 471 (95% confidence interval: 459 to 484 units).\nWhen Cat1 is SpB, Resp1.p is estimated to be 449 (95% confidence interval: 440 to 457 units). When Cat1 is SpC, Resp1.p is estimated to be 520 (95% confidence interval: 509 to 532 units).\nFinally, note that you can also get a quick plot of the effects by handing the emmeans() output to plot().\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nHere you can compare the effects of the levels of Cat1 on Resp1.p to see if the effects differ between the levels.\n\nforComp &lt;- pairs(emmOut)\n\nforComp\n\n contrast  ratio     SE  df null z.ratio p.value\n StA / StB 1.050 0.0177 Inf    1   2.880  0.0111\n StA / StC 0.905 0.0162 Inf    1  -5.552  &lt;.0001\n StB / StC 0.862 0.0128 Inf    1  -9.968  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \nTests are performed on the log scale \n\n\nThe output shows the results of the multiple comparison (pairwise) testing.\nThe values in the p.value column tell you the results of the hypothesis test comparing the coefficients between the two levels of your categorical predictor.\nFor example:\n\nthere is evidence that Resp1.p is significantly larger when Cat 1 = SpA than when Cat1 = SpB (p = 0.0111). Resp1.p is expected to be ~ 5 ± 2% bigger when Cat 1 = SpA than when Cat 1 = SpB.\nthere is evidence that Resp1.p is significantly smaller when Cat 1 = SpA than when Cat1 = SpC (p = 0). Resp1.p is expected to be ~ 9 ± 2% smaller when Cat 1 = SpA than when Cat 1 = SpC.\n\nNote that you can also get the results from the pairwise testing visually by handing the output of pairs() to plot().\n\n\n\n\n\n\n\n\n\nExample 2: Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1\n\n\n\n\n\nExample 2: Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1\nRecall that Example 2 contains two predictors and both are categorical:\nResp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1\nThe best-specified model (now with poisson error distribution assumption) is:\n\nbestMod2.p\n\n\nCall:  glm(formula = Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1, family = poisson(link = \"log\"), \n    data = myDat2.p)\n\nCoefficients:\n        (Intercept)            Cat2TypeB            Cat2TypeC  \n             2.7213              -0.3365              -0.4526  \n          Cat2TypeD            Cat3Treat  Cat2TypeB:Cat3Treat  \n            -1.2744              -0.3810              -0.6176  \nCat2TypeC:Cat3Treat  Cat2TypeD:Cat3Treat  \n            -0.5999               0.4381  \n\nDegrees of Freedom: 49 Total (i.e. Null);  42 Residual\nNull Deviance:      132.5 \nResidual Deviance: 35.27    AIC: 239.5\n\n\nthat was fit to data in myDat2.p:\n\nstr(myDat2.p)\n\n'data.frame':   50 obs. of  3 variables:\n $ Cat2   : Factor w/ 4 levels \"TypeA\",\"TypeB\",..: 1 2 4 1 2 4 1 1 2 2 ...\n $ Cat3   : Factor w/ 2 levels \"Control\",\"Treat\": 2 2 1 1 2 1 1 1 1 1 ...\n $ Resp2.p: int  10 3 3 19 4 4 9 13 15 12 ...\n\n\nand the dredge() table you used to pick your bestMod2.p is in\n\ndredgeOut2.p\n\nGlobal model call: glm(formula = Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1, family = poisson(link = \"log\"), \n    data = myDat2.p)\n---\nModel selection table \n  (Int) Ct2 Ct3 Ct2:Ct3 df   logLik  AICc delta weight\n8 2.721   +   +       +  8 -111.758 243.0  0.00   0.77\n4 2.860   +   +          5 -117.038 245.4  2.41   0.23\n2 2.461   +              4 -134.275 277.4 34.41   0.00\n3 2.344       +          2 -150.075 304.4 61.38   0.00\n1 2.087                  1 -160.376 322.8 79.81   0.00\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n# Set up your predictors for the visualized fit\nforCat2&lt;-unique(myDat2.p$Cat2) # every level of your categorical predictor\nforCat3&lt;-unique(myDat2.p$Cat3) # every level of your categorical predictor\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat2 = forCat2, Cat3 = forCat3) # expand.grid() function makes sure you have all combinations of predictors\n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod2.p, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  geom_point(data = myDat2.p, # add observations to your plot\n             mapping = aes(x = Cat2, y = Resp2.p, col = Cat3), \n             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot\n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat2, y = Fit, ymin = Lower, ymax = Upper, col = Cat3),\n              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot\n              size=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat2, y = Fit, fill = Cat3), \n             shape = 22, # shape of point\n             size=3, # size of point\n             col = 'black', # outline colour for point\n             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot\n  ylab(\"Resp2.p, (units)\")+ # change y-axis label\n  xlab(\"Cat2, (units)\")+ # change x-axis label\n  labs(fill=\"Cat3, (units)\", col=\"Cat3, (units)\") + # change legend title\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nWhat are your modelled effects (with uncertainty)?\nAs above, we can use the emmeans package to see the effects of the predictors on the scale of the response:\n\nemmOut &lt;- emmeans(object = bestMod2.p, # your model\n            specs = ~ Cat2 + Cat3 + Cat2:Cat3, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat2  Cat3     rate    SE  df asymp.LCL asymp.UCL\n TypeA Control 15.20 1.740 Inf     12.14     19.03\n TypeB Control 10.86 1.250 Inf      8.67     13.59\n TypeC Control  9.67 1.800 Inf      6.72     13.91\n TypeD Control  4.25 1.030 Inf      2.64      6.84\n TypeA Treat   10.38 0.894 Inf      8.77     12.29\n TypeB Treat    4.00 0.707 Inf      2.83      5.66\n TypeC Treat    3.62 0.673 Inf      2.52      5.22\n TypeD Treat    4.50 1.500 Inf      2.34      8.65\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nSo now we have a modelled value of our response for each level of our categorical predictors. For example:\nThe modelled prediction for Resp2.p when Cat2 is TypeA and Cat3 is Treat is 15.2 (95% confidence interval: 12.1 - 19.0 units).\nThe modelled prediction for Resp2.p when Cat2 is TypeC and Cat3 is Control is 9.7 units (95% confidence interval: 6.7 - 13.9 units).\nFinally, note that you can also get a quick plot of the effects by handing the emmeans() output to plot().\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nAs with Example 1, you can also find out which combinations of predictor levels are leading to statistically different model predictions in Resp2.p:\n\nforComp &lt;- pairs(emmOut, # emmeans output\n                 simple = \"Cat2\") # contrasts within this categorical predictor\n\nforComp\n\nCat3 = Control:\n contrast      ratio    SE  df null z.ratio p.value\n TypeA / TypeB 1.400 0.227 Inf    1   2.074  0.1616\n TypeA / TypeC 1.572 0.343 Inf    1   2.074  0.1617\n TypeA / TypeD 3.576 0.960 Inf    1   4.750  &lt;.0001\n TypeB / TypeC 1.123 0.245 Inf    1   0.532  0.9513\n TypeB / TypeD 2.555 0.685 Inf    1   3.496  0.0027\n TypeC / TypeD 2.275 0.695 Inf    1   2.690  0.0359\n\nCat3 = Treat:\n contrast      ratio    SE  df null z.ratio p.value\n TypeA / TypeB 2.596 0.510 Inf    1   4.852  &lt;.0001\n TypeA / TypeC 2.865 0.586 Inf    1   5.142  &lt;.0001\n TypeA / TypeD 2.308 0.794 Inf    1   2.429  0.0717\n TypeB / TypeC 1.103 0.283 Inf    1   0.384  0.9808\n TypeB / TypeD 0.889 0.335 Inf    1  -0.312  0.9895\n TypeC / TypeD 0.806 0.307 Inf    1  -0.567  0.9420\n\nP value adjustment: tukey method for comparing a family of 4 estimates \nTests are performed on the log scale \n\n\nHere you can more easily see the contrasts. Contrast ratios similar to 1 mean that the predictor levels lead to a similar value of the response.\nBased on a threshold P-value of 0.05, you can see that the effects at some combinations of your predictors are not statistically different from each other. For example, the coefficient (predicted Resp2.p) when Cat3 = Control and Cat2 = TypeA (15.2) is 1.4 times (or 40%) more than the predicted Resp2.p when Cat3 = Control and Cat2 = TypeB (10.9), but there is no evidence that the difference has any meaning as P = 0.16 for this comparison.\nOn the other hand, some combinations of your predictors are statistically different from each other. For example, a comparison of modelled Resp2.p when Cat3 = Control and Cat2 = TypeB (10.9) is 2.6 times (or 160%) higher than when Cat3 = Control and Cat2 = TypeD (4.2) and there is strong evidence that this difference is meaningful as P &lt; 0.0027.\nNote that you can also get the results from the pairwise testing visually by handing the output of pairs() to plot().\n\n\n\n\n\n\n\n\n\nExample 3: Resp3.p ~ Cont4 + 1\n\n\n\n\n\nExample 3: Resp3.p ~ Cont4 + 1\nRecall that Example 3 contains one predictor and the predictor is continuous:\nResp3 ~ Cont4 + 1\nThe best-specified model (now with poisson error distribution assumption) is:\n\nbestMod3.p\n\n\nCall:  glm(formula = Resp ~ Cont4 + 1, family = poisson(link = \"log\"), \n    data = myDat3.p)\n\nCoefficients:\n(Intercept)        Cont4  \n   1.577204     0.006674  \n\nDegrees of Freedom: 49 Total (i.e. Null);  48 Residual\nNull Deviance:      74.74 \nResidual Deviance: 62.96    AIC: 256.7\n\n\nthat was fit to data in myDat3.p:\n\nstr(myDat3.p)\n\n'data.frame':   50 obs. of  2 variables:\n $ Cont4  : num  67.5 61.9 71.5 116.6 55.9 ...\n $ Resp3.p: int  12 8 4 12 0 2 4 10 8 7 ...\n\n\nand the dredge() table you used to pick your bestMod3.p is\n\ndredgeOut3.p\n\nGlobal model call: glm(formula = Resp ~ Cont4 + 1, family = poisson(link = \"log\"), \n    data = myDat3.p)\n---\nModel selection table \n  (Intrc)    Cont4 df   logLik  AICc delta weight\n2   1.577 0.006674  2 -126.339 256.9  0.00  0.992\n1   2.069           1 -132.230 266.5  9.61  0.008\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n\n# Set up your predictors for the visualized fit\nforCont4&lt;-seq(from = min(myDat3.p$Cont4), to = max(myDat3.p$Cont4), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod3.p, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat3.p, # add observations to your plot\n             mapping = aes(x = Cont4, y = Resp3.p)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont4, y = Fit),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont4, y = Upper),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont4, y = Lower),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ylab(\"Resp3.p, (units)\") + # change y-axis label\n  \n  xlab(\"Cont4, (units)\") + # change x-axis label\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nWhat are your modelled effects (with uncertainty)?\n\ntrendsOut &lt;- emtrends(bestMod3.p,\n                      specs = ~ Cont4 + 1, # your predictors\n                      var = \"Cont4\") # your predictor of interest\n\ntrendsOut\n\n Cont4 Cont4.trend      SE  df asymp.LCL asymp.UCL\n  71.5     0.00667 0.00193 Inf   0.00288    0.0105\n\nConfidence level used: 0.95 \n\n\nNote that these are the same estimates as we find in the summary() of your model object\n\ncoef(summary(bestMod3.p))\n\n               Estimate  Std. Error   z value     Pr(&gt;|z|)\n(Intercept) 1.577203770 0.155306248 10.155443 3.133832e-24\nCont4       0.006674037 0.001934204  3.450534 5.594789e-04\n\n\nThis is an estimate of the modelled effects in the linked world. We need to consider our log link function to report the modelled effects in the response world.\nSince you have a poisson error distribution assumption, you can convert the estimate made by emtrends() on to the response scale with:\n\ntrendsOut.df &lt;- data.frame(trendsOut) # convert trendsOut to a dataframe\n\nRR &lt;- exp(trendsOut.df$Cont4.trend) # get the coefficient on the response scale\n\nRR\n\n[1] 1.006696\n\n\nWe can also use the same conversion on the confidence limits of the modelled effect, for the upper:\n\nRR.up &lt;- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale\n  \nRR.up\n\n[1] 1.01052\n\n\nand lower 95% confidence level:\n\nRR.low &lt;- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale\n\nRR.low\n\n[1] 1.002887\n\n\nThis tells you that for a unit change in Cont4, Resp3 increases by 1.007x (95% confidence interval is 1.003 to 1.011) which is an increase of 0.67%.\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nThis question is only applies to categorical predictors of which their on none in Example 3.\n\n\n\n\n\n\n\n\n\nExample 4: Resp4.p ~ Cat5 + Cont6 + Cat5:Cont6 + 1\n\n\n\n\n\nExample 4: Resp4.p ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nRecall that Example 4 contains two predictors, one is categorical and one is continuous:\nResp4 ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nThe best-specified model (now with poisson error distribution assumption) is:\n\nbestMod4.p\n\n\nCall:  glm(formula = Resp4.p ~ Cat5 + Cont6 + Cat5:Cont6 + 1, family = poisson(link = \"log\"), \n    data = myDat4.p)\n\nCoefficients:\n  (Intercept)        Cat5StB        Cat5StC          Cont6  Cat5StB:Cont6  \n     4.374118       0.074861      -0.262595       0.004075      -0.002618  \nCat5StC:Cont6  \n     0.003898  \n\nDegrees of Freedom: 49 Total (i.e. Null);  44 Residual\nNull Deviance:      183.1 \nResidual Deviance: 50.35    AIC: 386.8\n\n\nthat was fit to data in myDat4.p:\n\nstr(myDat4.p)\n\n'data.frame':   50 obs. of  3 variables:\n $ Cat5   : Factor w/ 3 levels \"StA\",\"StB\",\"StC\": 2 1 3 2 2 1 1 3 3 2 ...\n $ Cont6  : num  60.9 55.9 82.8 89.4 53.4 ...\n $ Resp4.p: int  85 112 109 87 99 98 92 101 87 110 ...\n\n\nand the dredge() table you used to pick your bestMod4.p is\n\ndredgeOut4.p\n\nGlobal model call: glm(formula = Resp4.p ~ Cat5 + Cont6 + Cat5:Cont6 + 1, family = poisson(link = \"log\"), \n    data = myDat4.p)\n---\nModel selection table \n  (Int) Ct5      Cn6 Ct5:Cn6 df   logLik  AICc  delta weight\n8 4.374   + 0.004075       +  6 -187.378 388.7   0.00  0.999\n4 4.291   + 0.005194          4 -196.412 401.7  13.00  0.001\n3 4.261     0.005378          2 -207.753 419.8  31.05  0.000\n2 4.669   +                   3 -239.402 485.3  96.62  0.000\n1 4.665                       1 -253.742 509.6 120.86  0.000\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n\n# Set up your predictors for the visualized fit\nforCat5&lt;-unique(myDat4.p$Cat5) # all levels of your categorical predictor\nforCont6&lt;-seq(from = min(myDat4.p$Cont6), to = max(myDat4.p$Cont6), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat5 = Cat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod4.p, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat4.p, # add observations to your plot\n             mapping = aes(x = Cont6, y = Resp4.p, col = Cat5)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont6, y = Fit, col = Cat5),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont6, y = Upper, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont6, y = Lower, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ylab(\"Resp4, (units)\") + # change y-axis label\n  \n  xlab(\"Cont6, (units)\") + # change x-axis label\n  \n  labs(fill=\"Cat5, (units)\", col=\"Cat5, (units)\") + # change legend title\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nWhat are your modelled effects (with uncertainty)?\nThe process for estimating effects of categorical vs. continuous predictors is different.\nFor categorical predictors (Cat5), you can use emmeans() to give you the effects on the response scale directly:\n\nemmOut &lt;- emmeans(object = bestMod4.p, # your model\n            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat5 Cont6  rate   SE  df asymp.LCL asymp.UCL\n StA   73.5 107.1 2.32 Inf     102.7       112\n StB   73.5  95.2 2.61 Inf      90.2       100\n StC   73.5 109.7 2.70 Inf     104.5       115\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the log scale \n\n\nWhen Cat5 is StA, Resp4.p is estimated to be 74 (95% confidence interval: to 103 units).\nWhen Cat5 is StB, Resp4.p is estimated to be 74 (95% confidence interval: to 90 units). When Cat5 is StC, Resp4.p is estimated to be 74 (95% confidence interval: to 105 units).\nFor continuous predictors (Cont6), you need to use the emtrends() function and then convert the coefficients to the response scale:\n\ntrendsOut &lt;- emtrends(bestMod4.p,\n                      specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors\n                      var = \"Cont6\") # your predictor of interest\n\ntrendsOut\n\n Cat5 Cont6 Cont6.trend       SE  df asymp.LCL asymp.UCL\n StA   73.5     0.00408 0.000829 Inf   0.00245   0.00570\n StB   73.5     0.00146 0.001450 Inf  -0.00139   0.00430\n StC   73.5     0.00797 0.000903 Inf   0.00620   0.00974\n\nConfidence level used: 0.95 \n\n\nSince you have a poisson error distribution assumption, you can convert the estimate made by emtrends() on to the response scale with the exp() function:\n\ntrendsOut.df &lt;- data.frame(trendsOut) # convert trendsOut to a dataframe\n\nRR &lt;- exp(trendsOut.df$Cont6.trend) # get the coefficient on the response scale\n\nRR\n\n[1] 1.004084 1.001458 1.008005\n\n\nYou can also use the same conversion on the confidence limits of the modelled effect, for the upper:\n\nRR.up &lt;- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale\n  \nRR.up\n\n[1] 1.005716 1.004309 1.009791\n\n\nand lower 95% confidence level:\n\nRR.low &lt;- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale\n\nRR.low\n\n[1] 1.0024542 0.9986158 1.0062224\n\n\nLet’s organize these numbers so we can read the effects easier:\n\nRRAll &lt;- data.frame(Cat5 = trendsOut.df$Cat5, # include info on the Cat5 level\n                    RR = exp(trendsOut.df$Cont6.trend), # the modelled effect as a rate ratio\n                    RR.up = exp(trendsOut.df$asymp.UCL), # upper 95% confidence level of the modelled effect\n                    RR.down = exp(trendsOut.df$asymp.LCL)) # lower 95% confidence level of the modelled effect\n\nRRAll\n\n  Cat5       RR    RR.up   RR.down\n1  StA 1.004084 1.005716 1.0024542\n2  StB 1.001458 1.004309 0.9986158\n3  StC 1.008005 1.009791 1.0062224\n\n\nThis tells you that for a unit change in Cont6, Resp4.p\n\nincreases by 1.004 times (an increase of 0.41%) when Cat5 is StA (95% confidence interval: 1.002 to 1.006).\nincreases by 1.001 times (an increase of 0.15%) when Cat5 is StB (95% confidence interval: 0.999 to 1.004).\nincreases by 1.008 times (an increase of 0.8%) when Cat5 is StC (95% confidence interval: 1.006 to 1.01).\n\nNote that the 95% confidence interval for the effect of Cont6 on Resp4.p (the rate ratio) includes 1. When the rate ratio is 1, there is no effect of the continuous predictor on the response. See Section 5.2.4 for more explanation on this.\nYou can test for evidence that the effects of Cont6 on Resp4.p are significant with:\n\ntest(trendsOut) # get test if coefficient is different than zero.\n\n Cat5 Cont6 Cont6.trend       SE  df z.ratio p.value\n StA   73.5     0.00408 0.000829 Inf   4.918  &lt;.0001\n StB   73.5     0.00146 0.001450 Inf   1.005  0.3150\n StC   73.5     0.00797 0.000903 Inf   8.828  &lt;.0001\n\n\nNote that these tests are done on the link scale but can be used for reporting effects on the response scale.\nFrom the results, you can see that:\n\nthere is evidence that there is an effect of Cont6 on Resp4.p when Cat5 = StA. (i.e. the slope on the link scale is different than zero, and the rate ratio on the response scale is different than 1; P &lt; 0.0001).\nthere is no evidence that there is an effect of Cont6 on Resp4.p when Cat5 = StB. (i.e. the slope on the link scale is not different than zero, and the rate ratio on the response scale is not different than 1; P = 0.32).\nthere is evidence that there is an effect of Cont6 on Resp4.p when Cat5 = StC. (i.e. the slope on the link scale is different than zero, and the rate ratio on the response scale is different than 1; P &lt; 0.0001).\n\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nYou can also find out which effects of Cont6 on Resp4.p are different from one another using pairs() on the emtrends() output:\n\nforCompRR &lt;- pairs(trendsOut, \n                   simple = \"Cat5\")\n\nforCompRR\n\nCont6 = 73.5:\n contrast  estimate      SE  df z.ratio p.value\n StA - StB  0.00262 0.00167 Inf   1.568  0.2598\n StA - StC -0.00390 0.00123 Inf  -3.180  0.0042\n StB - StC -0.00652 0.00171 Inf  -3.814  0.0004\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nRemember that you need to convert any predictor coefficients to the response scale when reporting.\nThe table above tells you that:\n\nThere is no evidence the effect of Cont6 on Resp4.p differs when Cat5 is StA vs. StB (P = 0.26).\nThere is evidence that the effect of Cont6 on Resp4.p is different when Cat5 is StA vs. StC (P = 0.0042). The effect of Cont6 on Resp4.p is exp(-0.0039) = 0.9961076 times as big when Cat5 is StA vs. StC.\nThere is evidence that the effect of Cont6 on Resp4.p is different when Cat5 is StB vs. StC (P = 0.0004). The effect of Cont6 on Resp4.p is exp(-0.00652) = 0.9935012 times as big when Cat5 is StB vs. StC.\n\nOnly if all the slopes were similar, you would want to test if the levels of your categorical predictor (Cat5) have significantly different coefficients (intercepts) from each other with pairs() on the output from emmeans() (the emmOut object).\n\n\n\n\n\n\nModels with a Gamma error distribution assumption:\nBackground\nFor models with a Gamma error distribution assumption, you will typically start with a link function that is the inverse or link = \"inverse\". You need to take this link function into consideration when you report your modelled effects.\nFor a model where link = \"inverse\", the interpretation of the coefficients on the response scale is less clear. In this case, you would use your model to make predictions of your response and report the effects by describing these changes in predictions. We will cover making predictions in the next chapter.\nHowever, you can also use link = \"log\" for your models with a Gamma error distribution assumption. Using “log” will let you use the reporting methods in the section on “Models with a poisson error distribution assumption”(Section 5.2.4). So, if you have a model with a Gamma error distribution assumption, use link = \"log\" for modelling, and follow the methods given above (Section 5.2.4).\n\n\n\nModels with a binomial error distribution assumption:\nBackground\nFor models with a binomial error distribution assumption, you will typically start with a link function that is the logit function or link = \"logit\". You need to take this link function into consideration when you report your modelled effects.\nThe logit function is also called the “log-odds” function as it is equal to the logarithm of the odds.\nModels with a binomial error distribution assumption model the probability of an event happening. This might be presence (vs. absence), mature (vs. immature), death (vs. alive), etc., but in all cases this is represented as the probability of your response being 1 (vs. 0).\nIf \\(p\\) is the probability of an event happening (e.g. response being 1), the probability an event doesn’t happen is \\(1-p\\) when an event can only have two outcomes. The odds are the probability of the event happening divided by the probability it doesn’t happen, or:\n\\(\\frac{p}{1-p}\\)\nThe logit function (or log-odds function) is then:\n\\(log_e(\\frac{p}{1-p})\\)\nJargon alert! Note that a GLM using the logit link function is also called logistic regression.\nAlternate link functions for binomially distributed data are probit and cloglog, but the logit link function is by far the one most used as the coefficients that result from the model are the easiest to interpret. But, as always, you should choose the link function that leads to the best-behaved residuals for your model.\nSimilar to reporting a rate ratio when your link = “log” (Section 5.2.4), you report your coefficients of a model with a binomial error distribution assumption (and link = “logit”) on the response scale by raising e to the power of the coefficients estimated on the link scale.\nWhen you have a continuous predictor, \\(e\\) is raised to the slope (estimated on the link scale) to give you an odds ratio (OR) on the response scale. 1 - OR gives you the % change in odds for a unit change in your continuous predictor.\nWhen you have a categorical predictor, \\(e\\) is raised to the intercept of each level of your predictor (estimated on the link scale) to give you the odds associated with that predictor level on the response scale.\nHere is the math linking the coefficient to the odds ratio:\n\n\n\n\n\n\nthe math\n\n\n\n\n\nGiven a binomial model, \\[\n\\begin{align}\nlog_e(\\frac{p_i}{1-p_i}) &= \\beta_1 \\cdot Cov_i + \\beta_0 \\\\\nResp_i &\\sim binomial(p_i)\n\\end{align}\n\\] \\(p_i\\) is the probability of success and \\(1-p_i\\) is the probability of failure, and the odds are \\(\\frac{p_i}{1-p_i}\\).\nThen the odds ratio (OR), or % change in odds due to a change in predictor is:\n\\[\n\\begin{align}\nOR&=\\frac{odds\\;when\\;Cov=a+1}{odds\\;when \\;Cov=a}\\\\[2em]\nOR&=\\frac{(\\frac{p}{1-p}|Cov=a+1)}{(\\frac{p}{1-p}|Cov=a)}\\\\[2em]\nlog_e(OR)&=(log_e\\frac{p}{1-p}|Cov=a+1) - (log_e\\frac{p}{1-p}|Cov=a)\\\\[2em]\nlog_e(OR)&=(\\beta_1\\cdot (a+1)+\\beta_0) - (\\beta_1\\cdot a+\\beta_0)\\\\[2em]\nlog_e(OR)&=\\beta_1\\cdot a + \\beta_1-\\beta_1\\cdot a\\\\[2em]\nOR &=e^{\\beta_1}\n\\end{align}\n\\]\n\n\n\nOne thing to note:\nThe emmeans() function will convert the coefficients (intercepts) from the link to the response scale. You can ask for this with the type = \"response\" argument.\nIn contrast, the emtrends() function does not convert the coefficients (slopes) to represent effects on the response scale. This is because emtrends() is reporting the slope of a straight line - the trend line on the link scale. But the line isn’t straight on the response scale.\nYou need to convert from the link to the response by hand when we use emtrends()23. How you make the conversion, and interpret the result, will depend on your error distribution assumption:\n\nHere we will go through examples of reporting for models with a binomial error distribution assumption. If you want more details on why you are using the methods below, check the section on “Models with a normal error distribution assumption” (Section 5.2.3) for context.\nExamples\nHere I have made new examples where the error distribution assumption is binomial This changes the response variables, indicated by Resp1.b, Resp2.b, etc. to be values of either 0 or 1.\n\n\n\n\n\n\nExample 1: Resp1.p ~ Cat1 + 1\n\n\n\n\n\nExample 1: Resp1.b ~ Cat1 + 1\nRecall that Example 1 contains only one predictor and it is categorical:\nResp1.b ~ Cat1 + 1\nThe best-specified model (now with a binomial error distribution assumption) is:\n\nbestMod1.b\n\n\nCall:  glm(formula = Resp1.b ~ Cat1 + 1, family = binomial(link = \"logit\"), \n    data = myDat1)\n\nCoefficients:\n(Intercept)      Cat1StB      Cat1StC  \n    -0.6931      -1.8718       1.6094  \n\nDegrees of Freedom: 49 Total (i.e. Null);  47 Residual\nNull Deviance:      68.03 \nResidual Deviance: 51.43    AIC: 57.43\n\n\nthat was fit to data in myDat1.b:\n\nstr(myDat1.b)\n\n'data.frame':   50 obs. of  2 variables:\n $ Cat1   : Factor w/ 3 levels \"StA\",\"StB\",\"StC\": 3 3 2 3 3 1 1 2 3 3 ...\n $ Resp1.b: int  1 0 0 1 1 0 0 0 1 0 ...\n\n\nand the dredge() table you used to pick your bestMod1.b is in dredgeOut1.b\n\ndredgeOut1.b\n\nGlobal model call: glm(formula = Resp1.b ~ Cat1 + 1, family = binomial(link = \"logit\"), \n    data = myDat1)\n---\nModel selection table \n  (Intrc) Cat1    R^2 df  logLik AICc delta weight\n2 -0.6931    + 0.2825  3 -25.714 57.9  0.00  0.998\n1 -0.3228      0.0000  1 -34.015 70.1 12.16  0.002\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n# Set up your predictors for the visualized fit\nforCat1&lt;-unique(myDat1.b$Cat1) # every value of your categorical predictor\n\n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat1=forCat1) # expand.grid() function makes sure you have all combinations of predictors\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod1.b, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n\nlibrary(ggplot2)\n\nggplot() + # start ggplot\n  geom_point(data = myDat1.b, # add observations to your plot\n             mapping = aes(x = Cat1, y = Resp1.b)) + \n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat1, y = Fit, ymin = Lower, ymax = Upper),\n              linewidth=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat1, y = Fit), \n             shape = 22, # shape of point\n             size = 3, # size of point\n             fill = \"white\", # fill colour for plot\n             col = 'black') + # outline colour for plot\n  ylab(\"probability Resp1.b = 1\")+ # change y-axis label\n  xlab(\"Cat1, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nWhat are your modelled effects (with uncertainty)?\nFor categorical predictors, you can get the coefficients on the response scale directly with the emmeans() function24 when you specify type = “response”. These are the probabilities (not odds or odds ratio) that Resp1.b = 1 at each level of your categorical predictor:\n\nlibrary(emmeans) # load the emmeans package\n\nemmOut &lt;- emmeans(object = bestMod1.b, # your model\n            specs = ~ Cat1, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat1   prob     SE  df asymp.LCL asymp.UCL\n StA  0.3333 0.1220 Inf   0.14596     0.594\n StB  0.0714 0.0688 Inf   0.00996     0.370\n StC  0.7143 0.0986 Inf   0.49239     0.866\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nNotice that the column reported is prob: these coefficients are already converted back onto the response scale (as probabilities).\nNote also that two types of uncertainty are measured here. SE stands for the standard error around the prediction, and is a measure of uncertainty of the average modelled effect. The lower.CL and upper.CL represent the 95% confidence limits of the prediction - so if you observed a new Resp1.b at a particular Cat1, there is a 95% probability that the probability Resp1.b is 1 would lie within the bounds of the confidence limits.\nFrom this output you see:\nWhen Cat1 is SpA, the probability that Resp1.b is 1 is estimated to be 0.33 (95% confidence interval: 0.15 to 0.59 units). This is equivalent to odds (\\(\\frac{p}{(1-p)}\\)) of 0.5.\nWhen Cat1 is SpB, the probability that Resp1.b is 1 is estimated to be 0.07 (95% confidence interval: 0.01 to 0.37 units). This is equivalent to odds (\\(\\frac{p}{(1-p)}\\)) of 0.08.\nWhen Cat1 is SpC, the probability that Resp1.b is 1 is estimated to be 0.71 (95% confidence interval: 0.49 to 0.87 units). This is equivalent to odds (\\(\\frac{p}{(1-p)}\\)) of 2.5.\nFinally, note that you can also get a quick plot of the effects by handing the emmeans() output to plot().\n\n\n\n\n\n\nwhere do these probabilities and odds come from?\n\n\n\n\n\nThe emmeans package provides an easy way of seeing the effects of Cat1 on Resp1.b the response scale, but you can also get this information from the summary() output:\n\ncoef(summary(bestMod1.b))\n\n              Estimate Std. Error   z value   Pr(&gt;|z|)\n(Intercept) -0.6931472  0.5477226 -1.265508 0.20568935\nCat1StB     -1.8718022  1.1734223 -1.595165 0.11067536\nCat1StC      1.6094379  0.7302967  2.203814 0.02753745\n\n\nYou can convert the coefficients to odds with the exp() function, for example:\nWhen Cat1 is SpA, the odds that Resp1.b is 1 is estimated to be exp(coef(summary(bestMod1.b))[1]) = 0.5. And you can get the probabilities directly with the plogis() function (plogis(coef(summary(bestMod1.b))[1]) = 0.333).\n\n\n\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nHere you can compare the effects of the levels of Cat1 on Resp1.b to see if the effects differ between the levels.\n\nforComp &lt;- pairs(emmOut)\n\nforComp\n\n contrast  odds.ratio     SE  df null z.ratio p.value\n StA / StB     6.5000 7.6300 Inf    1   1.595  0.2477\n StA / StC     0.2000 0.1460 Inf    1  -2.204  0.0705\n StB / StC     0.0308 0.0352 Inf    1  -3.041  0.0067\n\nP value adjustment: tukey method for comparing a family of 3 estimates \nTests are performed on the log odds ratio scale \n\n\nThe output shows the results of the multiple comparison (pairwise) testing.\nThe values in the p.value column tell you the results of the hypothesis test comparing the coefficients between the two levels of your categorical predictor.\nNote the odds.ratio column here. Comparing effects of Cat1 on the probability of Resp1.b is done by comparing odds of Resp1.b being 1 under each level of Cat1.\nFor example:\n\nthere is no evidence that the probability of Resp1.b being 1 is different when Cat 1 = SpA vs. when Cat1 = SpB (P = 0.2477).\nthere is little evidence that the probability of Resp1.b being 1 is different when Cat 1 = SpA than when Cat1 = SpC (P = 0.0705). The odds ratio is 0.2.\nthere is strong evidence that the probability of Resp1.b being 1 is different when Cat 1 = SpB than when Cat1 = SpC (P = 0.0067). The odds ratio is 0.0308.\n\n\n\n\n\n\n\nwhere did this odds ratio come from?\n\n\n\n\n\nThis is a ratio of the odds of that Resp1.b is 1 when Cat 1 = SpA vs. the odds of that Resp1.b is 1 when Cat 1 = SpC:\nFor example, the odds that Resp1.b is 1 when Cat 1 = SpA are:\n\npCat1.SpA &lt;- summary(emmOut)[1,2]\n\noddsCat1.SpA &lt;- pCat1.SpA /(1-pCat1.SpA)\n\noddsCat1.SpA\n\n[1] 0.5\n\n\nand the odds that Resp1.b is 1 when Cat 1 = SpC are:\n\npCat1.SpC &lt;- summary(emmOut)[3,2]\n\noddsCat1.SpC &lt;- pCat1.SpC /(1-pCat1.SpC)\n\noddsCat1.SpC\n\n[1] 2.5\n\n\nand the odds ratio StA/StB is\n\noddsCat1.SpA/oddsCat1.SpC\n\n[1] 0.2\n\n\n\n\n\nNote that you can also get the results from the pairwise testing visually by handing the output of pairs() to plot().\n\n\n\n\n\n\n\n\n\nExample 2: Resp2.p ~ Cat2 + Cat3 + Cat2:Cat3 + 1\n\n\n\n\n\nExample 2: Resp2.b ~ Cat2 + Cat3 + Cat2:Cat3 + 1\nRecall that Example 2 contains two predictors and both are categorical:\nResp2.b ~ Cat2 + Cat3 + Cat2:Cat3 + 1\nThe best-specified model (now with binomial error distribution assumption) is:\n\nbestMod2.b\n\n\nCall:  glm(formula = Resp2.b ~ Cat2 + Cat3 + Cat2:Cat3 + 1, family = binomial(link = \"logit\"), \n    data = myDat2.b)\n\nCoefficients:\n      (Intercept)            Cat2StB            Cat2StC          Cat3Treat  \n         -0.15415            0.23639           -0.08352           -0.10368  \nCat2StB:Cat3Treat  Cat2StC:Cat3Treat  \n         -0.29222            1.99958  \n\nDegrees of Freedom: 499 Total (i.e. Null);  494 Residual\nNull Deviance:      692.9 \nResidual Deviance: 649.7    AIC: 661.7\n\n\nthat was fit to data in myDat2.b:\n\nstr(myDat2.b)\n\n'data.frame':   500 obs. of  3 variables:\n $ Cat2   : Factor w/ 3 levels \"StA\",\"StB\",\"StC\": 2 2 3 2 2 3 2 1 1 1 ...\n $ Cat3   : Factor w/ 2 levels \"Control\",\"Treat\": 1 1 1 2 1 2 1 2 2 1 ...\n $ Resp2.b: int  0 0 0 1 1 1 0 0 0 1 ...\n\n\nand the dredge() table you used to pick your bestMod2.b is in\n\ndredgeOut2.b\n\nGlobal model call: glm(formula = Resp2.b ~ Cat2 + Cat3 + Cat2:Cat3 + 1, family = binomial(link = \"logit\"), \n    data = myDat2.b)\n---\nModel selection table \n     (Int) Ct2 Ct3 Ct2:Ct3      R^2 df   logLik  AICc delta weight\n8 -0.15420   +   +       + 0.082720  6 -324.843 661.9  0.00      1\n4 -0.38060   +   +         0.031870  4 -338.331 684.7 22.89      0\n2 -0.20190   +             0.023290  3 -340.538 687.1 25.27      0\n3 -0.11690       +         0.007163  2 -344.632 693.3 31.43      0\n1  0.04801                 0.000000  1 -346.430 694.9 33.01      0\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n# Set up your predictors for the visualized fit\nforCat2&lt;-unique(myDat2.b$Cat2) # every level of your categorical predictor\nforCat3&lt;-unique(myDat2.b$Cat3) # every level of your categorical predictor\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat2 = forCat2, Cat3 = forCat3) # expand.grid() function makes sure you have all combinations of predictors\n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod2.b, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  geom_point(data = myDat2.b, # add observations to your plot\n             mapping = aes(x = Cat2, y = Resp2.b, col = Cat3), \n             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot\n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat2, y = Fit, ymin = Lower, ymax = Upper, col = Cat3),\n              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot\n              size=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat2, y = Fit, fill = Cat3), \n             shape = 22, # shape of point\n             size=3, # size of point\n             col = 'black', # outline colour for point\n             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot\n  ylab(\"probability Resp2.b = 1\")+ # change y-axis label\n  xlab(\"Cat2, (units)\")+ # change x-axis label\n  labs(fill=\"Cat3, (units)\", col=\"Cat3, (units)\") + # change legend title\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nWhat are your modelled effects (with uncertainty)?\nAs above, we can use the emmeans package to see the effects of the predictors on the scale of the response:\n\nemmOut &lt;- emmeans(object = bestMod2.b, # your model\n            specs = ~ Cat2 + Cat3 + Cat2:Cat3, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat2 Cat3     prob     SE  df asymp.LCL asymp.UCL\n StA  Control 0.462 0.0523 Inf     0.362     0.564\n StB  Control 0.521 0.0585 Inf     0.407     0.632\n StC  Control 0.441 0.0515 Inf     0.344     0.543\n StA  Treat   0.436 0.0561 Inf     0.331     0.547\n StB  Treat   0.422 0.0521 Inf     0.325     0.526\n StC  Treat   0.840 0.0423 Inf     0.739     0.907\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nSo now you have a modelled value of your response for each level of your categorical predictors. For example:\nWhen Cat2 is StA and Cat3 is Treat, the probability that Resp1.b is 1 is estimated to be 0.44 (95% confidence interval: 0.33 to 0.55 units).\nWhen Cat2 is StB and Cat3 is Control, the probability that Resp1.b is 1 is estimated to be 0.52 (95% confidence interval: 0.41 to 0.63 units).\nFinally, note that you can also get a quick plot of the effects by handing the emmeans() output to plot().\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nAs with Example 1, you can also find out which combinations of predictor levels are leading to statistically different model predictions in Resp2.b:\n\nforComp &lt;- pairs(emmOut, # emmeans output\n                 simple = \"Cat2\") # contrasts within this categorical predictor\n\nforComp\n\nCat3 = Control:\n contrast  odds.ratio     SE  df null z.ratio p.value\n StA / StB      0.789 0.2490 Inf    1  -0.751  0.7331\n StA / StC      1.087 0.3220 Inf    1   0.282  0.9572\n StB / StC      1.377 0.4320 Inf    1   1.019  0.5646\n\nCat3 = Treat:\n contrast  odds.ratio     SE  df null z.ratio p.value\n StA / StB      1.057 0.3300 Inf    1   0.179  0.9826\n StA / StC      0.147 0.0573 Inf    1  -4.925  &lt;.0001\n StB / StC      0.139 0.0530 Inf    1  -5.183  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \nTests are performed on the log odds ratio scale \n\n\nBased on a threshold P-value of 0.05, you can see that the effects at some combinations of your predictors are not statistically different from each other. For example, there is no evidence that the probability of Resp2.b being 1 when Cat3 = Control and Cat2 = StA (0.46)25 than when Cat3 = Control and Cat2 = StB (0.52) are different from each other (P = 0.73).\nOn the other hand, some combinations of your predictors are statistically different from each other. For example, there is strong evidence that the probability of Resp2.b being 1 when Cat3 = Treat and Cat2 = StA (0.46) is less than that when Cat3 = Treat and Cat2 = StC (0.84) are different from each other (P &lt; 0.0001).\nNote that you can also get the results from the pairwise testing visually by handing the output of pairs() to plot().\n\n\n\n\n\n\n\n\n\nExample 3: Resp3.p ~ Cont4 + 1\n\n\n\n\n\nExample 3: Resp3.b ~ Cont4 + 1\nRecall that Example 3 contains one predictor and the predictor is continuous:\nResp3 ~ Cont4 + 1\nThe best-specified model (now with a binomial error distribution assumption) is:\n\nbestMod3.b\n\n\nCall:  glm(formula = Resp3.b ~ Cont4 + 1, family = binomial(link = \"logit\"), \n    data = myDat3.b)\n\nCoefficients:\n(Intercept)        Cont4  \n    -8.1519       0.1086  \n\nDegrees of Freedom: 299 Total (i.e. Null);  298 Residual\nNull Deviance:      415.8 \nResidual Deviance: 199.1    AIC: 203.1\n\n\nthat was fit to data in myDat3.b:\n\nstr(myDat3.b)\n\n'data.frame':   300 obs. of  2 variables:\n $ Cont4  : num  57.7 53.2 79.7 35.1 72.2 ...\n $ Resp3.b: int  1 0 1 0 0 1 1 1 1 0 ...\n\n\nand the dredge() table you used to pick your bestMod3.b is\n\ndredgeOut3.b\n\nGlobal model call: glm(formula = Resp3.b ~ Cont4 + 1, family = binomial(link = \"logit\"), \n    data = myDat3.b)\n---\nModel selection table \n   (Intrc)  Cont4    R^2 df   logLik  AICc  delta weight\n2 -8.15200 0.1086 0.5144  2  -99.533 203.1   0.00      1\n1  0.04001        0.0000  1 -207.884 417.8 214.68      0\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n\n# Set up your predictors for the visualized fit\nforCont4&lt;-seq(from = min(myDat3.b$Cont4), to = max(myDat3.b$Cont4), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cont4 = forCont4) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod3.b, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat3.b, # add observations to your plot\n             mapping = aes(x = Cont4, y = Resp3.b)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont4, y = Fit),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont4, y = Upper),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont4, y = Lower),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ylab(\"probability Resp2.b = 1\")+ # change y-axis label\n  \n  xlab(\"Cont4, (units)\") + # change x-axis label\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nThe model fit line may seem a little strange as the data are binary (can only be 0 or 1) but the model fit line goes through the space in between 0 and 1. This model fit line tells you how the probability of Resp3.b being 1 changes as Cont4 changes.\nWhat are your modelled effects (with uncertainty)?\n\ntrendsOut &lt;- emtrends(bestMod3.b,\n                      specs = ~ Cont4 + 1, # your predictors\n                      var = \"Cont4\") # your predictor of interest\n\ntrendsOut\n\n Cont4 Cont4.trend     SE  df asymp.LCL asymp.UCL\n  75.8       0.109 0.0116 Inf    0.0859     0.131\n\nConfidence level used: 0.95 \n\n\nNote that this is the same estimate as you find in the summary() output of your model object:\n\ncoef(summary(bestMod3.b))\n\n              Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -8.1518996 0.89160247 -9.142976 6.075711e-20\nCont4        0.1085946 0.01160302  9.359159 8.037377e-21\n\n\nThis is because emtrends() returns coefficients on the link scale.\nTo report the coefficients on a response scale, you need to consider your link function. Since you have a binomial error distribution assumption, you can convert the estimate made by emtrends() to the response scale with:\n\ntrendsOut.df &lt;- data.frame(trendsOut) # convert trendsOut to a dataframe\n\nOR &lt;- exp(trendsOut.df$Cont4.trend) # get the coefficient on the response scale\n\nOR\n\n[1] 1.11471\n\n\nYou can also use the same conversion on the confidence limits of the modelled effect, for the upper:\n\nOR.up &lt;- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale\n  \nOR.up\n\n[1] 1.140351\n\n\nand lower 95% confidence level:\n\nOR.low &lt;- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale\n\nOR.low\n\n[1] 1.089646\n\n\nThis coefficient - called the odds ratio (Section 5.2.6) - tells you that for a unit change in Cont4, the odds that Resp3.b is 1 increases by 1.115 times (95% confidence interval is 1.09 to 1.14) which is an increase in the odds of Resp3.b being 1 of 11.5%.\n\n\n\n\n\n\nwhere did this odds ratio come from?\n\n\n\n\n\nYou can get a better sense of the odds ratio (vs. odds vs. probabilities) by estimating it directly from the modelled probability that Resp3.b is 1:\nLet’s find the probability of Resp3.b = 1 when Cont4 is 60:\n\npCont4.60 &lt;- predict(bestMod3.b, # model\n                 newdata = data.frame(Cont4 = 60), # predictor values for the prediction\n                 type = \"response\", # give prediction on the response scale\n                 se.fit = TRUE) # include uncertainty\n\npCont4.60 &lt;- pCont4.60$fit\n\npCont4.60\n\n        1 \n0.1629792 \n\n\nSo there is a 16.3% probability of Resp3.b being 1 when Cont4 is 60.\nThe odds associated with Resp3.b being 1 when Cont4 is 60 is given by the probability of Cont4 is divided by the probability of absence:\n\noddsCont4.60 &lt;- pCont4.60/(1-pCont4.60)\n\noddsCont4.60\n\n        1 \n0.1947134 \n\n\nNow let’s find the probability of Resp3.b being 1 when Cont4 is 61:\n\npCont4.61 &lt;- predict(bestMod3.b, # model\n                 newdata = data.frame(Cont4 = 61), # predictor values for the prediction\n                 type = \"response\", # give prediction on the response scale\n                 se.fit = TRUE) # include uncertainty\n\npCont4.61 &lt;- pCont4.61$fit\n\npCont4.61\n\n        1 \n0.1783404 \n\n\nSo there is a 17.8% probability of Resp3.b being 1 when Cont4 is 61.\nThe odds associated with Resp3.b being 1 when Cont4 is 61 is given by the probability of presence divided by the probability of absence:\n\noddsCont4.61 &lt;- pCont4.61/(1-pCont4.61)\n\noddsCont4.61\n\n       1 \n0.217049 \n\n\nNow the odds ratio is found by comparing the change in odds when your predictor (Cont4) changes one unit\n\noddsCont4.61/oddsCont4.60\n\n      1 \n1.11471 \n\n\nNote that this is identical to the calculation of the odds ratio from the coefficient above:\n\nOR\n\n[1] 1.11471\n\n\nand you interpret this as a OR - 1 or 11.5% change in the odds of Resp3.b being 1 with a unit change of Cont4.\n\n\n\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nThis question only applies to categorical predictors of which their are none in Example 3.\n\n\n\n\n\n\n\n\n\nExample 4: Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1\n\n\n\n\n\nExample 4: Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nRecall that Example 4 contains two predictors, one is categorical and one is continuous:\nResp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1\nThe best-specified model (now with a binomial error distribution assumption) is:\n\nbestMod4.b\n\n\nCall:  glm(formula = Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1, family = binomial(link = \"logit\"), \n    data = myDat4.b)\n\nCoefficients:\n  (Intercept)        Cat5StB        Cat5StC          Cont6  Cat5StB:Cont6  \n    -28.55811       19.23667      -10.40094        0.15899       -0.09544  \nCat5StC:Cont6  \n      0.03623  \n\nDegrees of Freedom: 499 Total (i.e. Null);  494 Residual\nNull Deviance:      639.7 \nResidual Deviance: 169  AIC: 181\n\n\nthat was fit to data in myDat4.b:\n\nstr(myDat4.b)\n\n'data.frame':   500 obs. of  3 variables:\n $ Cat5   : Factor w/ 3 levels \"StA\",\"StB\",\"StC\": 2 1 1 1 2 1 2 1 3 2 ...\n $ Cont6  : num  206 191 312 223 210 ...\n $ Resp4.b: int  1 1 1 1 1 1 1 0 1 1 ...\n\n\nand the dredge() table you used to pick your bestMod4.b is\n\ndredgeOut4.b\n\nGlobal model call: glm(formula = Resp4.b ~ Cat5 + Cont6 + Cat5:Cont6 + 1, family = binomial(link = \"logit\"), \n    data = myDat4.b)\n---\nModel selection table \n     (Int) Ct5     Cn6 Ct5:Cn6     R^2 df   logLik  AICc  delta weight\n8 -28.5600   + 0.15900       + 0.60990  6  -84.513 181.2   0.00      1\n4 -19.4300   + 0.10820         0.59390  4  -94.548 197.2  15.98      0\n3 -12.4100     0.07056         0.49520  2 -148.970 302.0 120.77      0\n2   0.5276   +                 0.07603  3 -300.081 606.2 425.01      0\n1   0.6722                     0.00000  1 -319.850 641.7 460.51      0\nModels ranked by AICc(x) \n\n\nAnd here’s a visualization of the model effects:\n\n#### i) choosing the values of your predictors at which to make a prediction\n\n\n# Set up your predictors for the visualized fit\nforCat5&lt;-unique(myDat4.b$Cat5) # all levels of your categorical predictor\nforCont6&lt;-seq(from = min(myDat4.b$Cont6), to = max(myDat4.b$Cont6), by = 1)# a sequence of numbers in your continuous predictor range\n  \n# create a data frame with your predictors\nforVis&lt;-expand.grid(Cat5 = forCat5, Cont6 = forCont6) # expand.grid() function makes sure you have all combinations of predictors.  \n\n#### ii) using `predict()` to use your model to estimate your response variable at those values of your predictor\n\n\n# Get your model fit estimates at each value of your predictors\nmodFit&lt;-predict(bestMod4.b, # the model\n                newdata = forVis, # the predictor values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n#### iii) use the model estimates to plot your model fit\n\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat4.b, # add observations to your plot\n             mapping = aes(x = Cont6, y = Resp4.b, col = Cat5)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont6, y = Fit, col = Cat5),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont6, y = Upper, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont6, y = Lower, col = Cat5),\n              size = 0.4, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ylab(\"probability Resp2.b = 1\")+ # change y-axis label\n  \n  xlab(\"Cont6, (units)\") + # change x-axis label\n  \n  labs(fill=\"Cat5, (units)\", col=\"Cat5, (units)\") + # change legend title\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nWhat are your modelled effects (with uncertainty)?\nThe process for estimating effects of categorical vs. continuous predictors is different.\nFor categorical predictors (Cat5), you can use emmeans() to give you the effects on the response scale directly:\n\nemmOut &lt;- emmeans(object = bestMod4.b, # your model\n            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors\n            type = \"response\") # report coefficients on the response scale\n\nemmOut\n\n Cat5 Cont6  prob     SE  df asymp.LCL asymp.UCL\n StA    198 0.951 0.0346 Inf     0.819     0.988\n StB    198 0.964 0.0178 Inf     0.907     0.986\n StC    198 0.438 0.0941 Inf     0.269     0.622\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nThis output tells you that:\n\nWhen Cat5 is StA and Cont6 is 227 units26, there is a 95% probability that Resp4.b will be 1 (95% confidence interval: 0.82 to 0.99%)\nWhen Cat5 is StB and Cont6 is 227 units27, there is a 96% probability that Resp4.b will be 1 (95% confidence interval: 0.91 to 0.99 units).\nWhen Cat5 is StB and Cont6 is 227 units28, there is a 44% probability that Resp4.b will be 1 (95% confidence interval: 0.27 to 0.62 units).\n\nNote that emmeans() sets your continuous predictor (Cont6) to the mean value of Cont6 (227 units). You can also control this with the at = argument in the emmeans() function:\n\nemmOut &lt;- emmeans(object = bestMod4.b, # your model\n            specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1, # your predictors\n            type = \"response\", # report coefficients on the response scale\n            at = list(Cont6 = 150)) # control the value of your continuous predictor at which to make the coefficient estimates\n\n\nemmOut\n\n Cat5 Cont6     prob       SE  df asymp.LCL asymp.UCL\n StA    150 8.93e-03 0.009230 Inf  1.17e-03   0.06500\n StB    150 5.53e-01 0.078500 Inf  3.99e-01   0.69695\n StC    150 6.27e-05 0.000142 Inf  7.00e-07   0.00524\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\n\nFor continuous predictors (Cont6), you need to use the emtrends() function and then convert the coefficients to the response scale:\n\ntrendsOut &lt;- emtrends(bestMod4.b,\n                      specs = ~ Cat5 + Cont6 + Cat5:Cont6 + 1 , # your predictors\n                      var = \"Cont6\") # your predictor of interest\n\ntrendsOut\n\n Cat5 Cont6 Cont6.trend     SE  df asymp.LCL asymp.UCL\n StA    198      0.1590 0.0330 Inf    0.0942    0.2237\n StB    198      0.0635 0.0114 Inf    0.0413    0.0858\n StC    198      0.1952 0.0444 Inf    0.1083    0.2821\n\nConfidence level used: 0.95 \n\n\nSince you have a binomial error distribution assumption, you can convert the estimate made by emtrends() on to the response scale with the exp() function:\n\ntrendsOut.df &lt;- data.frame(trendsOut) # convert trendsOut to a dataframe\n\nOR &lt;- exp(trendsOut.df$Cont6.trend) # get the coefficient on the response scale\n\nOR\n\n[1] 1.172328 1.065611 1.215576\n\n\nYou can also use the same conversion on the confidence limits of the modelled effect, for the upper:\n\nOR.up &lt;- exp(trendsOut.df$asymp.UCL) # get the upper confidence interval on the response scale\n  \nOR.up\n\n[1] 1.250737 1.089631 1.325976\n\n\nand lower 95% confidence level:\n\nOR.low &lt;- exp(trendsOut.df$asymp.LCL) # get the lower confidence interval on the response scale\n\nOR.low\n\n[1] 1.098834 1.042120 1.114368\n\n\nLet’s organize these numbers so we can read the effects easier:\n\nORAll &lt;- data.frame(Cat5 = trendsOut.df$Cat5, # include info on the Cat5 level\n                    OR = exp(trendsOut.df$Cont6.trend), # the modelled effect as a rate ratio\n                    OR.up = exp(trendsOut.df$asymp.UCL), # upper 95% confidence level of the modelled effect\n                    OR.down = exp(trendsOut.df$asymp.LCL)) # lower 95% confidence level of the modelled effect\n\nORAll\n\n  Cat5       OR    OR.up  OR.down\n1  StA 1.172328 1.250737 1.098834\n2  StB 1.065611 1.089631 1.042120\n3  StC 1.215576 1.325976 1.114368\n\n\nThis tells you that for a unit change in Cont6, the odds that Resp4.b is 1:\n\nincreases by 17.2% with a unit change in Cont6 when Cat5 is StA (95% confidence interval: 9.9 to 25.1).\nincreases by 6.6% with a unit change in Cont6 when Cat5 is StB (95% confidence interval: 4.2 to 9).\nincreases by 21.6% with a unit change in Cont6 when Cat5 is StC (95% confidence interval: 11.4 to 32.6).\n\nYou can test for evidence that the effects of Cont6 on Resp4.b are significant (different than zero) with:\n\ntest(trendsOut) # get test if coefficient is different than zero.\n\n Cat5 Cont6 Cont6.trend     SE  df z.ratio p.value\n StA    198      0.1590 0.0330 Inf   4.813  &lt;.0001\n StB    198      0.0635 0.0114 Inf   5.588  &lt;.0001\n StC    198      0.1952 0.0444 Inf   4.401  &lt;.0001\n\n\nNote that these tests are done on the link scale but can be used for reporting effects on the response scale.\nFrom the results, you can see that the effects of Cont6 on Resp4.b at all levels of Cat5 are significant (P &lt; 0.0001).\nAre the modelled effects different than one another across categorical levels? (only needed for categorical predictors with more than two levels)\nYou can also find out which effects of Cont6 on Resp4.b are different from one another using pairs() on the emtrends() output:\n\nforCompOR &lt;- pairs(trendsOut, \n                   simple = \"Cat5\")\n\nforCompOR\n\nCont6 = 198:\n contrast  estimate     SE  df z.ratio p.value\n StA - StB   0.0954 0.0349 Inf   2.732  0.0173\n StA - StC  -0.0362 0.0553 Inf  -0.655  0.7895\n StB - StC  -0.1317 0.0458 Inf  -2.876  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nThe table above tells you that:\n\nThere is evidence that the effect of Cont6 on Resp4.b is different when Cat5 is StA vs. StB (P = 0.017). The effect of Cont6 on Resp4.b is exp(0.0954) = 1.1 times as big when Cat5 is StA vs. StB (i.e. the effect of Cont6 on Resp4.b is 10% more when Cat5 is StA vs. StB)\nThere is no evidence the effect of Cont6 on Resp4.b differs when Cat5 is StA vs. StC (P = 0.789).\nThere is evidence that the effect of Cont6 on Resp4.b is different when Cat5 is StB vs. StC (P = 0.011). The effect of Cont6 on Resp4.b is exp(-0.1317) = 0.88 times as big when Cat5 is StB vs. StC (i.e. the effect of Cont6 on Resp4.b is 12% less when Cat5 is StB vs. StC)\n\nOnly if all the slopes29 were similar, you would want to test if the levels of your categorical predictor (Cat5) have significantly different coefficients (intercepts) from each other with pairs() on the output from emmeans() (the emmOut object)."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#a-familiar-example",
    "href": "DSPPH_SM_Reporting.html#a-familiar-example",
    "title": "From statistical modelling to scientific report writing",
    "section": "A familiar example",
    "text": "A familiar example\n\nI’ll use the following example case as I discuss how to communicate your results:\nYou are interested in moose (or elk, Alces alces) and how their growth rate changes from population to population. You find information on mean annual weight change by moose in 12 locations in Sweden. You wonder if latitude can explain some of the variability in growth rates you are observing. You also wonder if minimum winter temperature might explain some variability in growth rate. Finally, you wonder if changes in growth rate with latitude and/or winter temperature depend on each other (i.e, latitudinal dependent effects on growth rate depend on what the minimum winter temperature is) and/or sex (male and female) - i.e. the environmentally dependent change in growth rate depends on what sex the animal is.\n\nAnd here’s a screenshot of the readme file for the moose data:"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#responses",
    "href": "DSPPH_SM_Reporting.html#responses",
    "title": "From statistical modelling to scientific report writing",
    "section": "Response(s)",
    "text": "Response(s)\n\nThe task:\nYour response variable30 is the observed variability you are trying to explain. Your response forms your research question - i.e. “why is my response varying?”. In this section, you need to identify the variability you’re trying to explain and why (your motivation).\n\n\nThe code:\nCode supporting your tasks in this section allow you to import your data and help you describe your response variable, e.g.:\n\nrm(list=ls()) # make sure your workspace is clear\n\n#Import the data\nmyDat &lt;- read.table(\"ExDat.csv\", # file name\n                    sep = ',',  # lines in the file are separated by a comma - this information is in the README file\n                    dec = '.', # decimals are denoted by '.' - this information is in the README file\n                    stringsAsFactors = TRUE, # make sure strings (characters) are imported as factors\n                    header = TRUE) # there is a column header in the file\n\nstr(myDat) # check the structure of the data file\n\n'data.frame':   24 obs. of  6 variables:\n $ Study  : int  1 2 3 4 5 6 7 8 9 12 ...\n $ Sex    : Factor w/ 2 levels \"Females\",\"Males\": 1 1 1 1 1 1 1 1 1 1 ...\n $ delMass: num  40.2 38.7 41.2 39.4 36.2 40 44.1 42.2 46.2 43.6 ...\n $ Lat    : num  58 57.7 58 57.9 59.8 61.6 62 62.7 64 65.5 ...\n $ WTemp  : num  -2.4 -1.7 -1.7 -1.7 -4.2 -5.6 -8.1 -7.7 -7.6 -9.3 ...\n $ Source : Factor w/ 3 levels \"Gothenburg\",\"Lund\",..: 2 3 3 2 3 2 1 1 3 3 ...\n\nsummary(myDat) # get summary information of each column\n\n     Study            Sex        delMass           Lat            WTemp        \n Min.   : 1.00   Females:12   Min.   :36.20   Min.   :57.70   Min.   :-11.900  \n 1st Qu.: 3.75   Males  :12   1st Qu.:41.95   1st Qu.:58.00   1st Qu.: -8.400  \n Median : 6.50                Median :47.35   Median :61.80   Median : -6.600  \n Mean   : 7.00                Mean   :47.77   Mean   :61.60   Mean   : -6.100  \n 3rd Qu.: 9.75                3rd Qu.:54.23   3rd Qu.:64.38   3rd Qu.: -2.225  \n Max.   :14.00                Max.   :60.60   Max.   :66.00   Max.   : -1.700  \n        Source  \n Gothenburg: 6  \n Lund      : 8  \n Stockholm :10  \n                \n                \n                \n\n\n\n\nThe write-up:\nUse the code above, the data readme file, as well as supporting literature to communicate about your response variable and motivation. Include:\n\nwhat your response variable is\nwhy you want to explain variability in your response\nyour research question\nhow your response variable was measured and a description of how your response varies. For example, give the range in your response if possible. If your response is a category, give the levels of the category (e.g. alive vs. dead). Always give units, if applicable.\n\nFor example:\n\nHere I want to explain variability in growth rate changes in moose (Alces alces). Moose size will be related to survival rates, and likely also their ability to reproduce (e.g. Sand et al. 1995). Being able to explain what controls growth rate in moose may help us explain why some populations are more productive than other populations, and it will help us predict productivity for populations at other places or times.\nMy research question is “why does growth rate vary?”. My observations of growth rate were measured from adult moose across 12 locations in Sweden. The growth rate varied from 36.2 to 60.6 kg per year."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#covariates",
    "href": "DSPPH_SM_Reporting.html#covariates",
    "title": "From statistical modelling to scientific report writing",
    "section": "Covariate(s)",
    "text": "Covariate(s)\n\nThe task:\nHere you will identify the covariates in your model - i.e. what variables might explain variability in your response. To write-up this section, start with the biological mechanism (or processes) that make you think the covariate could be influencing your response. Then describe how that covariate was measured. At this point you might identify mechanisms that you are not able to measure, e.g. because they were beyond the scope of your study. Make note of these. You will have a chance to talk about them at the end of this exercise (in Reporting).\n\n\nThe code:\nCode supporting your tasks in this section will help you describe your covariates, e.g.:\n\n# Getting information on my covariates\nrange(myDat$Lat) # ranges for continuous covariates\n\n[1] 57.7 66.0\n\nrange(myDat$WTemp) # ranges for continuous covariates\n\n[1] -11.9  -1.7\n\nunique(myDat$Sex) # factor levels for categorical covariates\n\n[1] Females Males  \nLevels: Females Males\n\n\n\n\nThe write-up:\nUse the code above, the data readme file, as well as supporting literature to communicate about your covariate variables. Include:\n\nwhat your covariate variables are\nwhy you think each covariate might explain variability in your response (mechanisms)\nhow each covariate variable was measured and a description of the covariate. This means at least giving the range of each covariate (for continuous covariates) or the levels of any categorical covariate. Remember to include units where applicable.\n\nFor example:\n\nHere I consider the effects of latitude (˚N), minimum winter temperature (˚C) and sex (male vs. female) on annual growth in adult moose.\nLatitude may impact the growth of the moose as the environment changes with latitude (e.g. temperature, food, light level, growing season). Differences in food availability may lead to growth differences [citation]31. In this study, latitude is measured in decimal degrees N and ranges from 57.7 to 66.0˚N.\nWinter harshness may impact annual moose growth by making energetic losses larger in the winter due to increased costs of temperature regulation [citation]. Here, I measure winter harshness as minimum winter temperature (˚C) which ranged from -11.9 to -1.7˚C in my study.\nGrowth differences might also occur due to sex of the animal as male and female moose have different life history strategies (e.g. reproductive investment)[citation]. Here, I include effects of sex with observations of male vs. female for each growth rate measurement."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#hypothesis",
    "href": "DSPPH_SM_Reporting.html#hypothesis",
    "title": "From statistical modelling to scientific report writing",
    "section": "Hypothesis",
    "text": "Hypothesis\n\nThe task:\nIn this section you will describe your research hypothesis including main effects and any interactions. Remember to include the formula notation we’ve been practicing in class (i.e. Response ~ Covariate…)\n\n\nThe code:\nNo code needed here!\n\n\nThe write-up:\nUse your answers to the previous sections to state your research hypothesis. Remember to consider interactions as well as the main effects. Include:\n\na description in words\na description in the formula notation\na definition of each term\n\nFor example,\n\nI will test the research hypothesis that varibility in adult moose growth rate (delMass, kg per year) is explained by latitude (Lat, ˚N), minimum winter temperature (WTemp, ˚C) and sex (Sex, male or female). I will test this by modelling:\ndelMass ~ Lat + WTemp + Sex + Lat:Sex + WTemp:Sex + Lat:WTemp + Lat:WTemp:Sex\nwhich includes main effects and all possible interaction effects of my covariates (where “:” indicates an interaction between covariates)."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#starting-model",
    "href": "DSPPH_SM_Reporting.html#starting-model",
    "title": "From statistical modelling to scientific report writing",
    "section": "Starting model",
    "text": "Starting model\n\nThe task:\nThis step involves describing how you built the statistical model to test your hypothesis. Here, you will:\n\nChoose your error distribution assumption\nChoose your shape assumption\nChoose your starting model\nFit your starting model\n\n\n\nThe code:\nCode supporting your tasks in this section will help you support your choices of your model assumptions, e.g.:\n\n# Choosing your error distribution assumption:\n## Understanding the nature of your response variable\nsummary(myDat$delMass) # a summary description of delMass\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  36.20   41.95   47.35   47.77   54.23   60.60 \n\n# Choosing your shape assumption:\n## This can include plotting your response variable vs. each continuous covariate\n### For Lat:\nlibrary(ggplot2) # load ggplot2 library\nggplot()+ # start ggplot\n  geom_point(data = myDat,\n             mapping = aes(x = Lat, y = delMass))+ # add observations\n  xlab(\"Latitude, ˚N\")+ # change x-axis label\n  ylab(\"Annual growth rate, kg per year\")+ # change y-axis label\n  labs(caption = \"Figure 1: Observed annual growth rate (kg per year) vs. latitude (˚N)\")+ # add figure caption\n  theme_bw()+ # change theme of plot\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n#### Relationship between Lat and delMass looks linear\n\n\n### For WTemp:\nggplot()+ # start ggplot\n  geom_point(data = myDat,\n             mapping = aes(x = WTemp, y = delMass))+ # add observations\n  xlab(\"Minimum winter temperature, ˚C\")+ # change x-axis label\n  ylab(\"Annual growth rate, kg per year\")+ # change y-axis label\n  labs(caption = \"Figure 2: Observed annual growth rate (kg per year) vs. minimum winter temperature (˚C)\")+ # add figure caption\n  theme_bw()+ # change theme of plot\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n#### Relationship between WTemp and delMass looks linear\n\n\n# Fitting your starting model\nstartMod&lt;-glm(formula = delMass ~ Lat + WTemp + Sex + Lat:Sex + WTemp:Sex + Lat:WTemp + Lat:WTemp:Sex, # hypothesis\n              data = myDat, # data\n              family = gaussian(link = \"identity\")) # error distribution assumption\n\nstartMod\n\n\nCall:  glm(formula = delMass ~ Lat + WTemp + Sex + Lat:Sex + WTemp:Sex + \n    Lat:WTemp + Lat:WTemp:Sex, family = gaussian(link = \"identity\"), \n    data = myDat)\n\nCoefficients:\n       (Intercept)                 Lat               WTemp            SexMales  \n          13.86368             0.43887             1.37886           -54.84374  \n      Lat:SexMales      WTemp:SexMales           Lat:WTemp  Lat:WTemp:SexMales  \n           1.09654            -5.46475            -0.02401             0.08799  \n\nDegrees of Freedom: 23 Total (i.e. Null);  16 Residual\nNull Deviance:      1200 \nResidual Deviance: 75.09    AIC: 113.5\n\n\n\n\nThe write-up:\nUse the code in the previous section as well as your theoretical understanding of the variables to communicate how you chose and fit your starting model. Include:\n\nwhat your error distribution assumption is and how you chose it\nwhat your shape assumption is and how you chose it\nwhat type of model you chose for your starting model (e.g. GLM)\n\nFor example,\n\nI built a model to test my hypothesis that varibility in adult moose growth rate (delMass, kg per year) is explained by latitude (Lat, ˚N), minimum winter temperature (WTemp, ˚C) and sex (Sex, male or female). My error distribution assumption was normal (gaussian) as delMass is continuous and could be positive or negative. My shape assumption regarding the relationship between delMass and each covariate was linear. This was chosen after inspecting plots of delMass and both WTemp and Lat (Figures 1 & 2). Plots were made using the ggplot2 package (Wickham 2016)32 In addition, a linear shape assumption was chosen to describe the relationship between Sex and delMass as Sex is a categorical variable.\nBased on these assumption I tested my hypothesis by fitting a generalized linear model with normal error distribution assumption to my data. All model fitting and analysis was done in R (R Core Team 2022).33"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#assessing-fit",
    "href": "DSPPH_SM_Reporting.html#assessing-fit",
    "title": "From statistical modelling to scientific report writing",
    "section": "Assessing fit",
    "text": "Assessing fit\n\nThe task:\nHere, you will describe how you assessed your model to ensure that it can be used to test your hypothesis. Remember that the assumptions you made above (error distribution & shape assumptions) were a “best guess” but it is only after the model is fit that we can confirm that it is useful. In this section you will:\n\nConsider covariate collinearity\nConsider observation independence\nConsider your error distribution assumption\nConsider your shape assumption\n\nand report your results.\n\n\nThe code:\nCode supporting your tasks in this section will help you assess your model for violations that may make it invalid for testing your hypothesis, e.g.:\n\n# Consider covariate collinearity \n## Fit a model without interactions\nstartMod.noInt &lt;- glm(formula = delMass ~ Lat + WTemp + Sex, # hypothesis\n              data = myDat, # data\n              family = gaussian(link = \"identity\")) # error distribution assumption\n\nstartMod.noInt\n\n\nCall:  glm(formula = delMass ~ Lat + WTemp + Sex, family = gaussian(link = \"identity\"), \n    data = myDat)\n\nCoefficients:\n(Intercept)          Lat        WTemp     SexMales  \n    -13.013        0.879       -0.105       12.000  \n\nDegrees of Freedom: 23 Total (i.e. Null);  20 Residual\nNull Deviance:      1200 \nResidual Deviance: 99.43    AIC: 112.2\n\n## Calculate Variance Inflation Factors (VIFs)\n#library(car) # load the car package\n#vif(startMod.noInt) # estimate VIFs\n\n## there are VIF values higher than the threshold value (in this case VIFs &gt; 3).  Let's remove WTemp and recalculate the VIFs:\n\nstartMod.noInt.noWTemp &lt;- glm(formula = delMass ~ Lat + Sex, # hypothesis\n              data = myDat, # data\n              family = gaussian(link = \"identity\")) # error distribution assumption\n\nstartMod.noInt.noWTemp\n\n\nCall:  glm(formula = delMass ~ Lat + Sex, family = gaussian(link = \"identity\"), \n    data = myDat)\n\nCoefficients:\n(Intercept)          Lat     SexMales  \n   -19.5975       0.9963      12.0000  \n\nDegrees of Freedom: 23 Total (i.e. Null);  21 Residual\nNull Deviance:      1200 \nResidual Deviance: 99.58    AIC: 110.3\n\n## Recalculating the VIFs\n\n#vif(startMod.noInt.noWTemp) # estimate VIFs\n\n## there are no longer any problems with covariate collinearity, as all VIFs &lt; 3\n\n## Refitting your starting model with the interactions back in:\nstartMod &lt;- glm(formula = delMass ~ Lat + Sex + Lat:Sex, # hypothesis\n              data = myDat, # data\n              family = gaussian(link = \"identity\")) # error distribution assumption\n\n\n\n# Consider observation independence\n## You wonder if Source could be violating your observation independence assumption.  Check to see with a plot of your residuals vs. Source\n\nlibrary(DHARMa) # load package\n\nsimulationOutput &lt;- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times and calculate residuals\n\nmyDat$Resid &lt;- simulationOutput$scaledResiduals # add residuals to data frame\n\nggplot()+ # start ggplot\n  geom_violin(data = myDat,\n             mapping = aes(x = Source, y = Resid))+ # add observations as a violin\n  geom_point(data = myDat,\n             mapping = aes(x = Source, y = Resid))+ # add observations as points\n  xlab(\"Source university\")+ # y-axis label\n  ylab(\"Scaled residual\")+ # x-axis label\n  labs(caption = \"Figure 3: A comparison of model residuals vs. university from which the data were sourced\")+ # figure caption\n  theme_bw()+ # change theme of plot\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n  ## The pattern is uniform.  You assume no problem with observation dependence.\n\n# Consider your error distribution assumption by inspecting residuals plotted vs. the fitted values\nplot(simulationOutput, asFactor=FALSE) # compare simulated data to our observations\n\n\n\n\n\n\n\n# Consider your shape assumption by inspecting residuals plotted vs. each covariate\nplot(simulationOutput, # compare simulated data to \n     form=myDat$Sex, # our observations\n     asFactor=TRUE) # whether the variable plotted is a factor\n\n\n\n\n\n\n\nplot(simulationOutput, # compare simulated data to \n     form=myDat$Lat, # our observations\n     asFactor=FALSE) # whether the variable plotted is a factor\n\n\n\n\n\n\n\n\n\n\nThe write-up:\nUse the code in the previous section to comment on your model’s validity in testing your research hypothesis. Include:\n\nhow you determined if there were problems with covariate collinearity and any actions you took if you detected a problem\nhow you determined if there were problems with observation dependence and any actions you took if you detected a problem\nhow you determined if your error distribution assumption was valid and any actions you took to address problems\nhow you determined if your shape assumption was valid and any actions you took to address problems\n\nFor example,\n\nI tested if collinearity among my covariates was making my model coefficients uncertain by estimating variance inflation factors (VIFs) with the car package (Fox & Weisberg 2019)34. Initial VIFs for Lat and WTemp were &gt; 23 indicating a high level of covariate collinearity. WTemp was removed from the model and the VIFs were re-estimated. VIFs in the new model were both 1, and it was concluded that there was no further issue with covariate collinearity. The new starting model fits the hypothesis:\ndelMass ~ Latitude + Sex + Latitude:Sex\nI tested my assumption of observation independence by determining if the observations were grouped by Source (the source university for the data35). I estimated my model residuals using the DHARMA package (Hartig 2022)36 and plotted my residuals vs. Source. I concluded my data were not dependent on one another (grouped by) Source as the residuals were evenly distributed across the three source universities (see figure 3).\nI assessed my error distribution assumption by inspected my residuals. Observed residuals were similar to those expected given my normal error distribution assumption. The Kolmogorov-Smirnov test comparing the observed to the expected distribution was not significant (P = 0.96). The dispersion and outlier tests were also not significant (P = 0.74 and P = 0.99 respectively). A plot of the residuals vs. fitted values showed a uniform cloud. From these results, I concluded that my error distribution assumption was appropriate.\nI assessed my shape assumption by inspected my residuals vs. each covariate. My residuals were evenly distributed with Latitude, indicating a linear shape assumption for Latitude was appropriate. The linear shape assumption for Sex was necessary as Sex is a categorical variable. A plot of the residuals vs. Sex showed residuals were evenly distributed across the two sexes. From these results, I concluded that my linear shape assumptions were appropriate.\nGiven these results, I determined that the new starting model (delMass ~ Latitude + Sex + Latitude:Sex) can be used to test my hypothesis."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#hypothesis-test",
    "href": "DSPPH_SM_Reporting.html#hypothesis-test",
    "title": "From statistical modelling to scientific report writing",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nThe task:\nHere, you will use your model to test your hypothesis. For the purposes of this course, you will do this with the model selection method we practiced in class37. In this section you will:\n\nfit and compare models representing all possible combinations of the covariates in your starting model\nuse the results to choose best-specified model(s)\nreport your results.\n\n\n\nThe code:\nCode supporting your tasks in this section will help you test and rank your models, e.g.:\n\nlibrary(MuMIn) # load package\n\noptions(na.action = \"na.fail\") # needed for dredge() function to prevent illegal model comparisons\n\n(dredgeOut&lt;-dredge(startMod, extra = \"R^2\")) # fit and compare a model set representing all possible predictor combinations\n\nGlobal model call: glm(formula = delMass ~ Lat + Sex + Lat:Sex, family = gaussian(link = \"identity\"), \n    data = myDat)\n---\nModel selection table \n    (Int)    Lat Sex Lat:Sex    R^2 df  logLik  AICc delta weight\n8  -1.596 0.7041   +       + 0.9340  5 -48.387 110.1  0.00  0.756\n4 -19.600 0.9963   +         0.9170  4 -51.130 112.4  2.26  0.244\n3  41.780          +         0.7200  3 -65.726 138.7 28.54  0.000\n2 -13.600 0.9963             0.1971  3 -78.366 163.9 53.82  0.000\n1  47.780                    0.0000  2 -80.999 166.6 56.46  0.000\nModels ranked by AICc(x) \n\n## Our best-specified model is delMass ~ Lat + Sex + Lat:Sex (model #8 in the dredge() output).\n\nbestMod&lt;-(eval(attr(dredgeOut, \"model.calls\")$`8`)) # extract model #8 from dredge table\n\n# You can make a pretty table to use in your write-up\nlibrary(gt) # load gt package\n\nmyTable &lt;- gt(dredgeOut) # make a pretty table\nmyTable &lt;- fmt_number(myTable, # to format the numbers in my table\n                    columns = everything(), # which columns to format\n                    decimals = 2) # round to 2 decimal places\nmyTable &lt;- tab_caption(myTable, \n                       caption = \"Table 1: Model selection table for hypothesis testing. Each row is a model fit to my data. Covariates included in each model are indicated by a number (for continuous covariates and intercept) or '+' (for categorical covariates). R^2 is the likelihood ratio R-squared, df indicates number of model parameters, logLik is the model Log-likelihood, AICc is the corrected Akaike Information Criteria, delta is the change in AICc between the model and that of the lowest AICc, and weight is the Akaike weights.\")\n\nmyTable\n\n\n\n\n\nTable 1: Model selection table for hypothesis testing. Each row is a model fit to my data. Covariates included in each model are indicated by a number (for continuous covariates and intercept) or '+' (for categorical covariates). R^2 is the likelihood ratio R-squared, df indicates number of model parameters, logLik is the model Log-likelihood, AICc is the corrected Akaike Information Criteria, delta is the change in AICc between the model and that of the lowest AICc, and weight is the Akaike weights.\n\n\n(Intercept)\nLat\nSex\nLat:Sex\nR^2\ndf\nlogLik\nAICc\ndelta\nweight\n\n\n\n\n−1.60\n0.70\n+\n+\n0.93\n5.00\n−48.39\n110.11\n0.00\n0.76\n\n\n−19.60\n1.00\n+\nNA\n0.92\n4.00\n−51.13\n112.36\n2.26\n0.24\n\n\n41.78\nNA\n+\nNA\n0.72\n3.00\n−65.73\n138.65\n28.54\n0.00\n\n\n−13.60\n1.00\nNA\nNA\n0.20\n3.00\n−78.37\n163.93\n53.82\n0.00\n\n\n47.78\nNA\nNA\nNA\n0.00\n2.00\n−81.00\n166.57\n56.46\n0.00\n\n\n\n\n\n\n## Could also be written as:\n # dredgeOut %&gt;%\n #   gt() %&gt;% # make a pretty table\n #   fmt_number(columns = everything(), # which columns to format\n #                     decimals = 2) %&gt;% # round to 2 decimal places\n #   tab_caption(caption = \"Table 1: Model selection table for hypothesis testing. Each row is a model fit to my data. Covariates included in each model are indicated by a number (for continuous covariates and intercept) or '+' (for categorical covariates). R^2 is the likelihood ratio R-squared, df indicates number of model parameters, logLik is the model Log-likelihood, AICc is the corrected Akaike Information Criteria, delta is the change in AICc between the model and that of the lowest AICc, and weight is the Akaike weights.\")\n\n\n\nThe write-up:\nUse the code in the previous section to explain how you chose your best-specified model. Include:\n\nthe method you are using to test your hypothesis\nhow you fit and rank your candidate model set\nhow you made your decision regarding your best-specified model\n\nFor example,\n\nI used model selection to test my hypothesis that varibility in adult moose growth rate is explained by latitude, sex and the interaction between the two. I used the dredge() function from the MuMIn package (Bartón 2022) to fit and rank models representing all possible covariate combinations. Models were ranked by corrected Akaike Information Criteria38 which [add a brief description of what AICc is].39\nThe model with the lowest AICc was chosen as the best-specified model, with models within ∆AICc = 2 of the lowest AICc model being considered equally best-specified.\nThe best-specified model was the full model:\ndelMass ~ Lat + Sex + Lat:Sex\nwith an Akaike weight (normalized relative likelihood) of 0.756. The next best model had an AIC of 2.26 more than the top model and an Akaike weight of 0.244."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#reporting",
    "href": "DSPPH_SM_Reporting.html#reporting",
    "title": "From statistical modelling to scientific report writing",
    "section": "Reporting",
    "text": "Reporting\n\nThe task:\nIn this section, you will report your hypothesis testing results. Specifically,\n\nReport your best-specified model(s)\nReport your modelled effects: This includes both reporting your coefficients and plotting the effects. I’ve moved visualization here to be more consistent with how we report biological results. A few things to note here:\n\nreporting the coefficients includes:\n\nreporting your coefficient estimates with uncertainty: make sure you report appropriate significant digits and units.\nreporting evidence that these coefficient estimates are different than zero: the fact that your covariate is in your best-specified model means that there is evidence of an effect of the covariate on the response (i.e. that the coefficient associated with the covariate is different than zero), BUT if you have a categorical variable with multiple levels, you need to test to see which coefficients for each level are different than zero.\nreporting evidence that these coefficient estimates are different than each other across the levels of your categorical covariate (if you have one): this is the multiple comparison testing we practiced in class. Remember that if you have both a continuous covariate and a categorical covariate, you need to test the coefficients (slopes) for the continuous covariate first and only if the slopes are similar across levels of the categorical covariate can you test if the intercepts are significantly different. This is due to the fact that we can get different intercept estimates just due to slopes not being equal.\nIn your visualization, remember to include your modelled fit, uncertainty and your observations in your plot)\n\n\nReport how well your model explains your response (explained deviance)\nReport the relative importance of each term in your model\n\n\n\nThe code:\nCode supporting your tasks in this section will help you communicate what your results tell you about your hypothesis, e.g.:\n\n# Reporting the modelled effects of your covariates on your response: \n\n## ** your coefficient estimates with uncertainty\n### Find the coefficients for Latitude.  As Sex is categorical, we do this for emmeans() and the reported coefficients are predicted mean response at each level of Sex. Note that emmeans() can report coefficients on the RESPONSE scale.\nlibrary(emmeans) # load emmeans package\nemmeans(object = bestMod, # your model\n        specs = ~ Lat + Sex + Lat:Sex, # your covariates\n       # at = list(Lat = 0), # to get coefficient predictions for Sex when Lat = 0\n        type = \"response\") # report coefficients on the response scale\n\n  Lat Sex     emmean    SE df lower.CL upper.CL\n 61.6 Females   41.8 0.575 20     40.6       43\n 61.6 Males     53.8 0.575 20     52.6       55\n\nConfidence level used: 0.95 \n\n### Find the coefficients for Latitude.  As Latitude is continuous, we do this with emtrends() and the reported coefficients are the slope describing the change in predicted mean response with a unit change in Latitude.  Note that emtrends() reports coefficients on the LINK scale.  You need to convert this to the response scale.  In our case, the error distribution assumption is normal so nothing needs to be done to convert the coefficients.\nemtrends(object = bestMod, # your model\n        specs = ~ Lat + Sex + Lat:Sex, # your covariates\n        var = \"Lat\") # name of your continuous cvoariate\n\n  Lat Sex     Lat.trend    SE df lower.CL upper.CL\n 61.6 Females     0.704 0.182 20    0.324     1.08\n 61.6 Males       1.289 0.182 20    0.908     1.67\n\nConfidence level used: 0.95 \n\n## ** evidence that these coefficient estimates are different than zero & ** evidence that these coefficient estimates are different than each other across the levels of your categorical covariate (if you have one)\n\n### Are the coefficients (slopes) of the latitude effect on growth different?\nforLatCompare &lt;- emtrends(object = bestMod, # your model\n                  specs = pairwise ~ Lat + Sex + Lat:Sex, # your covariates\n                  var = \"Lat\") # name of your continuous cvoariate\n\ntest(forLatCompare) # to get results of slope tests and comparisons\n\n$emtrends\n  Lat Sex     Lat.trend    SE df t.ratio p.value\n 61.6 Females     0.704 0.182 20   3.861  0.0010\n 61.6 Males       1.289 0.182 20   7.065  &lt;.0001\n\n\n$contrasts\n contrast                        estimate    SE df t.ratio p.value\n Lat61.6 Females - Lat61.6 Males   -0.584 0.258 20  -2.266  0.0347\n\nplot(forLatCompare, # plot coefficients with error\n     comparison = TRUE, # include arrows indicating thresholds for differences\n     type = \"response\") # plot on the response scale\n\n\n\n\n\n\n\n### Are the coefficients (intercepts) of Sex factor levels different than zero?\nforSexCompare &lt;- emmeans(object = bestMod, # your model\n                         specs = pairwise ~ Lat + Sex + Lat:Sex, # your covariates\n                         type = \"response\") # name of your continuous cvoariate\n\n\ntest(forSexCompare)\n\n$emmeans\n  Lat Sex     emmean    SE df t.ratio p.value\n 61.6 Females   41.8 0.575 20  72.704  &lt;.0001\n 61.6 Males     53.8 0.575 20  93.588  &lt;.0001\n\n\n$contrasts\n contrast                        estimate    SE df t.ratio p.value\n Lat61.6 Females - Lat61.6 Males      -12 0.813 20 -14.768  &lt;.0001\n\n## Plot your modelled effects:\n# Set up your covariates for the visualized fit\nforLat&lt;-seq(from = min(myDat$Lat), \n             to = max(myDat$Lat), \n             by = 1) # get a range of latitudes for making predictions\nforSex&lt;-unique(myDat$Sex) # get every level of my Sex covariate\nforVis&lt;-expand.grid(Lat=forLat, Sex=forSex) # create a data frame with all combinations of covariates\n\n# Get your model fit estimates at each value of your covariates\nmodFit&lt;-predict(bestMod, # the model\n                newdata = forVis, # the covariate values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\nggplot()+\n  geom_point(data = myDat, # data\n             mapping = aes(x = Lat, y = delMass, col = Sex))+ # add data to your plot\n  geom_ribbon(data = forVis, \n              mapping = aes(x = Lat, ymin = Lower, ymax = Upper, fill = Sex), alpha = 0.5)+ # add the uncertainty to your plot\n  geom_line(data = forVis, \n            mapping = aes(x = Lat, y = Fit, col = Sex))+ # add the model fit to your plot\n  ylab(\"Change in body mass (kg per year)\")+ # change y-axis label\n  xlab(\"Latitude (˚N)\")+ # change x-axis label\n  labs(caption = \"Figure 4: Modelled effects (lines, mean +/- standard error) of latitude and sex (colours) on growth rate \\nalong with observations (points)\")+\n  theme_bw()+ # change ggplot theme\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n# * Report how well your model explains your response (explained deviance)\n\nr.squaredLR(bestMod) # estimates the likelihood ratio R^2.  Could also estimate a traditional R^2 with 1-summary(bestMod)$deviance/summary(bestMod)$null.deviance here as the error distribution assumption is normal and the shape assumption is linear, but the likelihood ratio R^2 function is generally applicable to many error distribution assumptions and equivalent to the traditional R^2 when the error distribution assumption is normal and the shape assumption is linear.\n\n[1] 0.9339728\nattr(,\"adj.r.squared\")\n[1] 0.9350677\n\n# * report how important each model term (covariate fixed effects and any interactions) is in explaining the deviation in your response. \nsw(dredgeOut)\n\n                     Sex  Lat  Lat:Sex\nSum of weights:      1.00 1.00 0.76   \nN containing models:    3    3    1   \n\n\n\n\nThe write-up:\nUse the results of the code above to report your hypothesis testing results. Include:\n\na description of the terms (covariate fixed effects and any interactions) that are in your best-specified model\ncompare these terms to your starting model - are all terms in your starting model in you best-specified model?\nreport your modelled effects by reporting your coefficients. As we will discuss this week in class, report:\n\nyour coefficient estimates with uncertainty\nevidence that these coefficient estimates are different than zero\nevidence that these coefficient estimates are different than each other across the levels of your categorical covariate (if you have one)\n\nreport your modelled effects by plotting. Remember to include your model predictions, uncertainty as well as your observations. Include units on your axes and a figure number and legend.\nreport your estimate of the % of deviance in your response that your model explains\nconsider what might be explaining the remaining (unexplained) deviation in your response\nreport how important each model term (covariate fixed effects and any interactions) is in explaining the deviation in your response.\n\nFor example,\n\nMy best-specified model tells me that the growth varies with latitude and sex, and that the effect of latitude on growth varies with sex. The terms in my best-specified model are the same as those in my starting model indicating that there is evidence that the main effects of latitude and sex as well as the interaction are explaining variability in moose growth.\nGrowth rate is higher for males than females: the predicted growth when latitude is 61.6˚N (the mean of the latitudinal range) is 41.8 ± 0.6 kg year-1 for females and 53.8 ± 0.6 kg year-1 for males.40 Both these coefficients are significantly different than zero (t-test, P &lt; 0.0001 for males and females)^[note that we don’t need to add an adjustment for multiple comparisons here as there are only two levels in our categorical covariate).\nThe coefficient (slope) for the effect of a change of latitude on the growth rate is 0.70 ± 0.18 kg year-1 ˚N-1 for females and 1.29 ± 0.18 kg year-1 ˚N-1 for males. These slopes are significantly different from zero (t-test, P = 0.001 for females and P &lt; 0.0001 for males) and significantly different from one another (t-test, P = 0.035). These coefficients show that growth rates increase with latitude for both sexes, but that the effect of latitude on growth is higher for male vs. female moose.\nFigure 4 shows the modelled effects of latitude and sex on growth rate along with my observations.\nTogether the effects of latitude and sex explain 93% of the deviance in growth rate (Likelihood ratio R2).41\nThe remaining unexplained deviance may be due to other factors affecting growth rate such as winter harshness, food availability, etc. Note that I was not able to include minimum winter temperature in my hypothesis test due to high collinearity between latitude and minimum winter temperature. Therefore, I do not know if the measured latitudinal effect on growth might mechanistically be due to winter harshness (here estimated as minimum winter temperature). This could be tested if I was able to expand my data set to include sites where latitude and minimum winter temperature were less correlated.\nBased on the sum of Akaike weights, latitude and sex are equally important in explaining the deviance in growth rate as they appear in all models with Akaike weights &gt; 0. The interaction between latitude and sex is slightly less important appearing only in the best-specified model with an Akaike weight of 0.76."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#predicting",
    "href": "DSPPH_SM_Reporting.html#predicting",
    "title": "From statistical modelling to scientific report writing",
    "section": "Predicting",
    "text": "Predicting\n\nThe task:\nHere you may use your model to make predictions (remembering prediction limits). For the assignments and exam, I’ll explicitly ask you to make a prediction if we want to see one.\nIf you want to make a prediction (e.g. based on your best-specified model, what is your predicted mean annual growth of a female moose living in Sicily, Italy?), in this section you want to report your prediction results and give any limitations to the prediction.\n\n\nThe code:\nCode supporting your tasks in this section will help you communicate what prediction you made and the results, e.g.:\n\n# Based on your best-specified model, what is your predicted mean annual growth of an adult female moose living in Sicily, Italy?\n\n## Looking on the internet, we see that Sicily, IT is at 37.6˚N.\n\npredict(bestMod, # our model\n        newdata = data.frame(Sex = \"Females\", Lat = 37.6), # values of the covariates at which to make the prediction\n        se.fit=TRUE, # include an estimate of error around the prediction\n        type=\"response\") # make sure the prediction is on the response scale\n\n$fit\n       1 \n24.87708 \n\n$se.fit\n[1] 4.414466\n\n$residual.scale\n[1] 1.990439\n\n\n\n\nThe write-up:\nUse the code in the previous section to present the results of your prediction. Include\n\nthe prediction and an estimate of uncertainty\nany perceived prediction limits.\n\nFor example,\n\nI used my best-specified model to estimate the mean annual growth rate of adult female moose living in Sicily, Italy (~ 37.6˚N) as 24.9 +/- 4.4 kg per year (mean +/- standard error). While this is an estimate consistent with my model, it likely is unrealistic as the distribution of moose does not currently extend as far south as Sicily. It is likely there are limitations to their dispersal to or survival in this area and my model may not be valid for latitudes so far south.\n\n- Reporting your best-specified model   - Reporting your modelled patterns      - Plotting model fit, uncertainty and observations [[Visualizing]]      - Reporting effect sizes        - Report the coefficients - [[what are model coefficients]]         - What are the coefficient estimates (with uncertainty)?            - Is each coefficient significantly different from zero?            - Are the coefficients significantly different than one another?    - Reporting how well your model explains your response (explained variation)    - Report the relative importance of each term in your model\n\ncross validation\n\nvisreg vs. ggeffects\n\nthree ways to report what your model is saying about your hypothesis\n- visualize\n- report coefficients\n- give an example (response predicted under conditions)\n\n\n- model coefficients are model effects\n    - the effect on Resp with a change in Cov\n- Continuous\n    - coefficient is the change in Resp with a unit change in Cov\n    - what does it mean if coef is positive\n    - what does it mean if coef is negative\n    - what is error on the coef?\n    - what does it mean when a coef is no different than 0? - why that is the science\n    - how to test if two coefficients are similar\n- Categorical\n    - coefficient is the change in Resp with a change from one level of Cat to another\n    - what does it mean if coef is positive\n    - what does it mean if coef is negative\n    - what is error on the coef?\n    - what does it mean when a coef is no different than 0? - why that is the science\n    - how to test if two coefficients are similar\n- Show linear model equation and how it looks for Cat, Cont with coefficients\n- Describe generically and then define for each link function\n\nreport effect size and precision, not p-value \n\n\nEmphasise effect sizes to replace statistical significance with ecological relevance\nresearch question should follow the FINER criteria\nrefinements of your hypothesis after you collect the data will result in a hypothesis that reflects the sample, not the population\nexploratory or hypothesis generating analysis\nuse representative (unbiased) sampling\nto report results of hypothesis testing, report effect sizes, confidence intervals, interpretation of effect and confidence intervals, and can include p-values\np-values are the probability that we would obtain a result equal to or more extreme than that observed given a null hypothesis that is true\np-values tell us more about how compatible the data are with the model vs. evidence of underlying effects"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#visualizing-with-the-visreg-package",
    "href": "DSPPH_SM_Reporting.html#visualizing-with-the-visreg-package",
    "title": "From statistical modelling to scientific report writing",
    "section": "Visualizing with the visreg package",
    "text": "Visualizing with the visreg package\nThe first method uses the visreg package and the visreg() function which we’ve practiced before. Using your example bestMod, here are a few visualizations using the visreg package:\nVisualizing with a continuous covariate on the x-axis: Notice how you can include the gg = TRUE argument to plot this as a ggplot type figure. This allows you to add your data onto the visualization of your model.\n\nlibrary(visreg) # load visreg package\nlibrary(ggplot2) # load ggplot2\n\nvisreg(bestMod, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cont\", # covariate on x-axis\n       by = \"Cat\", # covariate plotted as colour\n       #breaks = 3, # if you want to control how the colour covariate is plotted\n       #cond = , # if you want to include a 4th covariate\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  geom_point(data = myDat, # data\n             mapping = aes(x = Cont, y = Resp, col = Cat))+ # add data to your plot\n  ylim(0, 60)+ # adjust the y-axis units\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cont, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nVisualizing with a categorical covariate on the x-axis: You can also plot the same model with the categorical covariate on the x-axis. Notice how the continuous covariate is represented by different levels in the colours on the plot. Here you’ve asked for three levels with breaks = 3.\n\nvisreg(bestMod, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cat\", # covariate on x-axis\n       by = \"Cont\", # covariate plotted as colour\n       breaks = 3, # if you want to control how the colour covariate is plotted\n       #cond = , # if you want to include a 4th covariate\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nYou can also specify at which levels the breaks should occur with the breaks = ... argument. For example, you can ask visreg to plot the modelled effects when Cont = 40 and Cont = 50:\n\nvisreg(bestMod, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"Cat\", # covariate on x-axis\n       by = \"Cont\", # covariate plotted as colour\n       breaks = c(40,50), # if you want to control how the colour covariate is plotted\n       #cond = , # if you want to include a 4th covariate\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nUnfortunately due to limitations of the visreg package, you can not easily add our data onto plots where the x-axis is a categorical covariate. But that’s ok, because there are other options…"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#visualizing-by-hand",
    "href": "DSPPH_SM_Reporting.html#visualizing-by-hand",
    "title": "From statistical modelling to scientific report writing",
    "section": "Visualizing “by hand”:",
    "text": "Visualizing “by hand”:\nWe will also practice how to visualize your model “by hand”. Here, “by hand” is a bit of a silly description as R will be doing the work for you. What I mean by “by hand” is that you will be building the plot yourselves by querying your model object.\nVisualizing with a continuous covariate on the x-axis:. To plot by hand, you will first make a data frame containing the value of your covariates at which you want to plot effects on the response:\n\n# Set up your covariates for the visualized fit\nforCont&lt;-seq(from = min(myDat$Cont), \n             to = max(myDat$Cont), \n             by = 0.1) # e.g. a sequence of Cont values\nforCat&lt;-unique(myDat$Cat) # every value of your categorical covariate\n\n# create a data frame with your covariates\nforVis&lt;-expand.grid(Cont=forCont, Cat=forCat) # expand.grid() function makes sure you have all combinations of covariates\n\nNext, you will use the predict() function44 to get the model estimates of your response variable at those values of your covariates:\n\n# Get your model fit estimates at each value of your covariates\nmodFit&lt;-predict(bestMod, # the model\n                newdata = forVis, # the covariate values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\nFinally, you will use these model estimates to make your plot:\n\nggplot()+\n  geom_point(data = myDat, # data\n             mapping = aes(x = Cont, y = Resp, col = Cat))+ # add data to your plot\n  geom_ribbon(data = forVis, \n              mapping = aes(x = Cont, ymin = Lower, ymax = Upper, fill = Cat), alpha = 0.5)+ # add the uncertainty to your plot\n  geom_line(data = forVis, \n              mapping = aes(x = Cont, y = Fit, col = Cat))+ # add the model fit to your plot\n  ylim(0, 60)+ # adjust the y-axis units\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cont, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nVisualizing with a categorical covariate on the x-axis:. The procedure for visualizing when you have a categorical covariate on the x-axis is similar: 1) choose the values of your covariates at which to make a prediction, 2) use predict() to use your model to estimate your response variable at those values of your covariate, and 3) use the model estimates to plot your model fit:\n\n# Set up your covariates for the visualized fit\nforCont&lt;-c(40, 50 ,60) # e.g. Cont values\nforCat&lt;-unique(myDat$Cat) # every value of your categorical covariate\n\n# create a data frame with your covariates\nforVis&lt;-expand.grid(Cont=forCont, Cat=forCat) # expand.grid() function makes sure you have all combinations of covariates\n\n# Get your model fit estimates at each value of your covariates\nmodFit&lt;-predict(bestMod, # the model\n                newdata = forVis, # the covariate values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n\nggplot()+\n  geom_jitter(data = myDat, # data\n             mapping = aes(x = Cat, y = Resp))+ # add data to your plot\n  geom_errorbar(data = forVis, \n              mapping = aes(x = Cat, y = Fit, col = Cont, ymin = Lower, ymax = Upper))+ # add the uncertainty to your plot\n  ylim(0, 60)+ # adjust the y-axis units\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cat, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#plotting-in-3d",
    "href": "DSPPH_SM_Reporting.html#plotting-in-3d",
    "title": "From statistical modelling to scientific report writing",
    "section": "Plotting in 3D",
    "text": "Plotting in 3D\nYou might notice the plots above are communicating 3-dimensions (one response + two covariates) in a 2-dimensional plot. There are other ways of making 3-dimensional plots in R, e.g. with the visreg package using the visreg2d() function in the visreg package:\n\nvisreg2d(fit = bestMod, # your model\n         xvar = \"Cont\", # what to plot on the x-axis\n         yvar = \"Cat\", # what to plot on the y-axis\n         scale = \"response\") # make sure fitted values (colours) are on the scale of the response\n\n\n\n\n\n\n\n\nor “by hand” using the geom_raster() function in the ggplot2 package:\n\n# Set up your covariates for the visualized fit\nforCont&lt;-seq(from = min(myDat$Cont), \n             to = max(myDat$Cont), \n             by = 0.1) # e.g. a sequence of Cont values\nforCat&lt;-unique(myDat$Cat) # every value of your categorical covariate\n\n# create a data frame with your covariates\nforVis&lt;-expand.grid(Cont=forCont, Cat=forCat) # expand.grid() function makes sure you have all combinations of covariates\n\n\n# Get your model fit estimates at each value of your covariates\nmodFit&lt;-predict(bestMod, # the model\n                newdata = forVis, # the covariate values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\n# create your plot\nggplot() + # start your ggplot\n  geom_raster(data = forVis, aes(x = Cont, y = Cat, fill = Fit))+ # add the 3 dimensions as a raster\n  geom_point(data = myDat, aes(x = Cont, y = Cat, colour = Resp)) # add your data\n\n\n\n\n\n\n\n\nEXTRA: As you can see, these plots represent the 3rd dimension by using colour. We can also make actual 3 dimensional plots in R with the plotly package45. These plots are interactive which makes them more useful than static 3d plots. Use your mouse to move the plot around!\n\nlibrary(plotly) # load the plotly package\n\nplot_ly(data = forVis, # the data with your model predictions (made above)\n        x = ~Cont, # what to plot on the x axis\n        y = ~Cat, # what to plot on the y axis\n        z = ~Fit, # what to plot on the z axis\n        type=\"scatter3d\", # type of plot\n        mode=\"markers\") %&gt;% # type of plot\n  add_markers(data = myDat, x = ~Cont, y = ~Cat, z = ~Resp) # add your data"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#reporting-your-best-specified-models-1",
    "href": "DSPPH_SM_Reporting.html#reporting-your-best-specified-models-1",
    "title": "From statistical modelling to scientific report writing",
    "section": "Reporting your best-specified model(s)",
    "text": "Reporting your best-specified model(s)\nReporting your best-specified model means reporting the terms - the covariates and any interactions - that are in your model. It’s good practice to present the model along with the results from the model selection. In this way, you can include multiple best-specified models if there is evidence that more than one might be useful.\nFor example, with your results from model selection being:\n\n\nGlobal model call: glm(formula = Resp ~ Cont + Cat + Cont:Cat, family = Gamma(link = \"inverse\"), \n    data = myDat)\n---\nModel selection table \n    (Int) Cat        Cnt Cat:Cnt    R^2 df   logLik  AICc delta weight\n8 0.07407   + -0.0003389       + 0.8703  5 -123.363 258.1  0.00  0.815\n4 0.08742   + -0.0006263         0.8554  4 -126.083 261.1  2.97  0.185\n2 0.05874   +                    0.6822  3 -145.763 298.0 39.96  0.000\n3 0.08423     -0.0008670         0.3726  3 -162.769 332.1 73.97  0.000\n1 0.04133                        0.0000  2 -174.425 353.1 95.02  0.000\nModels ranked by AICc(x) \n\n\nYou can present the top two models as:\n\nDepending on your hypothesis and results, you may want to present all models within ∆AICc &lt; 2 of the best-specified model, or all models with any Akaike weight (as above), or simply all models. We’ll discuss ways of making these choices in class."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#reporting-the-modelled-effects-of-your-covariates-on-your-response.",
    "href": "DSPPH_SM_Reporting.html#reporting-the-modelled-effects-of-your-covariates-on-your-response.",
    "title": "From statistical modelling to scientific report writing",
    "section": "Reporting the modelled effects of your covariates on your response.",
    "text": "Reporting the modelled effects of your covariates on your response.\nYou want to report how the covariates are affecting your response. e.g. does a change in the covariate lead to an increase in the response? or a decrease? or is the effect non-linear (for continuous covariates)?\nThe easiest way to do this is by visualization, as you saw above, but you also want to report the coefficients with their uncertainty:\n\nsummary(bestMod)$coefficients\n\n                       Estimate   Std. Error    t value     Pr(&gt;|t|)\n(Intercept)        0.0740662767 0.0070213172 10.5487724 7.241364e-14\nCatTreatment      -0.0025850597 0.0087148893 -0.2966257 7.680875e-01\nCont              -0.0003388666 0.0001490856 -2.2729675 2.774499e-02\nCatTreatment:Cont -0.0004082566 0.0001772830 -2.3028518 2.586062e-02\n\n\nNote that for categorical covariates with more than two levels (e.g. an example with a covariate consisting of Species A, B and C), you’ll want to take an extra step to determine if the effect of each level of the covariate is similar (e.g. is the effect of Species A on the response the same as the effect on Species B?). We’ll cover how to do this in Week 12."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#reporting-how-well-your-model-explains-your-response-1",
    "href": "DSPPH_SM_Reporting.html#reporting-how-well-your-model-explains-your-response-1",
    "title": "From statistical modelling to scientific report writing",
    "section": "Reporting how well your model explains your response",
    "text": "Reporting how well your model explains your response\nIf you will recall, your whole motivation for pursuing statistical modelling was to explain variation in your response. Thus, it is important that you quantify how much variation in your response you are able to explain by your model.\nNote that we will discuss this as “explained deviance” rather than “explained variation”. This is because the term “variance” is associated with models where the error distribution assumption is normal, whereas deviance is a more universal term.\nWhen you have a normal error distribution assumption and linear shape assumption, you can capture the amount of explained deviance simply by comparing the variance in your response (null variation) vs. the variance in your model residuals (residual variation) as the \\(R^2\\):\n\\(R^2 = 1 - \\frac{residual variation}{null variation}\\)\nFrom this equation, you can see how, if your model is able to explain all the variation in the response, the residual variation will be zero, and \\(R^2 = 1\\). Alternatively, if the model explains no variation in the response the residual variation equals the null variation and \\(R^2 = 0\\).\nFor models with other error distribution and shape assumptions, you need another way of estimating the goodness of fit of your model. You can do this through a pseudo \\(R^2\\).\nOne useful pseudo \\(R^2\\) is called the Likelihood Ratio \\(R^2\\) or \\(R^2_{LR}\\). The \\(R^2_{LR}\\) compares the likelihood of your best-specified model to the likelihood of the null model:\n\\(R^2_{LR} = 1-exp(-\\frac{2}{n}(log𝓛(model)-log𝓛(null)))\\)\nwhere \\(n\\) is the number of observations, \\(log𝓛(model)\\) is the log-likelihood of your model, and \\(log𝓛(null)\\) is the log-likelihood of the null model. The \\(R^2_{LR}\\) is the type of pseudo \\(R^2\\) that shows up in your dredge() output when you add extra = \"R^2\" to the dredge() call. You can calculate \\(R^2_{LR}\\) by hand, read it from our dredge() output, or estimate it using r.squaredLR() from the MuMIn package:\n\nr.squaredLR(bestMod)\n\n[1] 0.8702944\nattr(,\"adj.r.squared\")\n[1] 0.8711072\n\n\nNote here that two values of \\(R^2_{LR}\\) are reported. The adjusted pseudo \\(R^2\\) given here under attr(,\"adj.r.squared\") has been scaled so that \\(R^2_{LR}\\) can reach a maximum of 1, to be equivalent to a regular \\(R^2\\).\nOne nice feature of the \\(R^2_{LR}\\) is that it is equivalent to the regular \\(R^2\\) when our model assumes a normal error distribution assumption and linear shape assumption, so you can use \\(R^2_{LR}\\) for any of the models we’re discussing in class.\n\ncheck RR2 package"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#reporting-the-importance-of-each-covariate-in-explaining-your-response.",
    "href": "DSPPH_SM_Reporting.html#reporting-the-importance-of-each-covariate-in-explaining-your-response.",
    "title": "From statistical modelling to scientific report writing",
    "section": "Reporting the importance of each covariate in explaining your response.",
    "text": "Reporting the importance of each covariate in explaining your response.\nFinally, you want to report how relatively important each term is in your model in regards to explaining deviance in your response. This is also called “partitioning the explained deviance” to the covariates.\nTo get an estimate of how much deviance in your response one particular covariate explains, you may be tempted to compate the explained deviance (\\(R^2\\)) estimates of models fit to data with and without that particular covariate. Let’s try an example using your example statistical model:\n\ndredgeOut\n\nGlobal model call: glm(formula = Resp ~ Cont + Cat + Cont:Cat, family = Gamma(link = \"inverse\"), \n    data = myDat)\n---\nModel selection table \n    (Int) Cat        Cnt Cat:Cnt    R^2 df   logLik  AICc delta weight\n8 0.07407   + -0.0003389       + 0.8703  5 -123.363 258.1  0.00  0.815\n4 0.08742   + -0.0006263         0.8554  4 -126.083 261.1  2.97  0.185\n2 0.05874   +                    0.6822  3 -145.763 298.0 39.96  0.000\n3 0.08423     -0.0008670         0.3726  3 -162.769 332.1 73.97  0.000\n1 0.04133                        0.0000  2 -174.425 353.1 95.02  0.000\nModels ranked by AICc(x) \n\n\nIf you want to get an estimate as to how much response deviance the Cont covariate explains, you might compare the pseudo \\(R^2\\) of a model with and without the Cont covariate, e.g. comparing model #4 (that includes Cont) and model #2 (that doesn’t include Cont):\n\n# deviance explained by Cont:\nR2.mod4 &lt;- (dredgeOut$`R^2`[2]) # model #4 is the second row in dredgeOut\nR2.mod2 &lt;- (dredgeOut$`R^2`[3]) # model #2 is the third row in dredgeOut\n\nR2.mod4 - R2.mod2 # find estimated contribution of Cont to explained deviance\n\n[1] 0.1731384\n\n\nBut what if you chose to compare model #3 (that includes Cont) and model #1 (that doesn’t include Cont):\n\n# deviance explained by Cont:\nR2.mod3 &lt;- (dredgeOut$`R^2`[4]) # model #3 is the fourth row in dredgeOut\nR2.mod1 &lt;- (dredgeOut$`R^2`[5]) # model #1 is the fifth row in dredgeOut\n\nR2.mod3 - R2.mod1 # find estimated contribution of Cont to explained deviance\n\n[1] 0.372649\n\n\nQuite a different answer! Your estimates of the contribution of Cont to explaining the response deviation don’t agree because of collinearity among our covariates46. We’ll discuss this more in class.\nOne option for partitioning the explained deviance when you have collinearity among your covariates is hierarchical partitioning. Hierarchical partitioning estimates the average independent contribution of each covariate to the total explained variance by considering all models in the candidate model set47. This method is beyond the scope of the course but see the rdacca.hp package for an example of how to do this.\nAnother method for estimating the importance of each term (covariate or interaction) in your model is by again looking at the candidate model set ranking made by dredge(). Here you can measure the importance of a covariate by summing up the Akaike weights for any model that includes a particular covariate. The Akaike weight of a model compares the likelihood of the model scaled to the total likelihood of all the models in the candidate model set. The sum of Akaike weights for models including each covariate tells us how important the covariate is in explaining the deviance in your response. You can calculate the sum of Akaike weights with the sw() function in the MuMIn package:\n\nsw(dredgeOut)\n\n                     Cat  Cont Cat:Cont\nSum of weights:      1.00 1.00 0.82    \nN containing models:    3    3    1    \n\n\nHere we can see that Cat and Cont are equally important in explaining the deviance in Resp (they appear in all models that have any Akaike weight), while the interaction term between Cat and Cont is less important (only appearing in one model with Akaike weight, though this is the top model).\nCheck relaimpo"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#first-consider-the-limits-to-making-predictions",
    "href": "DSPPH_SM_Reporting.html#first-consider-the-limits-to-making-predictions",
    "title": "From statistical modelling to scientific report writing",
    "section": "First, consider the limits to making predictions",
    "text": "First, consider the limits to making predictions\nThe first step when considering using your model to make predictions is to decide if this is appropriate. By using your model to make a prediction, you are assuming that the same processes that govern the relationship between your covariate(s) and response will hold in the new place, time or environmental condition. For example, you might have used a linear shape assumption that proved appropriate for modelling how growth varies due to temperature, but now you want to estimate growth at an even higher temperature than you tested. Is this appropriate? Would it be possible that growth starts to respond non-linearly with temperature?"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#how-to-use-your-model-to-make-predictions",
    "href": "DSPPH_SM_Reporting.html#how-to-use-your-model-to-make-predictions",
    "title": "From statistical modelling to scientific report writing",
    "section": "How to use your model to make predictions",
    "text": "How to use your model to make predictions\nOnce you are convinced that making a prediction with your model is useful, you can use the predict() function to predict the value of your response by handing it values of each covariate at which you would like a response prediction. For example:\n\npredict(object = bestMod, # your model\n        newdata = data.frame(Cont = 57, Cat = \"Treatment\"), # the values of the covariates at which to make the prediction\n        type = \"response\", # to make the prediction on the response scale.\n        se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\n$fit\n       1 \n34.60783 \n\n$se.fit\n       1 \n1.060142 \n\n$residual.scale\n[1] 0.129131\n\n\nSo with Cont = 57 and Cat = Treatment, we would expect Resp to be 34.6 ± 1.1. This can be shown on our visualization here (prediction in black):"
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#example-1-resp1-cat1",
    "href": "DSPPH_SM_Reporting.html#example-1-resp1-cat1",
    "title": "From statistical modelling to scientific report writing",
    "section": "Example 1: Resp1 ~ Cat1",
    "text": "Example 1: Resp1 ~ Cat1\nFor example #1, assume your best-specified model shows that your response variable (Resp1) is explained by a categorical covariate (Cat1):\nResp1 ~ Cat1\nYour best-specified model for example #1 is in an object called bestMod1:\n\nbestMod1\n\n\nCall:  glm(formula = Resp1 ~ Cat1 + 1, family = gaussian(link = \"identity\"), \n    data = myDat1)\n\nCoefficients:\n(Intercept)      Cat1Sp2      Cat1Sp3  \n     22.500        4.194       -5.889  \n\nDegrees of Freedom: 99 Total (i.e. Null);  97 Residual\nNull Deviance:      8882 \nResidual Deviance: 7037     AIC: 717.2\n\n\nthat was fit to data in myDat1:\n\nsummary(myDat1)\n\n  Cat1        Resp1      \n Sp1:28   Min.   : 1.00  \n Sp2:36   1st Qu.:15.00  \n Sp3:36   Median :22.00  \n          Mean   :21.89  \n          3rd Qu.:30.00  \n          Max.   :40.00  \n\n\nand the dredge() table you used to pick your bestMod1 is in dredgeOut1\n\ndredgeOut1\n\nGlobal model call: glm(formula = Resp1 ~ Cat1, family = gaussian(link = \"identity\"), \n    data = myDat1)\n---\nModel selection table \n  (Intrc) Cat1    R^2 df   logLik  AICc delta weight\n2   22.50    + 0.2077  4 -354.584 717.6  0.00      1\n1   21.89      0.0000  2 -366.223 736.6 18.98      0\nModels ranked by AICc(x) \n\n\n\nReporting your statistical modelling results - Example 1\nRecall that reporting your statistical modelling results means:\n\nReporting your best-specified model(s)\nReporting your modelled effects\nReporting how well your model explains your response\nReporting how important each of your covariates is in explaining your response\n\n\nReporting best-specified model(s) - Example 1\nReporting your best-specified model means reporting the terms - the covariates and any interactions - that are in your model.\nFor Example 1, you will report that your best-specified model is Resp1 ~ Cat1, i.e. that there is evidence that Cat1 explains variability in Resp1. Remember from last week that you can also use the output from dredgeOut1 to report evidence for how you picked the best-specified model, e.g. if you want to report more than one model.\n\n\nGlobal model call: glm(formula = Resp1 ~ Cat1, family = gaussian(link = \"identity\"), \n    data = myDat1)\n---\nModel selection table \n  (Intrc) Cat1    R^2 df   logLik  AICc delta weight\n2   22.50    + 0.2077  4 -354.584 717.6  0.00      1\n1   21.89      0.0000  2 -366.223 736.6 18.98      0\nModels ranked by AICc(x) \n\n\n\n\nReporting your modelled effects - Example 1\n\nReporting the coefficients\nIf you remember from last week, you reported your modelled effects by reporting your model coefficients as your coefficients tell you about the effect your covariate has on your response.\nWith a continuous covariate, the coefficient is the slope of the line (on the linked scale) showing the amount of change in the response that is caused by a change in your continuous covariate.\nWith a categorical covariate, the coefficient is an intercept, or the model prediction of the response at that level of the categorical covariate (e.g. the modelled Resp1 when Cat1 is Sp1, Sp2 and Sp3). For categorical covariates, there is a coefficient estimated for each level of the covariate that can be thought of as an intercept. So as Cat1 in Example 1 has three levels (Sp1, Sp2 and Sp3), there will be three coefficients estimated (one for each).\nHere we will focus on three things:\n\nWhat are your coefficient estimates (with uncertainty)?\nIs each coefficient estimate different from zero?\nAre the coefficient estimates different than one another across categorical levels?\n\n\nWhat are your coefficient estimates (with uncertainty)?\nLast week, you found your coefficients in the “Estimate” column of the summary() output of your model:\n\ncoef(summary(bestMod1)) # extract the coefficients from summary()\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 22.500000   1.609663 13.978083 5.653157e-25\nCat1Sp2      4.194444   2.146217  1.954343 5.354014e-02\nCat1Sp3     -5.888889   2.146217 -2.743846 7.234664e-03\n\n\nInterpreting the coefficients from this output takes practice - especially for categorical covariates because of the way that R treats categorical covariates in regression. With R’s “dummy coding”, one level of the covariate (here Sp1) is incorporated into the intercept estimate 22.5 as the reference level. The other coefficients in the Estimate column represent the change in modelled response when you move from the reference level (Sp1) to another level.\nThe modelled response when Cat1 = Sp1 is 22.5 units.\nThe modelled response when Cat1 = Sp2 is 4.2 units higher than this reference level = 22.5 + 4.2 = 26.7 units.\nThe modelled Resp1 when Cat1 = Sp3 is -5.9 units lower than the reference level = 22.5 + -5.9 = 16.6 units.48\nSo all the information we need is in this summary() output, but not easy to see immediately. An easier way is to use the emmeans package which helps us by reporting the coefficients directly. “emmeans” is the estimated marginal means. The estimated marginal mean is the model prediction for the response when the covariates are held at particular values. If the value of the covariate is not defined, the response prediction is the average response prediction over all values in that covariate.\nIn our case, we use the emmeans package to get the mean value of the response at each level of the covariate. For categorical covariates, we do this with the emmeans() function:\n\nlibrary(emmeans) # load the emmeans package\n\nemmeans(object = bestMod1, # your model\n        specs = ~ Cat1, # your covariates\n        type = \"response\") # report coefficients on the response scale\n\n Cat1 emmean   SE df lower.CL upper.CL\n Sp1    22.5 1.61 97     19.3     25.7\n Sp2    26.7 1.42 97     23.9     29.5\n Sp3    16.6 1.42 97     13.8     19.4\n\nConfidence level used: 0.95 \n\n\nSo now we have a modelled value of our response for each level of our categorical covariate.\nWhen Cat1 is Sp1, Resp1 is estimated to be 22.5 ± 1.6 units.\nWhen Cat1 is Sp2, Resp1 is estimated to be 26.7 ± 1.4 units.\nWhen Cat1 is Sp3, Resp1 is estimated to be 16.6 ± 1.4 units.\nNote that this is the same information in the summary() output just easier to read.49\nNote also that two types of uncertainty are measured here. SE stands for the standard error around the prediction, and is a measure of uncertainty of the average modelled effect. The lower.CL and upper.CL represent the 95% confidence limits of the prediction - so if I observed a new Resp1 at a particular Cat1, there would be a 95% chance it would lie between the bounds of the confidence limits.\n\n\nIs each coefficient estimate different from zero?\nCat1 is in your best-specified model, so you know that there is evidence it has an effect50 on your response (Resp1). But is there an effect on Resp1 for all levels of Cat1 - are any of the coefficients statistically similar to 0?\n\nforComp &lt;- emmeans(object = bestMod1, # your model\n                   specs = pairwise ~ Cat1, # your covariates, with the request for a pairwise test\n                   type = \"response\") # report coefficients on the response scale\n\ntest(forComp)$emmeans # shows test of coefficients = 0\n\n Cat1 emmean   SE df t.ratio p.value\n Sp1    22.5 1.61 97  13.978  &lt;.0001\n Sp2    26.7 1.42 97  18.804  &lt;.0001\n Sp3    16.6 1.42 97  11.701  &lt;.0001\n\n\nHere we see that the coefficients for all levels are significantly different than zero (t-test, P &lt; 0.0001.)\n\n\nAre the coefficient estimates different than one another across categorical levels?\nFinally, we ask if all levels of Cat1 affect Resp1 in the same way - i.e. are the coefficients across factor levels significantly different from one another?\nTo get evidence about how each level in your categorical covariate affects your response, you need to test which effects differ from one another using multiple comparisons51, i.e. you will compare the modelled effect of each level of your categorical covariate vs. each other level of your categorical covariate to determine which are different from each other (called pairwise testing). You will do this by testing the null hypothesis that the modelled effects are similar to one another, typically rejecting the null hypothesis when p &lt; 0.05. Remember that the p-value is the probability that we observe a value at least as big as the one we observed even though our null hypothesis is true. In this case, we are looking at the difference between coefficients estimated for two levels of our covariate. The p-value is the probability of getting a difference at least as big as the one we observed even though there is actually no difference between the coefficients (the null hypothesis is true).\nA couple of things to note about multiple comparison testing:\n\nMultiple comparison testing on a categorical covariate should only be done after your hypothesis testing has given you evidence that the covariate has an effect on your response. That is, you should only do a multiple comparison test on a covariate if that covariate is in your best-specified model. As this is a test done after your hypothesis testing, it is called a post-hoc test.\nMultiple comparison testing can be a problem because you are essentially repeating a hypothesis test many times on the same data (i.e. are the effects of Sp1 different than those of Sp2? are the effects of Sp1 different than those of Sp3? are the effects of Sp2 different than those of Sp3?…). These repeated tests mean there is a high chance that you will find a difference in one test purely due to random chance, vs. due to there being an actual difference. To account for this, the multiple comparison tests you will perform have been formulated to correct for this increased error.\n\nMultiple comparison testing is very simple with the emmeans package. It just requires us to add that we would like to have multiple comparisons testing in our call by adding pairwise to the specs = argument:\n\nforComp &lt;- emmeans(object = bestMod1, # your model\n                   specs = pairwise ~ Cat1, # your covariates, with the request for a pairwise test\n                   type = \"response\") # report coefficients on the response scale\n\ntest(forComp)\n\n$emmeans\n Cat1 emmean   SE df t.ratio p.value\n Sp1    22.5 1.61 97  13.978  &lt;.0001\n Sp2    26.7 1.42 97  18.804  &lt;.0001\n Sp3    16.6 1.42 97  11.701  &lt;.0001\n\n\n$contrasts\n contrast  estimate   SE df t.ratio p.value\n Sp1 - Sp2    -4.19 2.15 97  -1.954  0.1292\n Sp1 - Sp3     5.89 2.15 97   2.744  0.0196\n Sp2 - Sp3    10.08 2.01 97   5.023  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nYou can see that you get two tables from this new emmeans() call. The first table ($emmeans) is the same data you saw before - the modelled coefficients (intercepts) for each level in Cat1. The second table ($contrasts) shows the results of the multiple comparison (pairwise) testing. The values in the p.value column tell us the results of the hypothesis test comparing the coefficients between the two levels. For example, for Sp1 vs. Sp2, there is a 13% (p = 0.13) probability of getting a difference in coefficients at least as big as 22.5 - 26.7 = -4.2 even though the null hypothesis (no difference) is true. This value 13% (p = 0.13) is too big (i.e. bigger than our threshold of 5% or p = 0.05) for us to believe we have evidence the coefficients are different from one another.\nBased on a threshold p-value of 0.05, we can see that:\nThe value of Resp1 when Cat1 is Sp1 is not statistically different than that when Cat1 is Sp2 as p = 0.13 is greater than p = 0.05.\nThe value of Resp1 when Cat1 is Sp1 is different (higher) than that when Cat1 is Sp3 as p = 0.02 is smaller than p = 0.05. The value of Resp1 when Cat1 is Sp2 is different (higher) than that when Cat1 is Sp3 as p &lt; 0.000152 is smaller than p = 0.05.53.\nNote that the p-values have been adjusted via the Tukey method which adjusts the difference that the two coefficients need to have to allow for the fact that we’re making multiple comparisons.54\nNote that you can also get the results from emmeans visually with\n\nplot(forComp,\n     comparisons = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nPlotting modelled effects\nAnd this brings us to another important aspect of reporting: visualizing your modelled effects. As mentioned last class, you want to include i) your model fit, ii) uncertainty around that fit, and iii) your observations on your plot. You can do this “by hand” with:\n\n# Set up your covariates for the visualized fit\nforCat1&lt;-unique(myDat1$Cat1) # every value of your categorical covariate\n\n# create a data frame with your covariates\nforVis&lt;-expand.grid(Cat1=forCat1) # expand.grid() function makes sure you have all combinations of covariates\n\n# Get your model fit estimates at each value of your covariates\nmodFit&lt;-predict(bestMod1, # the model\n                newdata = forVis, # the covariate values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\nlibrary(ggplot2)\n\nggplot() + # start ggplot\n  geom_point(data = myDat1, # add observations to your plot\n             mapping = aes(x = Cat1, y = Resp1), \n             position=position_jitter(width=0.1)) + # control position of data points so they are easier to see on the plot\n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat1, y = Fit, ymin = Lower, ymax = Upper),\n              size=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat1, y = Fit), \n             shape = 22, # shape of point\n             size = 3, # size of point\n             fill = \"white\", # fill colour for plot\n             col = 'black') + # outline colour for plot\n  ylab(\"Resp1, (units)\")+ # change y-axis label\n  xlab(\"Cat1, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\n\nReporting how well your model explains your response - Example 1\nAs a reminder from last week, you can estimate the deviance explained in your response by using a pseudo \\(R^2\\) called the Likelihood Ratio \\(R^2\\) or \\(R^2_{LR}\\).55. You can calculate \\(R^2_{LR}\\) by hand, read it from our dredge() output, or estimate it using r.squaredLR() from the MuMIn package:\n\nr.squaredLR(bestMod1)\n\n[1] 0.2076829\nattr(,\"adj.r.squared\")\n[1] 0.2078199\n\n\nNote here that two values of \\(R^2_{LR}\\) are reported. The adjusted pseudo \\(R^2\\) given here under attr(,\"adj.r.squared\") has been scaled so that \\(R^2_{LR}\\) can reach a maximum of 1, to be equivalent to a regular \\(R^2\\).\nSo you estimate that ~ 21% of the deviance in Resp1 is explained by Cat1 via bestMod1.\nOne nice feature of the \\(R^2_{LR}\\) is that it is equivalent to the regular \\(R^2\\) when your model assumes a normal error distribution assumption and linear shape assumption, so you can use \\(R^2_{LR}\\) for any of the models we’re discussing in class. Let’s check this here by comparing our \\(R^2_{LR}\\) to that from true \\(R^2\\) we calculated in Week 9:\n\n1-summary(bestMod1)$deviance/summary(bestMod1)$null.deviance\n\n[1] 0.2076829\n\n\n\\(R^2_{LR}\\) and \\(R^2\\) are identical because you have a normal error distribution assumption and linear shape assumption.\n\n\nReporting covariate importance - Example 1\nWith Example 1, you have only one covariate (Cat1) and so this covariate is responsible for explaining all of the variability in your response (Resp1). You can see that it appears in all models with any weight with your sw() function from the MuMIn package.\n\ndredgeOut1\n\nGlobal model call: glm(formula = Resp1 ~ Cat1, family = gaussian(link = \"identity\"), \n    data = myDat1)\n---\nModel selection table \n  (Intrc) Cat1    R^2 df   logLik  AICc delta weight\n2   22.50    + 0.2077  4 -354.584 717.6  0.00      1\n1   21.89      0.0000  2 -366.223 736.6 18.98      0\nModels ranked by AICc(x) \n\nsw(dredgeOut1)\n\n                     Cat1\nSum of weights:      1   \nN containing models: 1   \n\n\n\n\n\nPredicting with Example 1\nWith Example 1, we have only a categorical covariate. In such cases, it doesn’t make sense to use your model to make a prediction as you can’t make a prediction for a level not already in your covariate, e.g. there’s no way to predict Resp1 if Cat1 was Sp5 or Sp89.\nSo if you have only categorical covariates, you aren’t able to use your model for prediction. Coming up (Example 3) we’ll look at an example with a combination of continuous and categorical covariates and, in these cases, prediction is possible."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#example-2-resp2-cat2-cat3-cat2cat3",
    "href": "DSPPH_SM_Reporting.html#example-2-resp2-cat2-cat3-cat2cat3",
    "title": "From statistical modelling to scientific report writing",
    "section": "Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3",
    "text": "Example 2: Resp2 ~ Cat2 + Cat3 + Cat2:Cat3\nFor Example #2, assume your best-specified model is that your response variable (Resp2) is explained by two categorical covariates (Cat2 & Cat3) as well as the interaction between the covariates:\nResp2 ~ Cat2 + Cat3 + Cat2:Cat3\nYour best-specified model for example #2 is in an object called bestMod2:\n\nbestMod2\n\n\nCall:  glm(formula = Resp2 ~ Cat2 + Cat3 + Cat2:Cat3 + 1, family = gaussian(link = \"identity\"), \n    data = myDat2)\n\nCoefficients:\n          (Intercept)              Cat2TypeB              Cat2TypeC  \n               364.91                  37.37                 -75.94  \n            Cat2TypeD             Cat3Treat2            Cat3Control  \n              -141.97                 -43.63                  64.12  \n Cat2TypeB:Cat3Treat2   Cat2TypeC:Cat3Treat2   Cat2TypeD:Cat3Treat2  \n                42.71                  73.61                  66.02  \nCat2TypeB:Cat3Control  Cat2TypeC:Cat3Control  Cat2TypeD:Cat3Control  \n                81.25                 -19.08                 -71.72  \n\nDegrees of Freedom: 99 Total (i.e. Null);  88 Residual\nNull Deviance:      1249000 \nResidual Deviance: 377000   AIC: 1133\n\n\nthat was fit to data in myDat2:\n\nsummary(myDat2)\n\n     Resp2          Cat2         Cat3   \n Min.   :119.9   TypeA:27   Treat1 :29  \n 1st Qu.:275.6   TypeB:22   Treat2 :19  \n Median :338.8   TypeC:29   Control:52  \n Mean   :348.3   TypeD:22               \n 3rd Qu.:420.3                          \n Max.   :621.5                          \n\n\nand the dredge() table you used to pick your bestMod2 is in dredgeOut2\n\ndredgeOut2\n\nGlobal model call: glm(formula = Resp2 ~ Cat2 + Cat3 + Cat2:Cat3, family = gaussian(link = \"identity\"), \n    data = myDat2)\n---\nModel selection table \n  (Int) Ct2 Ct3 Ct2:Ct3     R^2 df   logLik   AICc delta weight\n8 364.9   +   +       + 0.69800 13 -553.634 1137.5  0.00  0.964\n4 354.9   +   +         0.62540  7 -564.419 1144.1  6.55  0.036\n2 388.5   +             0.55300  5 -573.243 1157.1 19.63  0.000\n1 348.3                 0.00000  2 -613.508 1231.1 93.64  0.000\n3 331.1       +         0.03933  4 -611.502 1231.4 93.92  0.000\nModels ranked by AICc(x) \n\n\n\nReporting Example 2\n\nReporting best-specified model(s) - Example 2\nFor Example 2, you will report that your best-specified model is Resp2 ~ Cat2 + Cat3 + Cat2:Cat3, i.e. that there is evidence that Cat2 and Cat3 explain variability in Resp2, and that there is an interaction between Cat2 and Cat3 - i.e. the effect of Cat2 on Resp2 depends on the value of Cat3. Remember that you can also use the output from dredgeOut2 to report evidence for how you picked the best-specified model, e.g. if you want to report more than one model.\n\n\nGlobal model call: glm(formula = Resp2 ~ Cat2 + Cat3 + Cat2:Cat3, family = gaussian(link = \"identity\"), \n    data = myDat2)\n---\nModel selection table \n  (Int) Ct2 Ct3 Ct2:Ct3     R^2 df   logLik   AICc delta weight\n8 364.9   +   +       + 0.69800 13 -553.634 1137.5  0.00  0.964\n4 354.9   +   +         0.62540  7 -564.419 1144.1  6.55  0.036\n2 388.5   +             0.55300  5 -573.243 1157.1 19.63  0.000\n1 348.3                 0.00000  2 -613.508 1231.1 93.64  0.000\n3 331.1       +         0.03933  4 -611.502 1231.4 93.92  0.000\nModels ranked by AICc(x) \n\n\n\n\nReporting modelled effects - Example 2\n\nReporting coefficients\n\nWhat are your coefficient estimates (with uncertainty)?\nAgain, for categorical covariates, there is a coefficient estimated for each level of the covariate. If there is one or more interactions among covariates, there will be one coefficient for each combination of levels among covariates. Let’s look at the summary() output of your model to understand better:\n\ncoef(summary(bestMod2)) # extract the coefficients from summary()\n\n                        Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)            364.90879   24.73850 14.7506420 1.629062e-25\nCat2TypeB               37.37325   32.98467  1.1330491 2.602712e-01\nCat2TypeC              -75.94350   33.87459 -2.2419017 2.748208e-02\nCat2TypeD             -141.96558   38.32472 -3.7042819 3.695537e-04\nCat3Treat2             -43.62974   36.41409 -1.1981554 2.340735e-01\nCat3Control             64.11573   30.29835  2.1161457 3.715672e-02\nCat2TypeB:Cat3Treat2    42.70879   53.60009  0.7968045 4.277091e-01\nCat2TypeC:Cat3Treat2    73.60837   54.15228  1.3592849 1.775295e-01\nCat2TypeD:Cat3Treat2    66.02110   55.13228  1.1975037 2.343260e-01\nCat2TypeB:Cat3Control   81.25030   43.24327  1.8789121 6.356678e-02\nCat2TypeC:Cat3Control  -19.07730   41.29748 -0.4619483 6.452584e-01\nCat2TypeD:Cat3Control  -71.72444   46.17117 -1.5534463 1.239057e-01\n\n\nComparing this output to that of Example 1 shows many more estimated coefficients for Example 2. This is because we have one coefficient for each level of each covariate, as well as coefficients for each combination of levels of the covariates.\nAgain, it takes a little practice to read the coefficients from the summary() output. For Example 2:\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat1 is 365 units (the intercept).\nThe modelled prediction for Resp2 when Cat2 is TypeB and Cat3 is Treat1 is 365 + 37 = 402 units.\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat2 is 365 - 44 = 321 units.\nThe modelled prediction for Resp2 when Cat2 is TypeC and Cat3 is Treat2 is 365 - 76 - 44 + 74 = 319 units.\nAs above, we can use the emmeans package to more easily see these coefficients:\n\nemmeans(object = bestMod2, # your model\n        specs = ~ Cat2 + Cat3 + Cat2:Cat3, # your covariates\n        type = \"response\") # report coefficients on the response scale\n\n Cat2  Cat3    emmean   SE df lower.CL upper.CL\n TypeA Treat1     365 24.7 88      316      414\n TypeB Treat1     402 21.8 88      359      446\n TypeC Treat1     289 23.1 88      243      335\n TypeD Treat1     223 29.3 88      165      281\n TypeA Treat2     321 26.7 88      268      374\n TypeB Treat2     401 32.7 88      336      466\n TypeC Treat2     319 32.7 88      254      384\n TypeD Treat2     245 29.3 88      187      304\n TypeA Control    429 17.5 88      394      464\n TypeB Control    548 21.8 88      504      591\n TypeC Control    334 15.9 88      302      366\n TypeD Control    215 18.9 88      178      253\n\nConfidence level used: 0.95 \n\n\nSo now we have a modelled value of our response for each level of our categorical covariate. For example:\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat1 is 365 +/- 25 units.\nThe modelled prediction for Resp2 when Cat2 is TypeB and Cat3 is Treat1 is 402 +/- 22 units.\nThe modelled prediction for Resp2 when Cat2 is TypeA and Cat3 is Treat2 is 321 +/- 27 units.\nThe modelled prediction for Resp2 when Cat2 is TypeC and Cat3 is Treat2 is 319 +/- 33 units.\n\n\nIs each coefficient estimate different from zero?\nHere we find out if there is an effect on Resp1 for all levels of Cat1 - are any of the coefficients statistically similar to 0?\n\nforComp &lt;- emmeans(object = bestMod2, # your model\n                   specs = pairwise ~ Cat2 + Cat3 + Cat2:Cat3, # your covariates, with the request for a pairwise test\n                   type = \"response\") # report coefficients on the response scale\n\ntest(forComp)$emmeans # shows test of coefficients = 0\n\n Cat2  Cat3    emmean   SE df t.ratio p.value\n TypeA Treat1     365 24.7 88  14.751  &lt;.0001\n TypeB Treat1     402 21.8 88  18.439  &lt;.0001\n TypeC Treat1     289 23.1 88  12.487  &lt;.0001\n TypeD Treat1     223 29.3 88   7.617  &lt;.0001\n TypeA Treat2     321 26.7 88  12.024  &lt;.0001\n TypeB Treat2     401 32.7 88  12.264  &lt;.0001\n TypeC Treat2     319 32.7 88   9.746  &lt;.0001\n TypeD Treat2     245 29.3 88   8.381  &lt;.0001\n TypeA Control    429 17.5 88  24.526  &lt;.0001\n TypeB Control    548 21.8 88  25.102  &lt;.0001\n TypeC Control    334 15.9 88  21.040  &lt;.0001\n TypeD Control    215 18.9 88  11.397  &lt;.0001\n\n\nHere we see that the coefficients for all combinations of levels of our two covariates are significantly different than zero (t-test, P &lt; 0.0001.)\n\n\nAre the coefficient estimates different than one another across categorical levels?\nAs with Example 1, we can also find out which combinations of covariate levels are leading to statistically different model predictions in Resp2:\n\nforComp &lt;- emmeans(object = bestMod2, # your model\n            specs = pairwise ~ Cat2 + Cat3 + Cat2:Cat3, # your covariates, with pairwise to indicate you want multiple comparisons\n            type = \"response\") # report coefficients on the response scale\n\ntest(forComp)\n\n$emmeans\n Cat2  Cat3    emmean   SE df t.ratio p.value\n TypeA Treat1     365 24.7 88  14.751  &lt;.0001\n TypeB Treat1     402 21.8 88  18.439  &lt;.0001\n TypeC Treat1     289 23.1 88  12.487  &lt;.0001\n TypeD Treat1     223 29.3 88   7.617  &lt;.0001\n TypeA Treat2     321 26.7 88  12.024  &lt;.0001\n TypeB Treat2     401 32.7 88  12.264  &lt;.0001\n TypeC Treat2     319 32.7 88   9.746  &lt;.0001\n TypeD Treat2     245 29.3 88   8.381  &lt;.0001\n TypeA Control    429 17.5 88  24.526  &lt;.0001\n TypeB Control    548 21.8 88  25.102  &lt;.0001\n TypeC Control    334 15.9 88  21.040  &lt;.0001\n TypeD Control    215 18.9 88  11.397  &lt;.0001\n\n\n$contrasts\n contrast                      estimate   SE df t.ratio p.value\n TypeA Treat1 - TypeB Treat1    -37.373 33.0 88  -1.133  0.9923\n TypeA Treat1 - TypeC Treat1     75.944 33.9 88   2.242  0.5248\n TypeA Treat1 - TypeD Treat1    141.966 38.3 88   3.704  0.0180\n TypeA Treat1 - TypeA Treat2     43.630 36.4 88   1.198  0.9878\n TypeA Treat1 - TypeB Treat2    -36.452 41.0 88  -0.889  0.9991\n TypeA Treat1 - TypeC Treat2     45.965 41.0 88   1.120  0.9929\n TypeA Treat1 - TypeD Treat2    119.574 38.3 88   3.120  0.0940\n TypeA Treat1 - TypeA Control   -64.116 30.3 88  -2.116  0.6129\n TypeA Treat1 - TypeB Control  -182.739 33.0 88  -5.540  &lt;.0001\n TypeA Treat1 - TypeC Control    30.905 29.4 88   1.051  0.9959\n TypeA Treat1 - TypeD Control   149.574 31.1 88   4.805  0.0004\n TypeB Treat1 - TypeC Treat1    113.317 31.8 88   3.563  0.0277\n TypeB Treat1 - TypeD Treat1    179.339 36.5 88   4.912  0.0002\n TypeB Treat1 - TypeA Treat2     81.003 34.5 88   2.348  0.4517\n TypeB Treat1 - TypeB Treat2      0.921 39.3 88   0.023  1.0000\n TypeB Treat1 - TypeC Treat2     83.338 39.3 88   2.119  0.6110\n TypeB Treat1 - TypeD Treat2    156.947 36.5 88   4.299  0.0025\n TypeB Treat1 - TypeA Control   -26.742 28.0 88  -0.956  0.9982\n TypeB Treat1 - TypeB Control  -145.366 30.9 88  -4.711  0.0005\n TypeB Treat1 - TypeC Control    68.278 27.0 88   2.531  0.3355\n TypeB Treat1 - TypeD Control   186.947 28.9 88   6.477  &lt;.0001\n TypeC Treat1 - TypeD Treat1     66.022 37.3 88   1.769  0.8297\n TypeC Treat1 - TypeA Treat2    -32.314 35.3 88  -0.914  0.9988\n TypeC Treat1 - TypeB Treat2   -112.396 40.1 88  -2.804  0.1965\n TypeC Treat1 - TypeC Treat2    -29.979 40.1 88  -0.748  0.9998\n TypeC Treat1 - TypeD Treat2     43.631 37.3 88   1.169  0.9900\n TypeC Treat1 - TypeA Control  -140.059 29.0 88  -4.828  0.0003\n TypeC Treat1 - TypeB Control  -258.683 31.8 88  -8.134  &lt;.0001\n TypeC Treat1 - TypeC Control   -45.038 28.1 88  -1.605  0.9027\n TypeC Treat1 - TypeD Control    73.631 29.9 88   2.465  0.3757\n TypeD Treat1 - TypeA Treat2    -98.336 39.6 88  -2.481  0.3654\n TypeD Treat1 - TypeB Treat2   -178.418 43.9 88  -4.064  0.0056\n TypeD Treat1 - TypeC Treat2    -96.001 43.9 88  -2.186  0.5636\n TypeD Treat1 - TypeD Treat2    -22.391 41.4 88  -0.541  1.0000\n TypeD Treat1 - TypeA Control  -206.081 34.1 88  -6.043  &lt;.0001\n TypeD Treat1 - TypeB Control  -324.705 36.5 88  -8.894  &lt;.0001\n TypeD Treat1 - TypeC Control  -111.061 33.3 88  -3.335  0.0532\n TypeD Treat1 - TypeD Control     7.609 34.8 88   0.218  1.0000\n TypeA Treat2 - TypeB Treat2    -80.082 42.2 88  -1.895  0.7587\n TypeA Treat2 - TypeC Treat2      2.335 42.2 88   0.055  1.0000\n TypeA Treat2 - TypeD Treat2     75.945 39.6 88   1.916  0.7460\n TypeA Treat2 - TypeA Control  -107.746 31.9 88  -3.374  0.0478\n TypeA Treat2 - TypeB Control  -226.369 34.5 88  -6.562  &lt;.0001\n TypeA Treat2 - TypeC Control   -12.725 31.1 88  -0.409  1.0000\n TypeA Treat2 - TypeD Control   105.945 32.7 88   3.237  0.0693\n TypeB Treat2 - TypeC Treat2     82.417 46.3 88   1.781  0.8238\n TypeB Treat2 - TypeD Treat2    156.026 43.9 88   3.554  0.0285\n TypeB Treat2 - TypeA Control   -27.663 37.1 88  -0.745  0.9998\n TypeB Treat2 - TypeB Control  -146.287 39.3 88  -3.719  0.0172\n TypeB Treat2 - TypeC Control    67.357 36.4 88   1.852  0.7846\n TypeB Treat2 - TypeD Control   186.027 37.8 88   4.923  0.0002\n TypeC Treat2 - TypeD Treat2     73.609 43.9 88   1.677  0.8739\n TypeC Treat2 - TypeA Control  -110.081 37.1 88  -2.967  0.1366\n TypeC Treat2 - TypeB Control  -228.704 39.3 88  -5.815  &lt;.0001\n TypeC Treat2 - TypeC Control   -15.060 36.4 88  -0.414  1.0000\n TypeC Treat2 - TypeD Control   103.609 37.8 88   2.742  0.2240\n TypeD Treat2 - TypeA Control  -183.690 34.1 88  -5.387  &lt;.0001\n TypeD Treat2 - TypeB Control  -302.313 36.5 88  -8.281  &lt;.0001\n TypeD Treat2 - TypeC Control   -88.669 33.3 88  -2.663  0.2624\n TypeD Treat2 - TypeD Control    30.000 34.8 88   0.861  0.9993\n TypeA Control - TypeB Control -118.624 28.0 88  -4.242  0.0030\n TypeA Control - TypeC Control   95.021 23.6 88   4.023  0.0064\n TypeA Control - TypeD Control  213.690 25.7 88   8.299  &lt;.0001\n TypeB Control - TypeC Control  213.644 27.0 88   7.918  &lt;.0001\n TypeB Control - TypeD Control  332.314 28.9 88  11.514  &lt;.0001\n TypeC Control - TypeD Control  118.669 24.7 88   4.809  0.0004\n\nP value adjustment: tukey method for comparing a family of 12 estimates \n\n\nAgain, the second table ($contrasts) shows the results of the multiple comparison testing. Based on a threshold p-value of 0.05, we can see that the effects at some combinations of our covariates are not statistically different from each other.\nFor example, the coefficient (predicted Resp2) when Cat3 = Treat1 and Cat2 = TypeA is 365 and this is 37 lower than the coefficient when Cat3 = Treat1 and Cat2 = TypeB, but this difference is not statistically different as p = 0.99 for this comparison.\nOn the other hand, some combinations of our covariates are statistically different from each other. For example, A comparison of modelled Resp2 when Cat2 = TypeD and Cat3 = Treat2 (245) vs. Cat2 = TypeB and Cat3 = Control (548) shows that they differ by 302 and are statistically different from one another with p &lt; 0.001.\nYou can also get the results from emmeans visually with\n\nplot(forComp,\n     comparisons = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting modelled effects\nYou can visualize your results (model fit, uncertainty and observations) for Example 2 with:\n\n# Set up your covariates for the visualized fit\nforCat2&lt;-unique(myDat2$Cat2) # every level of your categorical covariate\nforCat3&lt;-unique(myDat2$Cat3) # every level of your categorical covariate\n  \n# create a data frame with your covariates\nforVis&lt;-expand.grid(Cat2 = forCat2, Cat3 = forCat3) # expand.grid() function makes sure you have all combinations of covariates\n\n# Get your model fit estimates at each value of your covariates\nmodFit&lt;-predict(bestMod2, # the model\n                newdata = forVis, # the covariate values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  geom_point(data = myDat2, # add observations to your plot\n             mapping = aes(x = Cat2, y = Resp2, col = Cat3), \n             position=position_jitterdodge(jitter.width=0.75, dodge.width=0.75)) + # control position of data points so they are easier to see on the plot\n  geom_errorbar(data = forVis, # add the uncertainty to your plot\n              mapping = aes(x = Cat2, y = Fit, ymin = Lower, ymax = Upper, col = Cat3),\n              position=position_dodge(width=0.75), # control position of data points so they are easier to see on the plot\n              size=1.2) + # control thickness of errorbar line\n  geom_point(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cat2, y = Fit, fill = Cat3), \n             shape = 22, # shape of point\n             size=3, # size of point\n             col = 'black', # outline colour for point\n             position=position_dodge(width=0.75)) + # control position of data points so they are easier to see on the plot\n  ylab(\"Resp2, (units)\")+ # change y-axis label\n  xlab(\"Cat2, (units)\")+ # change x-axis label\n  labs(fill=\"Cat3, (units)\", col=\"Cat3, (units)\") + # change legend title\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\n\n\nReporting how well your model explains your response - Example 2\nAs for Example 1, you can report the explained deviance as the Likelihood Ratio \\(R^2\\):\n\nr.squaredLR(bestMod2)\n\n[1] 0.6980474\nattr(,\"adj.r.squared\")\n[1] 0.6980507\n\n\nSo ~ 70% of the deviance in Resp2 is explained by Cat2 and Cat3 and the interaction between the two covariates.\n\n\nReporting covariate importance - Example 2\nYou can report the importance of each model term in explaining deviance in Resp2 with the sum of Akaike weights function (sw() in the MuMIn package), as we did in class:\n\ndredgeOut2\n\nGlobal model call: glm(formula = Resp2 ~ Cat2 + Cat3 + Cat2:Cat3, family = gaussian(link = \"identity\"), \n    data = myDat2)\n---\nModel selection table \n  (Int) Ct2 Ct3 Ct2:Ct3     R^2 df   logLik   AICc delta weight\n8 364.9   +   +       + 0.69800 13 -553.634 1137.5  0.00  0.964\n4 354.9   +   +         0.62540  7 -564.419 1144.1  6.55  0.036\n2 388.5   +             0.55300  5 -573.243 1157.1 19.63  0.000\n1 348.3                 0.00000  2 -613.508 1231.1 93.64  0.000\n3 331.1       +         0.03933  4 -611.502 1231.4 93.92  0.000\nModels ranked by AICc(x) \n\nsw(dredgeOut2)\n\n                     Cat2 Cat3 Cat2:Cat3\nSum of weights:      1.00 1.00 0.96     \nN containing models:    3    3    1     \n\n\nThis tells you that Cat2 and Cat3 appear in all models with any weight and that the interaction term Cat2:Cat3 is slightly less important in explaining deviance in Resp2, though it does appear in the best-specified model.\n\n\n\nPredicting with Example 2\nAs with Example 1, it doesn’t make sense to make predictions with your model for Example 2 as you can’t make predictions to levels of your categorical covariates not already in your model."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#example-3-resp3-cat4-cont5-cat4cont5",
    "href": "DSPPH_SM_Reporting.html#example-3-resp3-cat4-cont5-cat4cont5",
    "title": "From statistical modelling to scientific report writing",
    "section": "Example 3: Resp3 ~ Cat4 + Cont5 + Cat4:Cont5",
    "text": "Example 3: Resp3 ~ Cat4 + Cont5 + Cat4:Cont5\nFor Example #3, assume your best-specified model is that your response variable (Resp3) is explained by one categorical covariate (Cat4) and one continuous covariate (Cont5) as well as the interaction between the covariates:\nResp3 ~ Cat4 + Cont5 + Cat4:Cont5\nYour best-specified model for example #3 is in an object called bestMod3:\n\nbestMod3\n\n\nCall:  glm(formula = Resp3 ~ Cat4 + Cont5 + Cat4:Cont5 + 1, family = gaussian(link = \"identity\"), \n    data = myDat3)\n\nCoefficients:\n    (Intercept)        Cat4Urban         Cat4Wild            Cont5  \n      98.346793        -0.236424        -0.867336        -0.002931  \nCat4Urban:Cont5   Cat4Wild:Cont5  \n       0.015117         0.030858  \n\nDegrees of Freedom: 99 Total (i.e. Null);  94 Residual\nNull Deviance:      4454 \nResidual Deviance: 642.8    AIC: 483.9\n\n\nthat was fit to data in myDat3:\n\nsummary(myDat3)\n\n     Resp3           Cat4        Cont5      \n Min.   : 92.19   Farm :34   Min.   :310.9  \n 1st Qu.: 97.88   Urban:39   1st Qu.:418.7  \n Median :104.22   Wild :27   Median :497.8  \n Mean   :103.98              Mean   :509.8  \n 3rd Qu.:108.63              3rd Qu.:609.6  \n Max.   :118.41              Max.   :695.5  \n\n\nand the dredge() table you used to pick your bestMod3 is in dredgeOut3\n\ndredgeOut3\n\nGlobal model call: glm(formula = Resp3 ~ Cat4 + Cont5 + Cat4:Cont5, family = gaussian(link = \"identity\"), \n    data = myDat3)\n---\nModel selection table \n   (Int) Ct4       Cn5 Ct4:Cn5    R^2 df   logLik  AICc  delta weight\n8  98.35   + -0.002931       + 0.8557  7 -234.930 485.1   0.00      1\n4  92.25   +  0.009650         0.8166  5 -246.915 504.5  19.39      0\n2  96.93   +                   0.7923  4 -253.133 514.7  29.61      0\n3  94.92      0.017780         0.0848  3 -327.278 660.8 175.73      0\n1 104.00                       0.0000  2 -331.708 667.5 182.46      0\nModels ranked by AICc(x) \n\n\n\nReporting Example 3\n\nReporting best-specified model(s) - Example 3\nFor Example 3, you will report that your best-specified model is Resp3 ~ Cat4 + Cont5 + Cat4:Cont5. This model says that there is evidence that Cat4 and Cont5 explain variability in Resp3, and that there is an interaction between Cat4 and Cont5 - i.e. the effect of Cont5 on Resp3 depends on the value of Cat4. Remember that you can also use the output from dredgeOut3 to report evidence for how you picked the best-specified model, e.g. if you want to report more than one model.\n\n\nGlobal model call: glm(formula = Resp3 ~ Cat4 + Cont5 + Cat4:Cont5, family = gaussian(link = \"identity\"), \n    data = myDat3)\n---\nModel selection table \n   (Int) Ct4       Cn5 Ct4:Cn5    R^2 df   logLik  AICc  delta weight\n8  98.35   + -0.002931       + 0.8557  7 -234.930 485.1   0.00      1\n4  92.25   +  0.009650         0.8166  5 -246.915 504.5  19.39      0\n2  96.93   +                   0.7923  4 -253.133 514.7  29.61      0\n3  94.92      0.017780         0.0848  3 -327.278 660.8 175.73      0\n1 104.00                       0.0000  2 -331.708 667.5 182.46      0\nModels ranked by AICc(x) \n\n\n\n\nReporting modelled effects - Example 3\n\nReporting coefficients\nThe coefficients estimated for this model will include values of the response (Resp3) at each level of the categorical covariate (Cat4) as well as slopes describing the change in the response when the continuous covariate (Cont5) changes, and a different slope will be estimated for each level in Cat4 (this is because of the interaction in the model).\n\nWhat are your coefficient estimates (with uncertainty)?\nAs above, you can take a look at your modelled coefficients with the summary() output:\n\ncoef(summary(bestMod3))\n\n                   Estimate  Std. Error     t value     Pr(&gt;|t|)\n(Intercept)     98.34679337 1.868900753 52.62280151 1.536692e-71\nCat4Urban       -0.23642403 2.895404981 -0.08165491 9.350947e-01\nCat4Wild        -0.86733554 3.238562391 -0.26781498 7.894285e-01\nCont5           -0.00293078 0.003740830 -0.78345704 4.353283e-01\nCat4Urban:Cont5  0.01511745 0.005611511  2.69400763 8.361918e-03\nCat4Wild:Cont5   0.03085821 0.006183238  4.99062262 2.756559e-06\n\n\nThis shows:\nThe model prediction of Resp3 when Cat4 is Farm and Cont5 is 0 is 98.34 units56 (the intercept).\nThe model prediction of Resp3 when Cat4 is Urban and Cont5 is 0 is 98.34 - 0.24 = 98.1 units.\nThe model prediction of Resp3 when Cat4 is Wild and Cont5 is 0 is 98.34 - 0.87 = 97.5 units.\nThe slope of the relationship between Cont5 and Resp3 when Cat4 is Farm is -0.0029.57\nThe slope of the relationship between Cont5 and Resp3 when Cat4 is Urban is -0.0029 + 0.015 = 0.0121.\nThe slope of the relationship between Cont5 and Resp3 when Cat4 is Wild is -0.0029 + 0.031 = 0.0281.\nAgain, interpreting the coefficients from the summary() output is tedious and not necessary: You can use the emmeans package to give you the modelled response for each level of the categorical covariate (Cat4) directly:\n\nemmeans(object = bestMod3, # your model\n        specs = ~ Cat4 + Cont5 + Cat4:Cont5, # your covariates\n        type = \"response\") # report coefficients on the response scale\n\n Cat4  Cont5 emmean    SE df lower.CL upper.CL\n Farm    510   96.9 0.458 94     95.9     97.8\n Urban   510  104.3 0.421 94    103.5    105.2\n Wild    510  111.7 0.511 94    110.7    112.7\n\nConfidence level used: 0.95 \n\n\nNote that emmeans() sets our continuous covariate (Cont5) to the mean value of Cont5 (510 units). We can also control this with the at = argument:\n\nemmeans(object = bestMod3, # your model\n        specs = ~ Cat4 + Cont5 + Cat4:Cont5, # your covariates\n        type = \"response\",  # report coefficients on the response scale\n        at = list(Cont5 = 0)) # control the value of your continuous covariate at which to make the coefficient estimates\n\n Cat4  Cont5 emmean   SE df lower.CL upper.CL\n Farm      0   98.3 1.87 94     94.6      102\n Urban     0   98.1 2.21 94     93.7      103\n Wild      0   97.5 2.64 94     92.2      103\n\nConfidence level used: 0.95 \n\n\nBy setting at = 0, you get the intercept - i.e. the modelled Resp3 when Cont5 = 0 for each level of Cat4, and this is what is reported in the summary() output.\nSimilarly, you can get the emmeans package to give you the slope coefficients for the continuous covariate (Cont5) using the emtrends() function, rather than interpretting them from the summary() output:\n\nemtrends(bestMod,  ~ Cat4, # your categorical covariate\n         var = \"Cont5\") # your continuous covariates\n\n Cat4  Cont5.trend      SE df lower.CL upper.CL\n Farm     -0.00293 0.00374 94 -0.01036   0.0045\n Urban     0.01219 0.00418 94  0.00388   0.0205\n Wild      0.02793 0.00492 94  0.01815   0.0377\n\nConfidence level used: 0.95 \n\n\n\nOne thing to note:\n\nThe emmeans() function will convert the coefficients (intercepts) from the link to the response scale. You can ask for this (as we did above) with the type = \"response\" argument. Note that it makes no difference for these examples as we are using a normal error distribution assumption and so the link and response scale are identical (i.e. when link = \"identity\").\nIn contrast, the emtrends() function does not convert the coefficients (slopes) to represent effects on the response scale. This is because emtrends() is reporting the slope of a straight line - the trend line on the link scale. But the line isn’t straight on the response scale.\n\nWe need to convert from the link to the response by hand when we use emtrends()^[remember, the coefficients for categorical covariates are also being converted from the link to the response scale. It is just that R is doing it for us in the emmeans() function when we add the argument `type = “response”). How we make the conversion, and interpret the result, will depend on our error distribution assumption:\n\nAgain, if we are using link = \"identity\" (as with a normal error distribution), the link and response scale are identical and no conversion is necessary. In this case, the coefficient tells you the absolute change in your response from a unit change in your covariate.\n\nIf you are using link = \"log\" (as with a poisson error distribution, and sometimes also a Gamma error distribution), you get your coefficient on the response scale by taking e to the coefficient on the link scale (with the R function exp()). This coefficient is called the rate ratio58 and it tells you the % change in the response for a unit change in your covariate.\nBut why do we convert coefficients from the link to response scale with ex when link = \"log\"?\nGiven\n\\[\n\\begin{align}\nlog_e(\\mu_i|Cov_i) &= \\beta_1 \\cdot Cov_i + \\beta_0 \\\\\nResp_i &\\sim poisson(\\mu_i)\n\\end{align}\n\\]\nThen the rate ratio (RR), or % change in the response for a unit change in the covariate becomes,\n\\[\n\\begin{align}\nRR&=\\frac{(\\mu_i|Cov = a+1)}{(\\mu_i|Cov = a)}\\\\[2em]\nlog_e(RR)&=log_e(\\frac{(\\mu_i|Cov = a+1)}{(\\mu_i|Cov = a)})\\\\[2em]\nlog_e(RR)&=log_e(\\mu_i|Cov = a+1)-log_e(\\mu_i|Cov = a)\\\\[2em]\nlog_e(RR)&=(\\beta_1\\cdot (a+1)+\\beta_0)-(\\beta_1\\cdot (a)+\\beta_0)\\\\[2em]\nlog_e(RR)&=\\beta_1\\cdot a + \\beta_1+\\beta_0-\\beta_1\\cdot a+\\beta_0\\\\[2em]\nlog_e(RR)&=\\beta_1\\\\[2em]\nRR &=exp^{\\beta_1}\n\\end{align}\n\\]\nIf you are using link = \"logit\" (as with a binomial error distribution), you also get your coefficient on the response scale by taking e to the coefficient on the link scale (with the R function exp()), but here your coefficient on the response scale tells you the % change in the odds (probability of a success vs. the probability of a failure) given a unit change in your covariate - this estimate is called the odds ratio.\nBut why do we convert coefficients from the link to response scale with ex when link = \"logit\"?\nGiven a binomial model, \\[\n\\begin{align}\nlog_e(\\frac{p_i}{1-p_i}) &= \\beta_1 \\cdot Cov_i + \\beta_0 \\\\\nResp_i &\\sim binomial(p_i)\n\\end{align}\n\\] \\(p_i\\) is the probability of success and \\(1-p_i\\) is the probability of failure, and the odds are \\(\\frac{p_i}{1-p_i}\\).\nThen the odds ratio (OR), or % change in odds due to a change in covariate is:\n\\[\n\\begin{align}\nOR&=\\frac{odds\\;when\\;Cov=a+1}{odds\\;when \\;Cov=a}\\\\[2em]\nOR&=\\frac{(\\frac{p}{1-p}|Cov=a+1)}{(\\frac{p}{1-p}|Cov=a)}\\\\[2em]\nlog_e(OR)&=(log_e\\frac{p}{1-p}|Cov=a+1) - (log_e\\frac{p}{1-p}|Cov=a)\\\\[2em]\nlog_e(OR)&=(\\beta_1\\cdot (a+1)+\\beta_0) - (\\beta_1\\cdot a+\\beta_0)\\\\[2em]\nlog_e(OR)&=\\beta_1\\cdot a + \\beta_1-\\beta_1\\cdot a\\\\[2em]\nOR &=exp^{\\beta_1}\n\\end{align}\n\\]\n\nFor a model where link = \"inverse\", the interpretation on the response scale is less clear. In this case, you would use your model to make predictions and report the effects by describing these changes in predictions.\n\n\n\nIs each coefficient estimate different from zero?\nYou can find out if the coefficients are different than zero, similar to above. You use emmeans() with test() when the covariate is categorical;\n\nforInt &lt;- emmeans(object = bestMod3, # your model\n                  specs = ~ Cat4 + Cont5 + Cat4:Cont5, # your covariates\n                  type = \"response\",  # report coefficients on the response scale\n                  at = list(Cont5 = 0)) # control the value of your continuous covariate at which to make the coefficient estimates\n\ntest(forInt) # get test if coefficient is different than zero.\n\n Cat4  Cont5 emmean   SE df t.ratio p.value\n Farm      0   98.3 1.87 94  52.623  &lt;.0001\n Urban     0   98.1 2.21 94  44.364  &lt;.0001\n Wild      0   97.5 2.64 94  36.856  &lt;.0001\n\n\nand we see that all coefficients are significantly different than zero (t-test, P &lt; 0.0001).\nFor the continuous covariate, you use emtrends() with test():\n\nforSlope &lt;- emtrends(bestMod, # your model\n                      ~ Cat4, # your categorical covariate, with a request for pairwise testing\n                      var = \"Cont5\") # your continuous covariate\n\ntest(forSlope)\n\n Cat4  Cont5.trend      SE df t.ratio p.value\n Farm     -0.00293 0.00374 94  -0.783  0.4353\n Urban     0.01219 0.00418 94   2.914  0.0045\n Wild      0.02793 0.00492 94   5.673  &lt;.0001\n\n\nAnd we can see that\n\nthe slope associated with Cont5 when Cat4 = Farm is not different than 0 (p = 0.44). This means that there is no effect of Cont5 on Resp3 when Cat4 = Farm.\nthe slope associated with Cont5 when Cat4 = Urban is different than 0 (p = 0.0045) and positive (0.012). This means that Resp3 increases with Cont5 when Cat4 = Urban.\nthe slope associated with Cont5 when Cat4 = Wild is different than 0 (p &lt; 0.0001) and positive (0.028). This means that Resp3 increases with Cont5 when Cat4 = Wild.\n\n\n\nAre the coefficient estimates different than one another across categorical levels?\n\nAnd as with the other examples, you can find which coefficients are significantly different from eachother across the levels of your categorical covariate. A rule here is to always check for differences in the coefficients associated with your continuous covariate(s) first as these differences are slope differences. If you have a difference in slope it is very likely you have intercept differences because the lines are not parallel.\n\nYou can also find out which slopes (i.e. the effect of Cont5 on Resp3) are different across the levels of Cat4 using emtrends() with a request for pairwise testing:\n\nforCompSlope &lt;- emtrends(bestMod, # your model\n                         pairwise ~ Cat4, # your categorical covariate, with a request for pairwise testing\n                         var = \"Cont5\") # your continuous covariate\n\nforCompSlope\n\n$emtrends\n Cat4  Cont5.trend      SE df lower.CL upper.CL\n Farm     -0.00293 0.00374 94 -0.01036   0.0045\n Urban     0.01219 0.00418 94  0.00388   0.0205\n Wild      0.02793 0.00492 94  0.01815   0.0377\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast     estimate      SE df t.ratio p.value\n Farm - Urban  -0.0151 0.00561 94  -2.694  0.0226\n Farm - Wild   -0.0309 0.00618 94  -4.991  &lt;.0001\n Urban - Wild  -0.0157 0.00646 94  -2.437  0.0437\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nFrom $contrasts you see that all slope estimates for all levels of Cat4 are significantly different from one another (0.0001 ≤ p ≤ 0.044).\nNote that you can visualize these differences in slopes with:\n\nplot(forCompSlope,\n     comparisons = TRUE)\n\n\n\n\n\n\n\n\nIf slopes were similar, you will want to test if the levels of your categorical covariate (Cat4) have significantly different coefficients (intercepts) from each other with emmeans():\n\nforCompInt &lt;- emmeans(object = bestMod3, # your model\n            specs = pairwise ~ Cat4 + Cont5 + Cat4:Cont5, # your covariates, with the request for a pairwise test\n            type = \"response\") # report coefficients on the response scale\n\nforCompInt\n\n$emmeans\n Cat4  Cont5 emmean    SE df lower.CL upper.CL\n Farm    510   96.9 0.458 94     95.9     97.8\n Urban   510  104.3 0.421 94    103.5    105.2\n Wild    510  111.7 0.511 94    110.7    112.7\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                               estimate    SE df t.ratio p.value\n Farm Cont5509.767 - Urban Cont5509.767    -7.47 0.622 94 -12.014  &lt;.0001\n Farm Cont5509.767 - Wild Cont5509.767    -14.86 0.686 94 -21.667  &lt;.0001\n Urban Cont5509.767 - Wild Cont5509.767    -7.39 0.662 94 -11.175  &lt;.0001\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nplot(forCompInt,\n     comparisons = TRUE)\n\n\n\n\n\n\n\n\nFrom the bottom table ($contrasts), you can see that the intercept (modelled Resp2) modelled for each level in Cat4 is significantly different than every other level (all p-values are &lt; 0.0001).\n\n\n\n\nPlotting modelled effects\nYou can visualize your results (model fit, uncertainty and observations) for Example 3 with (the code is different than the other examples as you have a continuous covariate now):\n\n# Set up your covariates for the visualized fit\nforCat4&lt;-unique(myDat3$Cat4) # every level of your categorical covariate\nforCont5&lt;-seq(from = min(myDat3$Cont5), to = max(myDat3$Cont5), by = 1)# a sequence of numbers in your continuous covariate range\n  \n# create a data frame with your covariates\nforVis&lt;-expand.grid(Cat4 = forCat4, Cont5 = forCont5) # expand.grid() function makes sure you have all combinations of covariates\n\n# Get your model fit estimates at each value of your covariates\nmodFit&lt;-predict(bestMod3, # the model\n                newdata = forVis, # the covariate values\n                type = \"response\", # make the predictions on the response scale\n                se.fit = TRUE) # include uncertainty estimate\n\nforVis$Fit&lt;-modFit$fit # add your fit to the data frame\nforVis$Upper&lt;-modFit$fit+modFit$se.fit # add your uncertainty to the data frame\nforVis$Lower&lt;-modFit$fit-modFit$se.fit # add your uncertainty to the data frame\n\nlibrary(ggplot2) # load ggplot2 library\n\nggplot() + # start ggplot\n  \n  geom_point(data = myDat3, # add observations to your plot\n             mapping = aes(x = Cont5, y = Resp3, col = Cat4)) + # control position of data points so they are easier to see on the plot\n  \n  geom_line(data = forVis, # add the modelled fit to your plot\n              mapping = aes(x = Cont5, y = Fit, col = Cat4),\n              size = 1.2) + # control thickness of line\n  \n    geom_line(data = forVis, # add uncertainty to your plot (upper line)\n              mapping = aes(x = Cont5, y = Upper, col = Cat4),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n      geom_line(data = forVis, # add uncertainty to your plot (lower line)\n              mapping = aes(x = Cont5, y = Lower, col = Cat4),\n              size = 0.8, # control thickness of line\n              linetype = 2) + # control style of line\n  \n  ylab(\"Resp3, (units)\") + # change y-axis label\n  \n  xlab(\"Cont5, (units)\") + # change x-axis label\n  \n  labs(col=\"Cat4, (units)\") + # change legend title\n  \n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nAs you saw in the previous section, the slope describing the effect of Cont5 on Resp3 when Cat4 is Farm was not significantly different than 0 (p = 0.44). Usually you would then not plot this line when plotting effects to avoid misinterpretation, but I’ll leave it on the plot for now.\n\n\nReporting how well your model explains your response - Example 3\nAs with the previous examples, you can report the explained deviance as the Likelihood Ratio \\(R^2\\):\n\nr.squaredLR(bestMod3)\n\n[1] 0.855657\nattr(,\"adj.r.squared\")\n[1] 0.8567834\n\n\nSo 86 % of the deviance in Resp3 is explained by variation in Cat4, Cont5 and the interaction between the two covariates.\n\n\nReporting covariate importance - Example 3\nAs with the previous examples, you can report the importance of each model term in explaining deviance in the response with the sum of Akaike weights function (sw() in the MuMIn package):\n\ndredgeOut3\n\nGlobal model call: glm(formula = Resp3 ~ Cat4 + Cont5 + Cat4:Cont5, family = gaussian(link = \"identity\"), \n    data = myDat3)\n---\nModel selection table \n   (Int) Ct4       Cn5 Ct4:Cn5    R^2 df   logLik  AICc  delta weight\n8  98.35   + -0.002931       + 0.8557  7 -234.930 485.1   0.00      1\n4  92.25   +  0.009650         0.8166  5 -246.915 504.5  19.39      0\n2  96.93   +                   0.7923  4 -253.133 514.7  29.61      0\n3  94.92      0.017780         0.0848  3 -327.278 660.8 175.73      0\n1 104.00                       0.0000  2 -331.708 667.5 182.46      0\nModels ranked by AICc(x) \n\nsw(dredgeOut3)\n\n                     Cat4 Cont5 Cat4:Cont5\nSum of weights:      1    1     1         \nN containing models: 3    3     1         \n\n\nThis tells you that Cat4, Cont5 appear in all models with any weight, indicating all terms have equal importance in explaining the deviance of the response.\n\n\n\nPredicting with Example 3\nUnlike with the other examples in this week’s notes, with Example 3, the presence of a continuous covariate (Cont5) means you can make predictions with your model - i.e. you can estimate what Resp3 might be if Cont5 was higher or lower. Because you also have a categorical covariate (Cat4) in your model, you do need to specify a level for Cat4 when you make your prediction.\nRecall from Week 11 that we used the predict() function to make our predictions. For example, you can estimate the predicted Resp3 when Cont5 = 350 units and Cat4 = Urban with:\n\npredict(object = bestMod, # your model\n        newdata = data.frame(Cont5 = 350, Cat4 = \"Urban\"), # the values of the covariates at which to make the prediction\n        type = \"response\", # to make the prediction on the response scale.\n        se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\n$fit\n       1 \n102.3757 \n\n$se.fit\n[1] 0.822138\n\n$residual.scale\n[1] 2.6151\n\n\nSo the predicted Resp3 when Cont5 = 350 units and Cat4 = Urban is 102.38 ± 0.82.\nTo get the predicted Resp3 when Cont5 = 1200 units when Cat4 is Wild, use:\n\npredict(object = bestMod, # your model\n        newdata = data.frame(Cont5 = 1200, Cat4 = \"Wild\"), # the values of the covariates at which to make the prediction\n        type = \"response\", # to make the prediction on the response scale.\n        se.fit = TRUE) # to include a measure of uncertainty around the prediction\n\n$fit\n       1 \n130.9924 \n\n$se.fit\n[1] 3.349379\n\n$residual.scale\n[1] 2.6151\n\n\nwhich shows the predicted Resp3 when Cont5 = 1200 units and Cat4 = Wild is 130.99 ± 3.35."
  },
  {
    "objectID": "DSPPH_SM_Reporting.html#footnotes",
    "href": "DSPPH_SM_Reporting.html#footnotes",
    "title": "From statistical modelling to scientific report writing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthis is called the Nagelkerke’s modified statistic - see ?r.squaredLR for more information↩︎\nsee also the course notes from Model Validation↩︎\ni.e. in the dredge() output↩︎\nyou will be using the predict() function again in an upcoming chapter… when you predict!↩︎\nnote that the p-values reported in the coefficient table are the result of a test of whether the coefficient associated with Sp2 is different than that of Sp1 (p = 0.001), and if the effect of Sp3 is different than that of Sp1 (p = 0.017)), but a comparison of effects of Sp2 vs. Sp3 is missing. We will discuss this more further along in these notes.↩︎\nemmeans stands for “estimated marginal means”↩︎\nnote that the summary() output becomes easier to read if you force the starting model not to have an intercept with: startMod&lt;-glm(formula = Resp1 ~ Cat1 - 1, data = myDat, family = gaussian(link=\"identity\")). This is fine to do here where you only have categorical predictors but causes problems when you start having continuous predictors in your model as well. So we won’t be practicing this in class and instead will use the very flexible and helpful emmeans package to help us report coefficients from our statistical models.↩︎\n“post hoc” is a Latin phrase meaning “after the event”↩︎\nwhen the P-value is very low, R reports is as simple &lt;.0001↩︎\nnote that now you get to assess the difference between coefficients for Sp2 and Sp3 which was missing in the summary() output above↩︎\nThe emmeans package is very flexible and has a lot of options as to how to make these corrections depending on your needs. Plenty more information is available here↩︎\n* in units of Resp3 per units of Cont4↩︎\nunits of Resp4↩︎\nunits will be the units of Resp4 divided by those of Cont6↩︎\nin units of Resp4 per units of Cont6↩︎\nin units of Resp4 per units of Cont6↩︎\nin units of Resp4 per units of Cont6↩︎\nin units of Resp4 per units of Cont6↩︎\nalso called the incidence rate ratio, incidence density ratio or relative risk↩︎\na reminder that the number e that the function exp() uses as its base is Euler’s number. It (along with the natural logarithm, log()) is used to model exponential growth or decay, and can be used here when you want models where the response can not be below zero.↩︎\nthis is what R uses when you say link = \"log\"↩︎\nfrom the emmeans package↩︎\nremember, the coefficients for categorical predictors are also being converted from the link to the response scale. It is just that R is doing it for us in the emmeans() function when we add the argument `type = “response”)↩︎\nfrom the emmeans package↩︎\ntaken from the emmOut output above↩︎\nthe mean of the range of Cont6↩︎\nthe mean of the range of Cont6↩︎\nthe mean of the range of Cont6↩︎\ni.e. effects associated with the continuous predictor↩︎\nNotice I write “response(s)” in the title of this section - plural. It is possible to have multiple response variables and we’ll discuss this in class. For the focus of this course though, we’ll be working with one response variable↩︎\nhere, I’ll write “citation” where you would support your statements with the existing literature↩︎\nfind this citation with citation(\"ggplot2\")↩︎\nyou can find which version of R you are using with citation() (nothing inside the parentheses)↩︎\nyou can find the citation for the car package with citation(\"car\"). Remember to cite every package you use in R.↩︎\nsee readme file above↩︎\nfind this with citation(\"DHARMa\")↩︎\nAs a reminder, in Week 9 we also discussed testing your hypothesis via p-values, as well as the limitations of this method when you have more than one covariate↩︎\nAICc is the corrected Akaike Information Criterion which is more conservative than traditional AIC, i.e. models with more covariates need to increase the explained deviance quite a bit before the AICc metric improves↩︎\nnote that you can also include tables directly from R using the gt package↩︎\nwatch your number of significant units here. Make sure they make sense for the type of measurement. And be consistent↩︎\nhere equivalent to the traditional R2 as we have a normal error distribution assumption and linear shape assumption. The traditional R2 is found with 1-summary(bestMod)$deviance/summary(bestMod)$null.deviance↩︎\nwe’ll go over this in class today, but see also your course notes from week 9↩︎\nmore to come!↩︎\nwe will be using the predict() function again… when we predict!↩︎\nsee [https://plotly.com/r/getting-started/] if you are interested in learning more about the plotly package↩︎\nsee also the course notes from Week 9↩︎\ni.e. in the dredge() output↩︎\nnote that the p-values reported in the coefficient table are the result of a test of whether the coefficient associated with Sp2 is different than that of Sp1 (p = 0.054), and if the effect of Sp3 is different than that of Sp1 (p = 0.0072), but a comparison of effects of Sp2 vs. Sp3 is missing. We’ll discuss this more further along in these notes.↩︎\nnote that the summary() output becomes easier to read if you force the starting model not to have an intercept with: startMod&lt;-glm(formula = Resp1 ~ Cat1-1, data = myDat, family = gaussian(link=\"identity\")). This is fine to do here where you only have categorical covariates but causes problems when you start having continuous covariates in your model as well. So we won’t be practicing this in class and instead will use the very flexible and helpful emmeans package to help us report coefficients from our statistical models.↩︎\nhere I say that your covariate “has an effect” on your response. Your covariate being in your best-specified model just means that there is evidence that variation in your covariate explains variation in your response. It’s up to you to think about the mechanisms underlying this relationship - i.e. is it causation or just correlation?↩︎\nsee section 11.6 in Crawley for more on multiple comparisons↩︎\nwhen the p-value is very low, R reports is as simple &lt;.0001↩︎\nnote that now we get to assess the difference between coefficients for Sp2 and Sp3 which was missing in the summary() output above↩︎\nThe emmeans package is very flexible and has a lot of options as to how to make these corrections depending on your needs. Plenty more information is available here↩︎\nsee Week 11’s course notes for more on this↩︎\nunits of Resp3↩︎\nunits will be the units of Resp3 divided by those of Cont5↩︎\nalso called the incidence rate ratio, incidence density ratio or relative risk↩︎"
  },
  {
    "objectID": "DSPPH_DA_Merge.html",
    "href": "DSPPH_DA_Merge.html",
    "title": "So you want to: merge your data sets",
    "section": "",
    "text": "Often times we are interested in exploring connections among variables from different sources. Merging your data files can be a way of collecting all the variables of interest so you can explore research questions and hypotheses about the data.\nIn addition, merging can be used to label your observations. We go through examples below."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "href": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "title": "So you want to: merge your data sets",
    "section": "in the base package using merge()",
    "text": "in the base package using merge()\n\nmDat&lt;-merge(Dat1, # data frame to merge\n             Dat2, # other data frame to merge\n             by = c(\"ID\", \"Day\")) # merge by variable(s)\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n\n\nNote that you don’t need to have the same number of observations (rows) in your two data frames to merge. Merging can be a great way of labelling your data. Here’s an example:\nConsider a data frame with strain information for each of your organism IDs:\n\n## ID: organism ID\n## Strain: strain of organism\n\nstr(Dat3)\n\n'data.frame':   20 obs. of  2 variables:\n $ ID    : chr  \"id24\" \"id30\" \"id33\" \"id26\" ...\n $ Strain: chr  \"A\" \"C\" \"B\" \"C\" ...\n\n\nNote that Dat1 and Dat2 each contained 40 observations - one observation for each of 20 IDs made on each of 2 days.\nIn contrast Dat3 only has 20 observations - information about the strain for each of 20 IDs.\nBy using merge(), we can add the strain information to mDat:\n\nallDat &lt;- merge(mDat, # one data frame\n                Dat3, # the other data frame\n                by = \"ID\") # variables to merge by\n\nstr(allDat)\n\n'data.frame':   40 obs. of  5 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n $ Strain: chr  \"C\" \"C\" \"A\" \"A\" ...\n\n\nNow we have our observations labelled by the strain information!\nSome things to note:\n\nif you leave out the by = function totally, R will look for column names that are similar between the two data frames and use that for the merge.\nyou can designate that the “merge by” variables have different column names in the two data frames. This is done with the by.x = and by.y = arguments. Check ?merge for more.\nyou can control what happens to unmatched columns (e.g. if an ID appeared in only one of the two data frames). This is done with the all =, all.x =, and all.y = arguments. Check ?merge for more."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "href": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "title": "So you want to: merge your data sets",
    "section": "in the dplyr package using full_join()",
    "text": "in the dplyr package using full_join()\nThe dplyr package includes the full_join() function as another way to merge your data frames\n\nlibrary(dplyr) # load dplyr package\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmDat&lt;-full_join(Dat1, # data frame to merge\n                Dat2, # other data frame to merge\n                by = join_by(ID, Day)) # merge by column\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id29\" \"id30\" \"id22\" \"id31\" ...\n $ Day   : num  1 2 1 2 1 1 2 1 2 2 ...\n $ Length: num  150 164 110 134 155 125 144 144 139 167 ...\n $ Temp  : num  7.4 7.3 6.9 7.5 5.6 7.8 7.8 6.3 7.9 7.1 ...\n\n\nSome things to note:\n\nThe full_join() function keeps all observations appearing in either data frame.\nThe left_join() function keeps all observations in the first data frame (Dat1) but you will lose any unmatched observations in the second data frame (Dat2).\nThe right_join() function keeps all observations in the second data frame (Dat2) but you will lose any unmatched observations in the first data frame (Dat1)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.\n\n\n\nThe Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions\n\n\n\n\n\n\n\nModules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#purpose",
    "href": "intro.html#purpose",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#interested-in-contributing-to-the-dsp-program",
    "href": "intro.html#interested-in-contributing-to-the-dsp-program",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "Modules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "handbookIntro.html",
    "href": "handbookIntro.html",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.\n\n\n\nThere are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.\n\n\n\nThe ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is.",
    "href": "handbookIntro.html#what-this-handbook-is.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is-not.",
    "href": "handbookIntro.html#what-this-handbook-is-not.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "There are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#dsp-program-tools",
    "href": "handbookIntro.html#dsp-program-tools",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#footnotes",
    "href": "handbookIntro.html#footnotes",
    "title": "Introduction to the Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComputational thinking includes skills in decomposition (breaking down tasks into small steps), pattern recognition (observing patterns in tasks and data), abstraction (identifying and extracting relevant information, ignoring or removing unnecessary information), algorithms (creating an ordered set of instructions for solving a problem), modelling and simulation (statistical modelling for hypothesis testing, imitating processes and problems), and evaluation (determining the effectiveness of a solution, generalizing to apply the solution to a new problem) - adapted from digitalcareers.csiro.au.↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html",
    "href": "DSPPH_SM_HypothesisTesting.html",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "",
    "text": "In this section you will learn:\n\n\n\n\n\n\nhow statistical models can be used to test your hypothesis by judging the evidence for your model\nabout methods to judge the evidence for your model\nto use the model selection method to judge the evidence for your model and test your hypothesis"
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#what-is-a-p-value",
    "href": "DSPPH_SM_HypothesisTesting.html#what-is-a-p-value",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "What is a P-value",
    "text": "What is a P-value\nThe P-value is used for null-hypothesis significance testing (NHST) (@Muff2022). The “P” in P-value stands for probability - the probability of observing an outcome given that the null hypothesis is true (@Muff2022; @Popovic2024). Remember that null-hypothesis tests assume that the tested effect is zero. In the case of hypothesis testing, the null hypothesis test assumes a coefficient describing the effect of a predictor on your response is zero.\nIn the case of hypothesis testing, the null hypothesis you are testing against is that a predictor’s coefficient is zero. So, the P-value associated with the hypothesis testing tells you the probability of getting a coefficient of the value you got even though the coefficient is in fact zero.\nWhen the P-value is very low, we say that there is evidence that the coefficient is not zero, i.e. evidence that your predictor has an effect on your response. By convention, we say a P-value is low if P &lt; 0.05; meaning that the evidence comes with a 5% probability that the coefficient is actually zero.\nFirst, let’s describe how this works in general, and then look at an example:\nTo determine a P-value associated with a model coefficient, the null-hypothesis testing estimates something called a test statistic based on the coefficient’s estimate and the error around it. This test statistic is assumed to come from a certain data distribution (the exact distribution will vary based on your model structure)\nLet’s look at an example using our model fit to the hypothesis WtChange ~ Prey + 1. By using summary() on our model, we get\n\nsummary(startMod) # look at our validated starting model\n\n\nCall:\nglm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.85062  -0.23669  -0.04888   0.25828   0.83052  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.996353   0.158745  -44.07   &lt;2e-16 ***\nPrey         0.079912   0.002557   31.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1414749)\n\n    Null deviance: 147.8242  on 69  degrees of freedom\nResidual deviance:   9.6203  on 68  degrees of freedom\nAIC: 65.728\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe coefficients table shows us that the Intercept was estimated as -7 ± 0.16 g and the slope associated with Prey10 is 0.08 ± 0.0026 \\(g \\cdot m^{3}\\cdot num^{-1}\\).\nFor each coefficient, you can see t-statistic (called t value in the table) and P-value (called Pr(&gt;|t\\) in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g. for the intercept in the example, -7/0.16 = -44.07). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the P-value. When P-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero11, and that the predictor associated with the coefficient can be included in our model (i.e. the predictor is explaining a significant amount of our response variability).\nHow to estimate P-values for your model\nThe output from the summary() function quickly becomes limiting when you have more than one predictor. Instead, you can use the anova() function to estimate the P-values associated with each model term.\nHere’s an example for our model testing WtChange \\(\\sim Prey + 1\\):\n\nanova(startMod, # model object\n      test = \"F\") # type of null hypothesis test to perform\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: WtChange\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev      F    Pr(&gt;F)    \nNULL                    69     147.82                     \nPrey  1    138.2        68       9.62 976.88 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote here that you need to indicate what type of null hypothesis testing you want:\n\nuse the F-test for error distribution assumptions like normal or gamma (i.e. distributions where the scale parameter is estimated)\nuse the Chi-square test for error distribution assumptions like poisson or binomial (i.e. distributions where the scale parameter is fixed)\n\nThe result is a table where each predictor has a row to report the results of the null hypothesis test. Here we see that there is strong evidence the coefficient associated with Prey is not zero (P &lt; \\(2.2 \\cdot 10^{-16}\\)).\nTwo more notes about using P-values:\n\nnote in the table above that it says “Terms added sequentially (first to last)”. This indicates that the coefficients of the predictors are tested by adding each predictor one at a time to the model, estimating the coefficient associated with the predictor, and testing the null hypothesis that the coefficient is not different than zero. This process is problematic when you have even a moderate amount of predictor collinearity. This is a big reason to prefer the model selection method of hypothesis testing that we outline below.\nBecause of the issues interpreting P-values, it is better to talk about what P-values tell you about the evidence for your hypothesis, rather than a strict idea of rejecting or not your hypothesis. Here is an illustration of how to interpret your P-values12:\n\n\n(from @Muff2022)"
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#limitations-of-p-values",
    "href": "DSPPH_SM_HypothesisTesting.html#limitations-of-p-values",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Limitations of P-values",
    "text": "Limitations of P-values\nAs mentioned in the previous section, problem with the P-value method of testing your research hypothesis comes when you have more than one predictor in your hypothesis. Correlation among your predictors13 means that it is difficult to trust your coefficient estimates. This means that you can not use the P-values as a way to determine which coefficients are significantly different than zero when you have correlated predictors. Said another way, your assessment of whether a predictor is useful in your model will be uncertain if you have correlated predictors. And correlated predictors are very common. For this reason, we will be hypothesis testing using model selection."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#what-is-model-selection",
    "href": "DSPPH_SM_HypothesisTesting.html#what-is-model-selection",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "What is model selection",
    "text": "What is model selection\nCompare the following two models:\n\nWtChange \\(\\sim \\beta_1\\cdot Prey + \\beta_0 + error\\)\nWtChange \\(\\sim \\beta_0 + error\\)\n\nNote that model 2 is obtained by making the coefficient of Prey (\\(\\beta_1\\)) equal to 0 (i.e. if the effect of Prey on WtChange was zero). If you determined which of these two models better fits your data, you will know if \\(\\beta_1\\) is likely to be 0 and, thus, whether or not you have evidence that Prey can explain variation in WtChange."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#how-do-you-use-hypothesis-testing-for-model-selection",
    "href": "DSPPH_SM_HypothesisTesting.html#how-do-you-use-hypothesis-testing-for-model-selection",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "How do you use hypothesis testing for model selection",
    "text": "How do you use hypothesis testing for model selection\nThe steps involved in testing your hypothesis using model selection is\n\nform your candidate model set\nfit and rank models in your candidate model set\nchoose your best-specified model(s)\n\nLet’s walk through these now.\n\nForm your candidate model set\nYour candidate model set contains models with all possible predictor combinations14. So the candidate model set for WtChange \\(\\sim Prey + 1\\) is:\nWtChange \\(\\sim Prey + 1\\)\nWtChange \\(\\sim 1\\)\n\n\n\n\n\n\nAnother example\n\n\n\n\n\nHere is another example:\nif your hypothesis is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\\(Resp \\sim Cov1 + Cov2 + 1\\)\n\\(Resp \\sim Cov1 + 1\\)\n\\(Resp \\sim Cov2 + 1\\)\n\\(Resp \\sim 1\\)\nNote that the more predictors you have in your model, the bigger your candidate model set.\n\n\n\nHopefully you are starting to see that the difference among models in the candidate model set can be described by setting the coefficient associated with a predictor to zero. In this way, fitting and comparing the models in your candidate model set is a way of assessing the evidence for your hypothesis. This method is more robust to issues like predictor collinearity because you are assessing the evidence for a predictor’s effect on your response when each predictor is in a model alone and when it is in a model with other predictors.\nOne last note about your candidate model set: you must remember the biology when you form your candidate model set. There may be a biological reason why a certain model must not be included in your candidate model set (i.e. a model that defies biological logic). These should be excluded from your candidate model set. (@BurnhamAnderson2002).\n\n\nFit and rank models in your candidate model set\nEach model in the candidate model set is graded based on an estimate of the model’s “cost” vs. its “benefit”.\nThe model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) 15.\nThe model’s benefit is how well the model fits your data - i.e. how much of the variability in your response the model explains. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.16\n\n\n\n\n\n\nThe Principle of Parsimony\n\n\n\n\n\nThe principle of parsimony means that, when in doubt, you will choose the simpler explanation. This means that:\n\nmodels should have as few parameters as possible\nlinear models are preferred to non-linear models\nmodels with fewer assumptions are better\n\nThis said, there are times when you might choose a more complicated explanation over a simpler explanation. One example of this is when you prioritize a model’s ability to predict a future response vs. getting an accurate understanding of the underlying mechanisms. We will discuss this more in the upcoming section on Prediction.\n\n\n\nYou can fit and rank your models quickly using a function called dredge() in the MuMIn package17. The dredge() function fits and ranks models representing all possible predictor combinations based on your starting model - i.e. your default candidate model set. The output is a table ranking the models in your candidate model set.\nLet’s explore this now.\n\nlibrary(MuMIn) # load MuMIn package\n\noptions(na.action = \"na.fail\") # to avoid illegal model fitting\n\ndredgeOut &lt;- dredge(startMod) # create model selection table for validated starting model\n\nprint(dredgeOut)\n\nGlobal model call: glm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n---\nModel selection table \n  (Intrc)    Prey df   logLik  AICc  delta weight\n2  -6.996 0.07991  3  -29.864  66.1   0.00      1\n1  -2.238          2 -125.489 255.2 189.07      0\nModels ranked by AICc(x) \n\n\nNote the line:\n\noptions(na.action = \"na.fail\") # to avoid illegal model fitting\n\nThis is included because you need to make sure the data used to fit every model in your candidate model set stays the same. This could be violated if you have missing values in some of your predictor columns. This options() statement makes sure your model selection is following the rules.\nThe output of the dredge() function gives us\n\nthe Global model call (our original hypothesis), and\na Model selection table\n\nThe Model selection table contains one row for each model in our candidate model set. Let’s explore this now:\nFind the column called “(Intrc)”. This column tells us when the intercept is included in the model. If there is a number in that column, the associated model in your candidate model set (row) contains an intercept. Note that by default all models will contain an intercept18.\nFind the column called “Prey”. This column tells us when the Prey predictor is in the model. Notice the first row contains a number in the Prey column, while the second row is blank. This means that Prey is a predictor in the model reported in the first row but is missing from the model in the second. Note also that a number is recorded in the Prey column, row 1 (0.0799). This is the coefficient associated with the Prey predictor. Since we have a normal error distribution assumption, this coefficient can be considered the slope of a line19. If the predictor was a categorical predictor (vs. continuous predictor), a “+” would appear in the Model selection table when the categorical predictor was in the model.\nSo, in our example above, the model in the first row contains an intercept and the Prey - i.e. the first row is the model WtChange ~ Prey + 1. The model in the second row contains only an intercept - i.e. the second row is the model WtChange ~ 1.\nThe rest of the columns in the Model selection table contain information that help us rank the models.\n\nthe “df” column reports the number of model coefficients. Models that are more complicated (e.g. more predictors) will have a higher df as they require more coefficients to fit. Models with more terms are more “costly”. In the first row (WtChange ~ Prey + 1), df is 3 because the model fitting estimates a coefficient for Prey, for the intercept, and for the normal error distribution assumption (standard deviation). In the second row (WtChange ~ 1), df is 2 because the model fitting estimates a coefficient for the intercept, and for the normal error distribution assumption (standard deviation). So the model in the first row is more costly than the second row.\nthe “logLik” column reports the log-Likelihood of the model fit. The absolute value of this estimate will depend on the type of data you are modelling, but in general, the logLik is related to how much variation in your response the model explains. It can be used to compare models fit to the same data. This can be seen as a measure of the “benefit” of the model.\nthe “AICc” column reports information criteria for your models. Information criteria balances the cost (complexity) and benefit (explained variation) for your model. An example of information criterion is the Akaike Information Criterion (AIC). The AIC is estimated as:\n\n\\(AIC = 2\\cdot k - 2 \\cdot ln(L)\\)\nwhere \\(k\\) is the cost of the model (number of coefficients, like df above), and \\(L\\) is the maximum likelihood estimate made when the model was fit.\nThere are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes). The AICc is reported by default here, but you can control that in the dredge() function. In all cases, lower information criterion means more support for the model.\n\nthe “delta” (\\(\\Delta\\)) column is a convenient way to see how different each model’s AICc is from the model with the lowest AICc (\\(\\Delta AIC_i\\) is the change in AIC for model i vs. the model with the lowest AIC.)\nthe “weight” column reports Akaike weights for the model. The Akaike weights are a measure of the relative likelihood of the models. The sum of all the Akaike weights is 1, so we can get a relative estimate for the support for each model.\n\n\n\n\n\n\n\nAkaike weights\n\n\n\n\n\nHere is the equation to estimate the Akaike weights:\n\\[\nw_i = \\frac{exp(-\\frac{1}{2} \\cdot \\Delta AIC_i)}{\\sum_{r=1}^{R}exp(-\\frac{1}{2} \\cdot \\Delta AIC_r)}\n\\]\nwhere\n\n\\(w_i\\) is the Akaike weight for model i,\n\\(\\Delta AIC_i\\) is the change in AIC for model i vs. the model with the lowest AIC\n\\(\\Delta AIC_r\\) is the change in AIC for model r vs. the model with the lowest AIC. This is estimated for all models in the candidate model set (R models).\n\n@BurnhamAnderson2002\n\n\n\n\n\nChoose your best-specified model(s)\nUsing the model selection table, you can choose your best-specified model(s) and find out what it tells you about your hypothesis.\nIn general, your best-specified model will be the model with the lowest information criterion (e.g. AIC)20. This will be the model at the top of the model selection table.\nThat said, notice I write “best-specified model(s)” - possibly plural. This is because you might have models where the AIC estimates are very close to one another. A good rule of thumb is to report all models where the AIC is within 2 of the lowest AIC model (i.e. delta &lt; 2). Following @BurnhamAnderson2002,\n\n\n\nfor models where delta is\nthere is … for the model\n\n\n\n\n0-2\nsubstantial support\n\n\n4-7\nconsiderably less support\n\n\n&gt; 10\nessentially no support\n\n\n\nWith our example above:\n\nprint(dredgeOut)\n\nGlobal model call: glm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n---\nModel selection table \n  (Intrc)    Prey df   logLik  AICc  delta weight\n2  -6.996 0.07991  3  -29.864  66.1   0.00      1\n1  -2.238          2 -125.489 255.2 189.07      0\nModels ranked by AICc(x) \n\n\nwe have one best-specified model (model with substantial support):\nWtChange ~ Prey + 1 (AICc = 66.1)\nand essentially no support for the null model:\nWtChange ~ 1 (AICc = 255.2; delta = 189.1)\nWe can conclude that there is evidence that Prey explains variability in WtChange.\n\n\nWhat does your best-specified model(s) say about your hypothesis?\nModel selection is a way of hypothesis testing. So what does your best-specified model say about your hypothesis? By comparing your best-specified model to your starting model, you can see where there is evidence for the effects of each predictor, and where the effects are estimated to be zero.\nAs our best-specified model is\nWtChange ~ Prey + 1\nWe can conclude that there is evidence that Prey explains variability in WtChange.\n\n\n\n\n\n\nMore examples\n\n\n\n\n\nif our starting hypothesis was:\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\nA best-specified model of \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\) would indicate that we have evidence that there are effects of Cov1 and Cov2 on Resp and that the effect of Cov1 on Resp depends on Cov2 (an interaction).\nA best-specified model of \\(Resp \\sim Cov1 + Cov2 + 1\\) would indicate that we have evidence that there are effects of Cov1 and Cov2 on Resp but no evidence of an interaction effect.\nA best-specified model of \\(Resp \\sim Cov1 + 1\\) would indicate that we have evidence that there is an effect of Cov1 but not Cov2 on Resp.\nA best-specified model of \\(Resp \\sim 1\\) (i.e. the null hypothesis) would indicate that we have no evidence for effects of Cov1 or Cov2 on Resp. This is also a valid scientific result!\n\n\n\n\nIn the next section (on Reporting), we will discuss further how to communicate what your hypothesis testing results say about your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-p-values-1",
    "href": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-p-values-1",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Hypothesis testing using p-values",
    "text": "Hypothesis testing using p-values\nYou’ll remember from last week that you can use summary() on your starting model object to see the coefficients that were fit, along with their uncertainty (standard error) in the coefficients and a p-value. Let’s take another look with an example:\n\nIn the example, we are trying to explain variability in WtChange (g) with Prey \\(num \\cdot m^{-3}\\) by fitting a model to the hypothesis WtChange ~ Prey + 121 with a normal error distribution assumption and linear shape assumption. The coefficients table shows us that the Intercept was estimated as -10.2 +/- 3.6 g and the slope associated with Prey is 0.12 +/- 0.04 \\(g \\cdot m^{3}\\cdot num{-1}\\).\nFor each coefficient, you can see t-statistic (called t value in the table) and p-value (called Pr(&gt;|t\\) in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g. for the intercept in the example, -10.2/3.6 = -2.8). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the p-value. When p-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero, and that the covariate associated with the coefficient can be included in our model (i.e. the covariate is explaining a significant amount of our response variability).\n\nLimitations of p-values\nA problem with this method of testing your research hypothesis comes when you have more than one covariate in your hypothesis. As with Assumption #1 above, any correlation among your covariates will mean that you can not trust your coefficient estimates. This means that you can’t use the p-values as a way to determine which coefficients are significant when you have correlated covariates. Said another way, your assessment of whether a covariate is useful in your model will change if you have correlated covariates. And correlated covariates are very common."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-model-selection-1",
    "href": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-model-selection-1",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Hypothesis testing using model selection",
    "text": "Hypothesis testing using model selection\nAs mentioned above, an alternative method of testing your hypothesis is through model selection. Compare the following two models:\n\nWtChange \\(\\sim \\beta_1\\cdot Prey + \\beta_0 + error\\)\nWtChange \\(\\sim \\beta_0 + error\\)\n\nNote that model 2 is obtained by making \\(\\beta_1 = 0\\). If you determined which of these two models better fits our data, you will know if \\(\\beta_1\\) is likely to be 0 and, thus, whether or not Prey can explain variation in your response.\n\nThe candidate model set\nWe expand this idea from two models to all models in your “candidate model set”. This set are the models representing all possible covariate combinations. So if your hypothesis is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\\(Resp \\sim Cov1 + Cov2 + 1\\)\n\\(Resp \\sim Cov1 + 1\\)\n\\(Resp \\sim Cov2 + 1\\)\n\\(Resp \\sim 1\\)\nModel selection for hypothesis testing therefore involves:\n\nFirst, each model in your candidate model set is fit to your data.\nThen, each model is “graded” based on how well it is fit to the data and how complicated it is (more below).\nFinally, the models are ranked and you can choose the best-specified model.\n\n\n\nInformation criterion\nEach model in the candidate model set is graded based on an estimate of the model’s “cost” vs. “benefit”. The model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) 22, and the benefit is how well the model fits your data. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.\nThe cost-benefit information is combined to give each model a grade through a metric called “Information Criterion”. For example, Akaike Information Criterion23 is estimated as:\n\\(AIC = 2\\cdot k - 2 \\cdot ln(L)\\)\nwhere \\(k\\) is the cost of the model (number of parameters), and \\(L\\) is the maximum likelihood estimate made when the model was fit.24\nIn all cases, the model with the lowest AIC is our “best-specified” model, though we will discuss what should be done if you have two or more models that are equally “good” (i.e. within 2 AIC of the lowest AIC). Note that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.\n\n\nChoosing you best-specified model\n\nFor example, if your starting model is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is:\n1: \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n2: \\(Resp \\sim Cov1 + Cov2 + 1\\)\n3: \\(Resp \\sim Cov1 + 1\\)\n4: \\(Resp \\sim Cov2 + 1\\)\n5: \\(Resp \\sim 1\\).\nYou will fit each of these models and estimate their AIC, for example:\n1: \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\) (AIC = 60)\n2: \\(Resp \\sim Cov1 + Cov2 + 1\\) (AIC = 78)\n3: \\(Resp \\sim Cov1 + 1\\) (AIC = 20)\n4: \\(Resp \\sim Cov2 + 1\\) (AIC = 140)\n5: \\(Resp \\sim 1\\) (AIC = 234).\nYou then use this information to pick your best-specified model as the model with the lowest AIC25. Your best-specified model tells you how to interpret the evidence for your hypothesis. With the example above, you would conclude that the 3rd model (\\(Resp \\sim Cov1 + 1\\)) is your best-specified model (it has by far the lowest AIC), indicating that \\(Cov1\\) explains variablity in \\(Resp\\), but that there is no evidence that \\(Cov2\\) explains variability in \\(Resp\\) (as \\(Cov2\\) doesn’t appear in your best-specified model).\nModel selection like this can be easily done using the dredge() function in the MuMIn package. We’ll practice this in class."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#footnotes",
    "href": "DSPPH_SM_HypothesisTesting.html#footnotes",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ninference is the conclusion you make based on reasoning and evidence↩︎\nvs. categorical↩︎\nif you’re confused why we are choosing these assumptions, read the course notes section on Starting Model↩︎\nthese estimates are called parameters↩︎\nnote that some write this as “P value” and some as “p value” and some as “p-value”. There is no one rule. Just pick one and make it consistent through your text. I’ll try to do that here.↩︎\nfield↩︎\nsee more in your notes on Data curation and collection↩︎\nSee your notes on Model Validation for more on predictor collinearity↩︎\nbecause it is generally applicable↩︎\ni.e. the effect of Prey on WtChange↩︎\nfor example, a P value of 0.006 means that there is a 0.6% chance we would estimate the effect of Prey to be 0.12 \\(g \\cdot m^{3}\\cdot num{-1}\\) when it was in fact 0↩︎\nwe’ll come back to this in the Reporting section↩︎\nSee your notes on Model Validation for more on predictor collinearity↩︎\nthese are also called “nested” models as each model is “nested” in one of the other models when it only differs by one predictor. “Nested” is also used in experimental design to mean something totally different, so we will avoid using the term here.↩︎\nsee “The Principle of Parsimony” below↩︎\nsee the section on Starting Model↩︎\nnote the spelling and capitalization of this package name!↩︎\nindeed, your null model only contains an intercept↩︎\nand it is the same number given in the summary() output above. More on this coming up in the Reporting section!↩︎\nNote that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.↩︎\nrecall that the + 1 is to indicate that there is an intercept in our model. The +1 can be left out of the model formula and R will still estimate an intercept, but I will write it here for clarity↩︎\nsee section 9.3 and the “Principle of Parsimony”↩︎\nSection 9.17↩︎\nThere are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes).↩︎\nmodels within 2 of the lowest AIC are all chosen as the best-specified model↩︎"
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Contacts and Feedback for the DSP Program",
    "section": "",
    "text": "The DSP Program is a joint venture by colleagues across AU’s Department of Biology.\nProgram development is led by the DSP Program Taskforce:\n\nAnna B. Neuheimer (Aquatic Biology, Taskforce head)\nRobert Buitenwerf (Ecoinformatics and Biodiversity)\nAlejandro Ordonez Gloria (Ecoinformatics and Biodiversity)\nTove Hedegaard Jørgensen (Genetics, Ecology and Evolution & Centre for Educational Development)\nIan Marshall (Microbiology)\nBirgit Olesen (Aquatic Biology & Arctic Research Centre)\nPeter Teglberg Madsen (Zoophysiology)\nJesper Givskov Sørensen (Genetics, Ecology and Evolution)\n\n\n\nSend your questions and feedback to the DSP Program Taskforce\nWe welcome your questions and feedback. Please submit them using this form or send an email to abneuheimer@bio.au.dk."
  },
  {
    "objectID": "handbookSMIntro.html",
    "href": "handbookSMIntro.html",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistics",
    "href": "handbookSMIntro.html#why-statistics",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistical-modelling",
    "href": "handbookSMIntro.html#why-statistical-modelling",
    "title": "Statistical Modelling Handbook",
    "section": "Why statistical modelling?",
    "text": "Why statistical modelling?\nYou can quantify how much variability you can explain with your research hypothesis through statistical modelling. Your statistical model represents your research hypothesis in a mathematical structure. This mathematical structure can be tested against your data to determine what evidence there is for your hypothesis:\n\ncan I explain the variability that I am seeing? (Can I reject my hypothesis?)\ngiven my hypothesis, how much variation in the observations can I explain?\ngiven my hypothesis, what would I expect (predict) to observe under different times or locations?\n\nYour job then is to explain observation variability in time and space by creating a “model” of what (you think) is going on - hence statistical modelling.\nIt is important to remember that any model is only an approximation of what is going on in the real world. As many have said before\n\nAll models are wrong but some are useful.\n\nWe will discuss how you can build useful models that you can use to test your hypotheses.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "href": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "title": "Statistical Modelling Handbook",
    "section": "Introducing a statistical modelling framework",
    "text": "Introducing a statistical modelling framework\n\nThe focus of this section of the handbook is statistics that can be applied to your work as a biologist. For that reason, the motivation for what we are going to do together comes directly from your biological research hypotheses. As a biologist, you have a research hypothesis that you want to test. The method presented in this handbook will help you test it. You will learn how to move from biological theory to hypothesis to the statistical modelling process you will use to test your hypothesis.\nYou will learn this process of statistical modelling by walking through a “Statistical Modelling Framework”. This is a set of steps that you can use to go from your research hypothesis to designing a model, testing your hypothesis and communicating the results.\nThis handbook will walk through the parts of this framework one by one. During DSP modules throughout your degree, you will do the same in class while you practice applying the framework to case studies. In this way, you should see how the framework is generally applicable but also flexible. And after you leave the course you will be able to apply the framework to help you in your statistical analysis in other courses, your thesis, and your future career - in every case, your process will take its starting point and focus from the hypothesis that is motivating you.\nAs we will discuss later in the handbook, working through this framework will also guide you in creating the “guts” of a paper or report. As well as clarifying how and why you made your analysis choices, it will guide you in describing your motivation behind your research question (why is it worthwhile to spend time explaining this variation?) as well as the mechanisms behind your research hypothesis (why do I think X is responsible for the variation I’m trying to explain?). Once through, you’ll have a solid draft that can be the basis of a report, thesis chapter or scientific paper. We will talk about how this works in section.\nA note about our Statistical Modelling Framework: The steps in the image to the right as a linear process but it is not actually a linear process. As you will see in the examples, sometimes you will need to make best guesses5 as to what model might be useful and only after confronting the model with your data will you know if your guesses were reasonable (and useful!) - i.e. the model is a valid one that you can use to test your hypothesis. We will talk about how to find a useful model, how to choose when there are multiple options, and how to communicate your choices.\n\nSteps in the statistical modelling framework:\n\n\nResponse(s)\nHere you will define your research question by identifying your response variable(s).\n\nWhat variability are you trying to explain?\nAnd why is it worth explaining? (your motivation)\n\nNote: we’ll begin by discussing how to model hypotheses with just one response variable before discussing multiple response variable(s).\n\n\nPredictor(s)\nHere you will choose your predictor variables.\n\nwhat could explain the variability in your response?\nwhat are the possible mechanisms behind your argument?\n\n\n\nHypothesis\nHere you will see how your response and predictor variables come together to define your research hypothesis. And we will discuss how to write this hypothesis to begin building your statistical modelling.\n\n\nStarting model\nHere you will choose and fit the starting model that will be used to test your hypothesis. You will do this by choosing and communicating two key assumptions that will help you pick a useful modelling starting point. Then you will fit your model to your data (i.e. confronting your model with your data).\n\n\nModel validation\nHere you will investigate whether your model will be a useful one to test your hypothesis. Your steps here will include considering if you have correlated predictors or problems with observation dependence.\nYou will also considering if your starting model assumptions were realistic. After this step, you will have a model that you can confidently use to test your hypothesis.\n\n\nHypothesis testing\nHere you will test your hypothesis by assessing the evidence supporting your model. We will discuss a number of different methods to do this, but will focus on the model selection method as a robust way to evaluate what your model is telling you about your hypothesis.\n\n\nReporting\nHere you will report the results of your hypothesis testing.\nYou will report:\n\nyour best-specified model identified in the hypothesis testing\nthe effects (patterns) described by your model (including visualizing your model effects)\nhow well your model explains variability in your response.\n\n\n\nPredicting\nHere you will use your model to make predictions of your response under different conditions (while considering prediction limits).\n\n\n\nWhere we will begin: generalized linear models (GLMs)\nTo begin with, we will be discussing generalized linear models (GLMs) as models that can be useful to test many different hypotheses. Also, understanding how GLMs can be used to test your hypothesis will help you understand other, including more advanced, statistical models (Pongpipat_et_al_PracticalExtensionStatisticsForPsychology).\nRemember: the model you choose is just an approximation of the real world. This means that often times alternative models would be possible (models like t-tests, ANOVAs, ANCOVAs, etc.6). In fact, you may be collaborating with someone who wants to model your hypothesis with a different method. In this handbook, we compare GLMs to alternative model types here. And remember: the steps in the statistical modelling framework are generally applicable. Regardless of the method you apply, you need to ground your choices in good biological and statistical theory.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#the-examples",
    "href": "handbookSMIntro.html#the-examples",
    "title": "Statistical Modelling Handbook",
    "section": "The examples",
    "text": "The examples\nHere we’ve gathered examples following our statistical modelling framework structure. You can request/contribute new examples here",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "href": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "title": "Statistical Modelling Handbook",
    "section": "From statistical modelling to scientific report writing",
    "text": "From statistical modelling to scientific report writing\nHere you can see how you can use the steps in the Statistical Modelling Framework to outline your communication of your hypothesis testing in reports and paper.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#where-to-from-here",
    "href": "handbookSMIntro.html#where-to-from-here",
    "title": "Statistical Modelling Handbook",
    "section": "Where to from here?",
    "text": "Where to from here?\nTBA: - boosted regression trees - Bayesian negative bionomial regression mixed models - nested design in mixed models - random forest vs. GAMs - meta-analysis - complex models - classification (categorical response) and regression (continous response) trees - boosted regression trees - boosted regression trees iteratively fit models gradually increasing emphasis on observations that were initially poorly fit - ADMB and TMB\n\nmultivariate statistics\nexploratoryordination/clustering\ncross validation\n\n[[Principle Component Analysis]] linearly transforms multivariate data into a new coordinate system where the majority of the variation in the data is captured with fewer dimensions than the initial data - Li et al. 2023\n[[Principle Component Analysis|PCA]] constructs a “map” of the samples where samples that are more similar are closer together\n\nAcknowledgements\nTBA",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#footnotes",
    "href": "handbookSMIntro.html#footnotes",
    "title": "Statistical Modelling Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhere, we can also use the term “research hypothesis” instead of explanation and you’ll see we quickly switch to using this term↩︎\nMath skills have often been underemphasized in many Biology educations. This has not been helpful or necessary. Biologists need math and are very capable at applying math to solve problems, but the math needs to be useful; that is, math that you can apply to your own needs as you research biology. The DSP Program aims at providing statistical modelling tools that will be useful to you as a biologist. It is a happy coincidence that the same skills can be applied to a lot of other situations as well. The programming and statistics skills you are learning here will be useful to you in the future - in your future courses, thesis-writing and a wide range of careers.↩︎\nmore on causal vs. correlative explanations coming soon↩︎\nuseful predictions will always include uncertainty↩︎\nand these will be educated guesses!↩︎\nDon’t worry if these terms don’t mean anything to you yet - more to come!↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html",
    "href": "DSPPH_SM_StartingModel.html",
    "title": "Statistical Modelling: Starting Model",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nlearn why you need a starting model to test your hypothesis\nlearn what model fitting means\nchoose and fit a starting model to start testing your hypothesis\ntake a first look at your fitted starting model"
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-whyNeed",
    "href": "DSPPH_SM_StartingModel.html#sec-whyNeed",
    "title": "Statistical Modelling: Starting Model",
    "section": "Why do you need a starting model?",
    "text": "Why do you need a starting model?\n\nImagine your research hypothesis is:\n\nResponse ~ Predictor + 1\n\nwhere you are hypothesizing that\n\nvariability in Response is explained by variability in Predictor.\n\nThis can be tested by determining the evidence for an effect of Predictor on your Response. An effect means that we get a change in Response when we observe a change in Predictor.\nIn fitting your model, you will be estimating the effect of Predictor on Response - the magnitude and direction of the effect, as well as an estimate of the uncertainty (error) in the effect.\n\n\n\n\n\n\nEffects are coefficients\n\n\n\n\n\nWhen we say that a predictor has “an effect” on a response, we are saying that a change in the predictor leads to a change in the response.\nThis change in the response that comes from a unit change in the predictor is estimated as a coefficient.\nThe coefficient of a continuous predictor is the slope. The slope describes the change in response that you get from a unit change in the predictor. For example, if your hypothesis is Growth ~ Temperature + 1, (and Temperature is continuous), the slope (coefficient) for Temperature will tell you the change in Growth you expect for a 1˚C change in Temperature.\nThe coefficient of a categorical predictor tells you how much the response will change when the categorical predictor changes from one category (level) to another. For example, if your hypothesis is Growth ~ Species + 1, (and Species is categorical with “Species A” and “Species B”), the coefficient for Species will tell you the change in Growth you expect when you change from one Species to another (e.g. “Species A” to “Species B”).\n\n\n\nYour starting model will let you estimate this effect (coefficient) of your predictor on your response. Your starting model will also let you estimate the error (uncertainty) around this effect (coefficient).\nOnce you have these estimates, you can test your hypothesis to see if the effect of the predictor on your response is meaningful (i.e. is the coefficient significantly different than zero?1)."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-WhatMeanFitModel",
    "href": "DSPPH_SM_StartingModel.html#sec-WhatMeanFitModel",
    "title": "Statistical Modelling: Starting Model",
    "section": "What does it mean to “fit a model”?",
    "text": "What does it mean to “fit a model”?\nLet’s take a step back and start by looking at the structure of a statistical model.\nA statistical model is a model of your hypothesis where the coefficients of the model (e.g. slope, intercept) are estimated from your data.\nA statistical model is a model that include both a deterministic part and a stochastic part.\n\nThe deterministic part represents your research hypothesis in math form - this describes how the predictors and response are related.\nThe stochastic part represents the error in your model. This includes error due to all the other possible factors or predictors that you have not been included in your hypothesis (called “process error”) as well as any error made when you made your observations (called “measurement error”).\n\n\n\n\n\n\n\nMore on statistical model form\n\n\n\n\n\nWe can represent a general statistical model as\n\\(E(Y_i) = Function(Pred1_i,  Pred2_i, ...) \\tag{Deterministic part}\\)\n\\(Resp_i \\sim Distribution(E(Y_i)) \\tag{Stochastic part}\\)\nwhere \\(Resp_i\\) is your response, \\(Pred1_i, Pred2_i\\) are your predictors for observation \\(i\\), and \\(E(Y_i)\\) is the expected value of your response.\nHere is an example for a case where variability in your response is explained by a continuous predictor:\n\nNote here that i) the shape assumption (deterministic part) is that the effect of the predictor on the response is linear, and ii) the error distribution assumption (stochastic part) is that the error is normal, meaning that your observations should be assumed to be normally distributed around the fitted value (\\(\\mu_i\\)).\n\n\n\nSo to choose (and eventually) fit your starting model, you need to choose both deterministic and stochastic assumptions. Happily, how we make our choices lies back in the biological world."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#choosing-your-starting-model",
    "href": "DSPPH_SM_StartingModel.html#choosing-your-starting-model",
    "title": "Statistical Modelling: Starting Model",
    "section": "Choosing your starting model",
    "text": "Choosing your starting model\nIt is important to note that there is more than one model you could use to test your hypothesis. This is because each model is a simplification and approximation of the real world, and there are a number of mathematical ways one can simplify and approximate the processes involved in your research hypothesis.\nIn this Handbook, you will learn about some very useful models, and how to choose among them. We will also discuss alternatives - this will give you other options, but will also help you communicate to other researchers that may choose different methods for their hypothesis testing.\nDespite the fact that there is more than one valid starting model, not all models are useful starting models. Here we will focus on finding a useful starting model.\nA useful starting model is one that reflects the mechanistic2 understanding underlying your research hypothesis and one that reflects the nature of your data (observations).\nNote: you will first be able to assess if your starting model is a useful one AFTER you have fit the model to the data3. At this stage you just need to pick an intelligent starting point - but what does that mean? And how do you do it?\n\n1. Choosing your error distribution assumption:\nTo choose your starting model, start by choosing the stochastic part of your model by choosing an error distribution assumption.\nThis describes how the data should be assumed to be distributed around your model fit (e.g. how to model the scatter of the data around the line in the figure above). Note I use the word “assumption” here - we are picking an existing mathematical form (data distribution4) that can approximate the behaviour of our observations. This is an assumption that we will test in the Validate section to come.\nHow do you choose?\n\nThink about theory: Note that the error in your statistical model (scatter in the plot above) is error around the response variable (i.e. on the y-axis). The key to choosing an error distribution assumption is then to look at your response variable.\n\n\nCan your response be a decimal (continuous) and positive or negative? Choose a normal error distribution assumption.\n\nCan your response be a decimal (continuous) but only positive? Start by using a Gamma error distribution assumption.\n\nCan your response only be a positive integer? Try a poisson error distribution assumption.\n\nCan your response only be one of two values? Start with a binomial error distribution assumption.\n\nPlot your data: Plotting your response variable can also help you determine the data distribution that would be the best starting point for your error distribution assumption.\n\n\n2. Choosing your shape assumption:\nThe next step in choosing your starting model is to choose your shape assumption. Your shape assumption specifies how your predictor(s) and your response are related to one another. This represents the deterministic part of your model and answers the question “what shape do I expect the relationship between my response and predictor to be?”.\nYour choices are linear (where a unit change in the predictor always leads to the same change in the response) or non-linear (where the effect of the predictor on the response depends on the value of the predictor). Note that you need to make a shape assumption choice for each of your predictors in your research hypothesis.\nHow do you choose?\nThink about theory: The first thing to consider is the nature of your predictor variable. Is it categorical? If yes, choose a linear shape assumption, as a non-linear shape assumption does not make sense for a categorical predictor (e.g. species).\nIs your predictor continuous? Then you need to think a bit more about the relationship between your predictor and response. Do you expect the predictor to always affect your response in the same way (i.e. a unit change in your response for a unit change in your predictor is expected to be the same over the range in your predictor)? Or do you expect that relationship to change as your predictor changes?\n\nPlot your data: Plotting your response vs. your predictor is another good exercise to get you thinking about what shape assumption will be appropriate. The GGally package has some good options for quickly plotting your data:\n\nlibrary(palmerpenguins) # loading palmer penguins data\nlibrary(GGally) # loading GGally package\nlibrary(dplyr) # loading dplyr package for select() \n\nmyDat&lt;-select(penguins, bill_length_mm, body_mass_g, species) # select a subset of columns to plot.  These would be your response and predictor columns\n\nggpairs(data=myDat, # your data \n        mapping=aes(col=species), # ggpairs will plot all columns in myDat, so we only need to tell it here any grouping variables we also want to include\n        upper=\"blank\" # keep the upper triangle of plots blank for this simple example.  Check ?ggpairs for more options.\n        )\n\n\n\n\n\n\n\n\nIn this course, we will primarily focus on statistical models assuming a linear shape assumption. You will see how flexible these can be, but we will also discuss what you should do if you want to assume non-linear5 relationships between your response and predictors.\n\n\nCommunicating your starting model\nTo communicate your starting model, report your research hypothesis, your response and predictors (main effects and interactions) along with descriptions of your error distribution and shape assumptions.\nFor each, describe how you arrived at your choices (e.g. “a poisson error distribution assumption was chosen as the response variable is count data…”). We will go over examples of all this in class."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-fitting",
    "href": "DSPPH_SM_StartingModel.html#sec-fitting",
    "title": "Statistical Modelling: Starting Model",
    "section": "Fitting your starting model",
    "text": "Fitting your starting model\nOnce you have chosen your starting model, you will fit the model to your data so that it can be used to test your hypothesis.\n As mentioned above, fitting your model means that you are going to use your data to estimate the value of the coefficients in your model (e.g. slope, intercept).\nThe best choices for the coefficient values are the coefficient values that give you the highest probability of observing your data. In other words, the best choices for the coefficients are values that are most likely given the data you have6. Fitting models this way is done using a method called maximum likelihood.\nFor example, think about how we would draw a “best-fit” linear line through this relationship:\n\nWhat we do intuitively is to find a line that minimizes the error7 in the model (i.e. the average difference between each observation and the fitted line). The coefficient values of this “best-fit” line have the maximum likelihood given our data.\nThe math involved when fitting your model follows the same logic to find the most likely values for your model coefficients. The actual math used will vary based on the type of model you are fitting (i.e. based on your error and shape assumptions), but in general, finding the right coefficients can be illustrated like this “gradient descent” illustration:\n\nThe most likely coefficient values are found by choosing a starting point for coefficient values and fitting the model. Then, the coefficient values are changed slightly and a new fit is made. The new fit is compared to the old fit to determine if the fit improved (i.e. the average error around the model decreased). This procedure is repeated8 until the error is reduced as much as possible. The resulting coefficient values are the most likely coefficient values for your model given your data.\nHere is another illustration showing gradient descent for two coefficients (e.g. two slopes):\n\nThe mathematical methods involved to fit our model will vary depending on our error distribution assumption and shape assumption. The math methods are behind the functions you will use to fit our models in R. Let’s look at one example of this now.\n\nGeneralized Linear Models (GLMs)\nGeneralized Linear Models or GLMs are statistical models that have a linear shape assumption but allow for a wide range of error distribution assumptions. The “generalized” in generalized linear model refers to the fact that the methods used to fit a GLM were developed from methods used to fit models that were restricted to a normal error distribution assumption (e.g. simple linear models and ANOVAs)9:\n\nSo we can use a GLM as our starting model when we have a shape assumption that is linear, and one of a variety of error distribution assumptions - see a little further along to find out which error distribution assumptions are supported by GLMs.\n\nHow to fit a GLM to data in R\n\nChoosing a link function:\nTo fit a GLM, you need to let R know what error distribution assumption you are using so that the math governing the stochastic part of your model can be “linked” to the original math developed for a normal error distribution assumption.\nSo, we need to choose a “link function” to use when we fit our model to data. This is not hard: There are canonical (default) link functions associated with each of the error distribution assumptions for your GLM. You can find them in R with ?family which will open up the help window to show:\n\nIn general, start with the default link function that matches the error distribution assumption you chose for your starting model.\n\n\n\n\n\n\n\nTransformations vs. GLMs\n\n\n\n\n\nYou may be familiar with the idea of transforming your response variable to be normally distributed for use in linear model. This idea stems from a time when methods were limited to those requiring a normal error distribution assumption. GLMs mean transformations are often no longer necessary as you can now choose an error distribution that reflects the nature of your data instead.\nWhile the idea of transformations is similar to the GLM’s link function, they are not the same. Transformations transform the response variable itself in order to try to constrain it to a normal error distribution assumption. In contrast, link functions “transform” the expected (fitted) value of the model while also indicating an error distribution that matches the original data.\nWhen possible, using a GLM with a link function is preferable to transforming your response variable. This is because transformations:\n\nchange the response variable itself making interpretation of modelled effects more difficult\ncan introduce bias in the coefficients,\ncan result in models that make predictions that are impossible, and\ncan be tricky to find an appropriate transformation.\n\nThat said, there may be times when you want to use a transformation of your response - e.g. to follow a method that is already established in your field. For these cases, you can still fit the model with the GLM strategy we describe here.\n\n\n\n\n\nFitting your GLM to your data:\nThe function for fitting a GLM in R is (helpfully) glm() and it is already installed in base R (no need to load another package).\nTo fit your model to the data you need to tell R your hypothesis10, your data, and your error distribution assumption:\n\nstartMod &lt;- glm(formula = Resp ~ Pred + 1, # your hypothesis\n                data = myDat, # your data\n                family = ...) # your error distribution assumption\n\nFor example, a GLM fit with a Gamma error distribution assumption would be:\n\nstartMod &lt;- glm(formula = Resp ~ Pred + 1, # your hypothesis\n                data = myDat, # your data\n                family = Gamma(link=\"inverse\")) # your error distribution assumption, with the canonical link \n\nAnd that’s it! You are now ready to fit a GLM as a starting model on the road to testing your research hypothesis!\n\n\n\n\nYour GLM model object\nBefore you can use your model to test your hypothesis, you need to validate your starting model. This will be the focus of the next section of your statistical modelling framework.\nFor now though, let’s take a first look at your starting model.\nFitting your GLM will produce a model object (called startMod above) - let’s explore this object. If you print information about the object itself, you will get something like:\n\nThis output includes11:\n\nA: a description of your starting model: This gives back information on the starting model you fit.\nB: estimates of the coefficients of your model: This gives you estimates of each coefficient in your model. Remember (as mentioned above) that the coefficients tell you the direction and magnitude of the effects of your predictor on your response. Coefficients in this output can be hard to interpret (e.g. they may be influenced by the error distribution assumption, and/or be complicated by many factor levels in a categorical predictor). We will be discussing how to get estimates of your coefficients in meaningful ways.\nC: Measures of how well your model performs: This section includes:\n\ndegrees of freedom for the null model (“Total” or “Null”, assuming no effects of your predictors on your response) and your starting model (“Residual”). Degrees of freedom are a measure of how complicated your model is and how much data you have.\ndeviance for the null model (assuming no effects of your predictors on your response) and remaining deviance after your starting model is applied. Deviance represents the variability in your response variable. It is this variability we are trying to explain. Comparing the Residual Deviance (remaining variability in your response after starting model is applied) with Null Deviance (original variability in your response that you were trying to explain) tells you how your model performs (i.e. how much variability in your response did you manage to explain).\nAIC stands for Akaike Information Criterion. AIC is another way of indicating model performance. It balances the explained variation with how complicated your starting model is. Your model complexity relates to how many predictors (and individual terms) are in your model as well as the shape of your model. We will talk much more about AIC in the Hypothesis Testing section of your model framework.\n\n\nYou can get a little more information about your model using the summary() command. Using summary(startMod) will lead to something like:\n\nSimilar to the output above, there are three sections produced:\n\nA: a description of your starting model\nB: estimates of the coefficients of your model: Notice that here you get more information about your coefficients. The summary() output also gives you information about the error around your coefficients as well as a test of significance of the coefficient. This test is a kind of hypothesis test, but the meaning behind the test and result will vary with your starting model structure. We will discuss this more in the Hypothesis Testing section coming up.\nC: Measures of how well your model performs: This section gives you similar information to the output above, but also includes the “Number of Fisher iterations”. This relates to how the model was fit. See the section on “Fitting your starting model”.\n\nIf you’re interested, here are some examples of GLM model objects. And if you’re not, skip to A first look at your starting model. As mentioned above, we will come back to coefficients again in the Reporting section of the Statistical Modelling Framework.\n\n\n\n\n\n\nGLM model object examples\n\n\n\n\n\nLet’s take a look at an examples of a GLM object in R.\nNOTE: the descriptions here are relevant for models with a normal error distribution assumption (i.e. using an “identity” link). We will generally not be using information from these objects in our Statistical Modelling Framework as it is easy to misinterpret the information (e.g. with error distribution assumptions other than normal).\n\nExample 1: Resp ~ ContPred + 1\nThe first example fits a GLM to test the hypothesis that\nResp ~ ContPred + 1\nwhere\n\nResp is your response variable,\nContPred is a continuous predictor,\nand your error distribution assumption is normal.\n\n\nstartMod.1 &lt;- glm(Resp ~ ContPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.1 object gives you:\n\nsummary(startMod.1)\n\n\nCall:\nglm(formula = Resp ~ ContPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4645.2  -2072.8     60.6   2060.8   4885.7  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20871.686   2176.542   9.589  &lt; 2e-16 ***\nContPred      -14.143      2.713  -5.212 2.74e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5551537)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance: 2764665463  on 498  degrees of freedom\nAIC: 9187.7\n\nNumber of Fisher Scoring iterations: 2\n\n\nAt the top of the output is the “Call”- the model you fit with the glm() function:\n\nsummary(startMod.1)$call\n\nglm(formula = Resp ~ ContPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\n\nBelow this are the coefficients. Notice there are two coefficients, and for each coefficient, you can see four values:\n\nthe coefficient estimate (Estimate),\nuncertainty (as standard error, Std.Error),\na t-statistic (t value, based on the estimate and error associated with the coefficient)12,\nand probability associated with the t-statistic (Pr(&gt;|t|)):\n\n\nsummary(startMod.1)$coefficients\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 20871.68582 2176.541631  9.589380 4.247405e-20\nContPred      -14.14253    2.713359 -5.212184 2.739251e-07\n\n\nThe t-statistic is used to test the null hypothesis that an estimate (a coefficient in this case) is not different than 0. The probability gives us the probability that you would get a t-statistic at least as large as you did even though the null hypothesis is in fact true. If the probability is very low, it is more likely our estimate is different than 0.\n\nNote that you have two coefficient estimates (rows) in the table above:\n\nThe first row of the coefficient table gives us the coefficient estimates associated with the intercept (2.087169^{4})\nThe second row is the coefficient estimate associated with ContPred. Note that as ContPred is a continuous variable, this estimate represents the slope of the linear effect of ContPred on Resp, i.e. for every unit change in ContPred, you get a -14.14 change in Resp.\n\n\n\nExample 2: Resp ~ CatPred + 1\nThe second example fits a GLM to test the hypothesis that\nResp ~ CatPred + 1\nwhere\n\nResp is your response variable,\nCatPred is a categorical predictor with three levels (“North”, “South”, “Central”),\nand your error distribution assumption is normal.\n\n\nstartMod.2 &lt;- glm(Resp ~ CatPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.2 object gives you:\n\nsummary(startMod.2)\n\n\nCall:\nglm(formula = Resp ~ CatPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2169.85   -731.45    -53.57    712.81   2912.31  \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6737.60      79.63   84.61   &lt;2e-16 ***\nCatPredSouth    2596.95     110.40   23.52   &lt;2e-16 ***\nCatPredCentral  5406.34     108.61   49.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 976451.4)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance:  485296363  on 497  degrees of freedom\nAIC: 8319.8\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nNote that we now have three coefficients in the table above. Recall from the Why do you need a starting model section that, with a categorical Predictor, fitting a model finds the mean predicted value of the Response at each category (level) of the categorical Predictor. The three coefficients are:\n\nThe coefficient labelled (Intercept) gives us the fitted value of our Resp when CatPred is “North” (6737.6). When you have a categorical predictor, R uses the Intercept coefficient to represent the predicted value of the response at one of the categories (levels).13 By default it chooses the first of the categories (levels) of your predictor (in this case, “North”)14\nThe coefficient labelled CatPredSouth gives you the difference between the predicted value of the response when CatPred = \"South\" and when CatPred = \"North\". So if you want to know the predicted value of Resp when CatPred = \"South\" you need to calculate 6737.6 + 2596.95 = 9334.55.\nThe coefficient labelled CatPredCentral gives you the difference between the predicted value of the response when CatPred = \"Central\" and when CatPred = \"North\". So if you want to know the predicted value of Resp when CatPred = \"Central\" you need to calculate 6737.6 + 5406.34 = 1.214394^{4}.\n\nYes, this is tedious way to calculate the coefficients in your model! And that is why use a different way when we come to the Reporting section of the Statistical Modelling Framework.\n\n\nExample 3: Resp ~ ContPred + CatPred + ContPred:CatPred + 1\nThe third example fits a GLM to test the hypothesis that\nResp ~ ContPred + CatPred + ContPred:CatPred + 1\nwhere\n\nResp is your response variable,\nContPred is a continuous predictor,\nCatPred is a categorical predictor with three levels (“North”, “South”, “Central”),\nContPred:CatPred indicates that you are including an interaction term representing a two-way interaction between your predictors,\nand your error distribution assumption is normal.\n\n\nstartMod.3 &lt;- glm(Resp ~ ContPred + CatPred + ContPred:CatPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.3 gives you:\n\nsummary(startMod.3)\n\n\nCall:\nglm(formula = Resp ~ ContPred + CatPred + ContPred:CatPred + \n    1, family = gaussian(link = \"identity\"), data = myDF)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2204.12   -545.06     58.81    574.09   3003.42  \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             22335.803   1283.405  17.404  &lt; 2e-16 ***\nContPred                  -19.516      1.604 -12.169  &lt; 2e-16 ***\nCatPredSouth            -2665.492   1818.608  -1.466 0.143373    \nCatPredCentral           -964.233   1805.741  -0.534 0.593594    \nContPred:CatPredSouth       6.660      2.266   2.939 0.003444 ** \nContPred:CatPredCentral     7.985      2.255   3.541 0.000436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 638994)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance:  315663026  on 494  degrees of freedom\nAIC: 8110.7\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nNote that we now have six coefficients in the table above:\n\nThe coefficient labelled (Intercept) gives us the fitted value of our Resp when CatPred is “North” and ContPred is set to the mean value of ContPred.\nThe coefficient labelled ContPred gives us the coefficient (slope) associated with ContPred when CatPred = \"North\".\nThe coefficient labelled CatPredSouth gives you the difference between the predicted value of the response when CatPred = \"South\" and when CatPred = \"North\" (with ContPred is set to the mean value of ContPred). So, when CatPred = \"Central\", the coefficient (intercept) for CatPred is 2.23358^{4} + -2665.49 = 1.967031^{4}.\nThe coefficient labelled CatPredCentral gives you the difference between the predicted value of the response when CatPred = \"Central\" and when CatPred = \"North\" (with ContPred is set to the mean value of ContPred). So, when CatPred = \"Central\", the coefficient (intercept) for CatPred is 2.23358^{4} + -964.23 = 2.137157^{4}.\nThe coefficient labelled ContPred:CatPredSouth gives us the difference between coefficient (slope) associated with ContPred when CatPred = \"South\" vs. the ContPred slope when CatPred = \"North\". So, when CatPred = \"South\", the coefficient (slope) for ContPred is -19.52 + 6.66 = -12.86.\nThe coefficient labelled ContPred:CatPredCentral gives us the difference between coefficient (slope) associated with ContPred when CatPred = \"Central\" vs. the ContPred slope when CatPred = \"North\". So, when CatPred = \"Central\", the coefficient (slope) for ContPred is -19.52 + 7.99 = -11.53.\n\nYes, this is tedious way to calculate the coefficients in your model! And that is why use a different way when we come to the Reporting section of the Statistical Modelling Framework."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-firstLookModel",
    "href": "DSPPH_SM_StartingModel.html#sec-firstLookModel",
    "title": "Statistical Modelling: Starting Model",
    "section": "A first look at your starting model",
    "text": "A first look at your starting model\nYou can use the visreg package to quickly visualize your modelled effects\n\nlibrary(visreg) # load visreg package\nlibrary(ggplot2) # load ggplot2\n\nvisreg(startMod.3, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"ContPred\", # predictor on x-axis\n       by = \"CatPred\", # predictor plotted as colour\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  geom_point(data = myDF, # data\n             mapping = aes(x = ContPred, y = Resp, col = CatPred))+ # add your data to your plot\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cont, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nRemember though: before you explore these modelled effects too closely, you have to validate your model."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#up-next",
    "href": "DSPPH_SM_StartingModel.html#up-next",
    "title": "Statistical Modelling: Starting Model",
    "section": "Up next",
    "text": "Up next\nNext we will discuss how you can make validate your model (make sure your starting model can be used to test your hypothesis), and then test your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#footnotes",
    "href": "DSPPH_SM_StartingModel.html#footnotes",
    "title": "Statistical Modelling: Starting Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore on this to come!↩︎\nI will keep mentioning mechanisms. In our statistical model building, we keep our focus on biologically meaningful or mechanistic explanations of variability in our response. This is because i) explaining the world through mechanisms is necessary for true understanding and to be able to show this understanding through prediction (e.g. the difference between correlation and causation). And ii) this is where the joy of being a biologist lies!↩︎\nmore on this to come↩︎\nA good time to review your notes on data distributions from earlier↩︎\nwhich might be non-linear of known shape, or non-linear of unknown shape↩︎\nParaphrased from Crawley 2013 pg. 451↩︎\nhere shown as Root Mean Square Error which is a method used when one has a normal error distribution assumption and linear shape assumption↩︎\neach repetition is called “an iteration”↩︎\nWe will discuss these other model types in upcoming classes↩︎\nfor a binomial error distribution assumption, you might have your response as the # successes in # of trials. In such cases, you would present your hypothesis as cbind(Success, Trials) ~ Predictor + 1↩︎\nnote that we will be discussing this more in depth when we get to the Reporting section of the Statistical Modelling Framework↩︎\nnote that the type of statistic shown will depend on the structure of your model including the error distribution assumption↩︎\nThis is called “dummy level coding”. You can avoid this with “level means coding”.↩︎\nNote that you can control this if needed.↩︎"
  },
  {
    "objectID": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "href": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "title": "Frequently Asked Questions",
    "section": "How do I send my comments/questions feedback?",
    "text": "How do I send my comments/questions feedback?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "href": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "title": "Frequently Asked Questions",
    "section": "I want to include a DSP module in my course. Who should I contact?",
    "text": "I want to include a DSP module in my course. Who should I contact?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "Structure of the DSP Program",
    "section": "",
    "text": "The DSP Program consists of a series of modules intended to allow students to gain, practice, apply and communicate their data-skills.\nSemester:\n\n\n\n\n\n\n\n\n\n\n\n\nSemester\n2\n3\n4\n5 (in 2024)\n6 (in 2025)\nKandidat (in 2025)\n\n\n\n\n\nIntro Module:\nBFTP (Marshall)\nModule:\nMikrobiologi for biologer (Koren, Marzocchi)\nCourse:\nProgramming and Statistics for Biologists\nModule:\nAkvatisk Biologi (Neuheimer)\nModule:\nPlanters Økofysiologi (Eller, Sorrell, Neuheimer)\nExit Module:\nCareer (Taskforce)\n\n\n\n\nothers in development\n\nothers in development\nExit Module:\nResearch (Taskforce)\nothers in development\n\n\n\n\n\n\n\nothers in development",
    "crumbs": [
      "What is the DSP Program?",
      "Structure of the DSP Program"
    ]
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#motivation",
    "href": "philosophy.html#motivation",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#terminology",
    "href": "philosophy.html#terminology",
    "title": "DSP Program - Philosophy",
    "section": "Terminology",
    "text": "Terminology\nData skills: We use the term “data skills” to represent the quantitative and computing skills involved in many fields of research (including biological) and in demand from a range of employers. This includes data literacy (the collecting, management, archiving and wrangling of data), computational thinking (computational logic, problem solving, pattern identification, algorithms), analytical skills (collecting and considering information, making decisions), model building and hypothesis testing, and other quantitative skills. Other terms used in the literature, community and job market are “data science” and “digital competences”\nPortfolio: We use the term “portfolio” to represent the collection of data skills the student will acquire throughout the degree. Later modules in the program are tied to elective courses so that students will have portfolios that vary based on their experience. In all cases, the students will create an explicit Data Skills Portfolio (DSP) to develop their awareness and confidence in their skills, help clarify the applicability of their skills across disciplines, and more easily communicate their skills to future employers.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#core-competencies",
    "href": "philosophy.html#core-competencies",
    "title": "DSP Program - Philosophy",
    "section": "Core competencies",
    "text": "Core competencies\nSkills are identified through the following core competencies\n\ncritical thinking\ngeneral programming\ndata management\ndata visualization\nstatistical modelling\nproject management\nskills marketing",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#guiding-principles-of-the-dsp-program",
    "href": "philosophy.html#guiding-principles-of-the-dsp-program",
    "title": "DSP Program - Philosophy",
    "section": "Guiding principles of the DSP Program",
    "text": "Guiding principles of the DSP Program\n\nRelevance\nThe data skills taught will be relevant and state-of-the-art with respect to the current needs of both the biological research community and the greater job market. Skills will be taught in the context of current biological research.\n\n\nBest Practices\nCourse content and instruction will follow current best-practices for teaching data skills and for teaching to a diversity of students (diversity of backgrounds, learning styles). To this end, a common teaching strategy will be developed.\n\n\nCohesion\nCohesion throughout the DSP Program is necessary for student learning and mastering of skills. Program cohesion will be developed through repetition, consistency and clarity: Students will have a chance to apply skills repeatedly and regularly with DSP courses and modules positioned in as many semesters as possible. Skills will be taught with consistency with instructors using a common framework, syntax and terminology across DSP courses and modules. Learned skills will be made clear to the students as they will be explictly trained to communicate why and how they are applying skills to accomplish tasks through the development of their Data Skills Portfolio.\n\n\nResilience\nProgram development will support resilience of both the student and the program as a whole.\nStudent resilience will be nurtured by:\n\nencouraging an understanding of the “how” and “why” behind the data skills they are learning so they are aware of the general applicability of their skills.\nrepeated exposure to the training throughout their career so they have a number of opportunities to practice skills,\ndeveloping awareness of the skills they are learning through the building of their Data Skills Portfolio that follows them throughout their degree,\nfeedback opportunities where students are able to identify areas they find challenging so that swift interventions are made, and no student is left behind, and\naccessible tools: Where possible, open-source programs and languages will be taught to allow students uninterrupted access to tools after they leave their education.\n\nProgram resilience will be nurtured by:\n\nthe DSP Program being a shared goal & responsibility across sections: The program will be grounded by input from all Sections in the Department of Biology. Teaching responsibilities will shared by all Sections. The current make-up of the DSP Program Taskforce is available here.\nbuilding in redundancy in teaching responsibilities: Courses and modules will be team taught as much as possible to allow for consistency in the program in the face of staff availability changes, and\ndevelopment of maintainable online resources: Online resources (including this handbook) will be structured to maintanence as minimal as possible.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#program-assessment",
    "href": "philosophy.html#program-assessment",
    "title": "DSP Program - Philosophy",
    "section": "Program assessment",
    "text": "Program assessment\nPlans for the assessment of the program for scope and effectiveness are in development. These will include:\n\nCourse evaluations to assess new courses and modules\nMidterm and final evaluation of the DSP program with students\nFeedback from employers 1-2 years after the first cohort graduates after the DSP Program.\nEmployment statistics of our graduates including employment rates and areas of employment.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "DSPPH_SM_Responses.html",
    "href": "DSPPH_SM_Responses.html",
    "title": "Statistical modelling: Responses",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nDefine your research question and response variable (what variability are you trying to explain?)\nPresent the motivation for your research question (why is it worth explaining?)\nThe first step of the statistical modelling framework is to identify your response variable1.\nYour response variable is the observed variability you are trying to explain. As mentioned earlier, all science is explaining variability - why something you observe is changing. Your response variable is the “thing” you are trying to explain. It is sometimes called by other names such as “dependent variable” or “y variable”.\nBefore you can proceed with your hypothesis making and testing, you need to be clear about the variation you are trying to explain and how it was observed. What is making you curious?\nThese questions are called “research questions” and they identify your response variable (contrast this with your research hypothesis in an upcoming section).\nThough it is not necessary to be able to proceed with statistical modelling, it is useful at this point to stop and think about why you want to explain the variation in your response. Why is it important to explain different tree heights? Or fish abundance? Or hormone level? Being clear about what variation you are trying to explain (your response variable) and why it is important to explain that variation will make up a good portion of your introduction section to a report or paper - and help shape your discussion section as well."
  },
  {
    "objectID": "DSPPH_SM_Responses.html#footnotes",
    "href": "DSPPH_SM_Responses.html#footnotes",
    "title": "Statistical modelling: Responses",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice I write “Response(s)” in the title of this section - plural. It is possible to have multiple response variables and we will discuss this elsewhere in the handbook when we discuss multivariate data. For the focus of this handbook though, we will begin by working with one response variable↩︎"
  }
]
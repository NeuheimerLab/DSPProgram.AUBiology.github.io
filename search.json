[
  {
    "objectID": "DSPPH_SM_Responses.html",
    "href": "DSPPH_SM_Responses.html",
    "title": "Statistical modelling: Responses",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nDefine your research question and response variable (what variability are you trying to explain?)\nPresent the motivation for your research question (why is it worth explaining?)\nThe first step of the statistical modelling framework is to identify your response variable1.\nYour response variable is the observed variability you are trying to explain. As mentioned earlier, all science is explaining variability - why something you observe is changing. Your response variable is the “thing” you are trying to explain. It is sometimes called by other names such as “dependent variable” or “y variable”.\nBefore you can proceed with your hypothesis making and testing, you need to be clear about the variation you are trying to explain and how it was observed. What is making you curious?\nThese questions are called “research questions” and they identify your response variable (contrast this with your research hypothesis in an upcoming section).\nThough it is not necessary to be able to proceed with statistical modelling, it is useful at this point to stop and think about why you want to explain the variation in your response. Why is it important to explain different tree heights? Or fish abundance? Or hormone level? Being clear about what variation you are trying to explain (your response variable) and why it is important to explain that variation will make up a good portion of your introduction section to a report or paper - and help shape your discussion section as well."
  },
  {
    "objectID": "DSPPH_SM_Responses.html#footnotes",
    "href": "DSPPH_SM_Responses.html#footnotes",
    "title": "Statistical modelling: Responses",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNotice I write “Response(s)” in the title of this section - plural. It is possible to have multiple response variables and we will discuss this elsewhere in the handbook when we discuss multivariate data. For the focus of this handbook though, we will begin by working with one response variable↩︎"
  },
  {
    "objectID": "philosophy.html",
    "href": "philosophy.html",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#motivation",
    "href": "philosophy.html#motivation",
    "title": "DSP Program - Philosophy",
    "section": "",
    "text": "The AU Strategy 2025 prioritizes the teaching of general skills and competences and specifically identifies digital skills as an important component of future education. The DSP Program builds on current data skills training efforts to make students more capable, confident & employable.\nThe DSP Program aims to\n\nprovide essential skills for biologists:\n\nBiology has historically been mislabelled a “soft-science” with little emphasis on the quantitative nature of the research, but this is rapidly changing (McCallen et al. 2019). Quantitative and computing skills are essential to robust hypothesis testing in biological research and are integral to supporting research ethics and transparency. These skills are necessary for the pursuit of robust, ethical, and independent science. Moreover, many biological questions concern the handling and analysis of “big data” - this includes time series analysis of sensor data, bioinformatics as well as processing of satellite imagery and spatial calculations in Geographic Information System (GIS). It is our responsibility to provide our students with the data skills necessary to address current research hypotheses in big-data areas and beyond, and to introduce computational thinking without sacrificing biological domain knowledge. To this end, we feel it is important that the data skills are taught BY biologists, ensuring skills are relevant, and teaching is rooted in the motivation of biological research.\n\nincrease the employability of our graduates:\n\nQuantitative and computing skills are also applicable to (and often a prerequisite for) a wide range of careers both within and beyond those careers traditionally held by biology graduates. By providing students with these skills, as well as the tools with which they can promote themselves in their job search, we can increase the employment success of our graduates. In addition, data skills are fundamental to the student’s ability to pursue graduate education and an academic career.\n\nincrease student recruitment and retention:\n\nWe can increase our attractiveness as a Biology program by increasing our ability to provide students with up-to-date data skills. Our reputation for providing students with useful, robust research skills and increasing the employment success of our graduates will increase the attractiveness for potential students of the program. In particular, this will increase the academic strength of our applicants as we attract students interested in developing quantitative skills (including data skills).",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#terminology",
    "href": "philosophy.html#terminology",
    "title": "DSP Program - Philosophy",
    "section": "Terminology",
    "text": "Terminology\nData skills: We use the term “data skills” to represent the quantitative and computing skills involved in many fields of research (including biological) and in demand from a range of employers. This includes data literacy (the collecting, management, archiving and wrangling of data), computational thinking (computational logic, problem solving, pattern identification, algorithms), analytical skills (collecting and considering information, making decisions), model building and hypothesis testing, and other quantitative skills. Other terms used in the literature, community and job market are “data science” and “digital competences”\nPortfolio: We use the term “portfolio” to represent the collection of data skills the student will acquire throughout the degree. Later modules in the program are tied to elective courses so that students will have portfolios that vary based on their experience. In all cases, the students will create an explicit Data Skills Portfolio (DSP) to develop their awareness and confidence in their skills, help clarify the applicability of their skills across disciplines, and more easily communicate their skills to future employers.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#core-competencies",
    "href": "philosophy.html#core-competencies",
    "title": "DSP Program - Philosophy",
    "section": "Core competencies",
    "text": "Core competencies\nSkills are identified through the following core competencies\n\ncritical thinking\ngeneral programming\ndata management\ndata visualization\nstatistical modelling\nproject management\nskills marketing",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#guiding-principles-of-the-dsp-program",
    "href": "philosophy.html#guiding-principles-of-the-dsp-program",
    "title": "DSP Program - Philosophy",
    "section": "Guiding principles of the DSP Program",
    "text": "Guiding principles of the DSP Program\n\nRelevance\nThe data skills taught will be relevant and state-of-the-art with respect to the current needs of both the biological research community and the greater job market. Skills will be taught in the context of current biological research.\n\n\nBest Practices\nCourse content and instruction will follow current best-practices for teaching data skills and for teaching to a diversity of students (diversity of backgrounds, learning styles). To this end, a common teaching strategy will be developed.\n\n\nCohesion\nCohesion throughout the DSP Program is necessary for student learning and mastering of skills. Program cohesion will be developed through repetition, consistency and clarity: Students will have a chance to apply skills repeatedly and regularly with DSP courses and modules positioned in as many semesters as possible. Skills will be taught with consistency with instructors using a common framework, syntax and terminology across DSP courses and modules. Learned skills will be made clear to the students as they will be explictly trained to communicate why and how they are applying skills to accomplish tasks through the development of their Data Skills Portfolio.\n\n\nResilience\nProgram development will support resilience of both the student and the program as a whole.\nStudent resilience will be nurtured by:\n\nencouraging an understanding of the “how” and “why” behind the data skills they are learning so they are aware of the general applicability of their skills.\nrepeated exposure to the training throughout their career so they have a number of opportunities to practice skills,\ndeveloping awareness of the skills they are learning through the building of their Data Skills Portfolio that follows them throughout their degree,\nfeedback opportunities where students are able to identify areas they find challenging so that swift interventions are made, and no student is left behind, and\naccessible tools: Where possible, open-source programs and languages will be taught to allow students uninterrupted access to tools after they leave their education.\n\nProgram resilience will be nurtured by:\n\nthe DSP Program being a shared goal & responsibility across sections: The program will be grounded by input from all Sections in the Department of Biology. Teaching responsibilities will shared by all Sections. The current make-up of the DSP Program Taskforce is available here.\nbuilding in redundancy in teaching responsibilities: Courses and modules will be team taught as much as possible to allow for consistency in the program in the face of staff availability changes, and\ndevelopment of maintainable online resources: Online resources (including this handbook) will be structured to maintanence as minimal as possible.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "philosophy.html#program-assessment",
    "href": "philosophy.html#program-assessment",
    "title": "DSP Program - Philosophy",
    "section": "Program assessment",
    "text": "Program assessment\nPlans for the assessment of the program for scope and effectiveness are in development. These will include:\n\nCourse evaluations to assess new courses and modules\nMidterm and final evaluation of the DSP program with students\nFeedback from employers 1-2 years after the first cohort graduates after the DSP Program.\nEmployment statistics of our graduates including employment rates and areas of employment.",
    "crumbs": [
      "What is the DSP Program?",
      "DSP Program - Philosophy"
    ]
  },
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "Structure of the DSP Program",
    "section": "",
    "text": "The DSP Program consists of a series of modules intended to allow students to gain, practice, apply and communicate their data-skills.\nSemester:\n\n\n\n\n\n\n\n\n\n\n\n\nSemester\n2\n3\n4\n5 (in 2024)\n6 (in 2025)\nKandidat (in 2025)\n\n\n\n\n\nIntro Module:\nBFTP (Marshall)\nModule:\nMikrobiologi for biologer (Koren, Marzocchi)\nCourse:\nProgramming and Statistics for Biologists\nModule:\nAkvatisk Biologi (Neuheimer)\nModule:\nPlanters Økofysiologi (Eller, Sorrell, Neuheimer)\nExit Module:\nCareer (Taskforce)\n\n\n\n\nothers in development\n\nothers in development\nExit Module:\nResearch (Taskforce)\nothers in development\n\n\n\n\n\n\n\nothers in development",
    "crumbs": [
      "What is the DSP Program?",
      "Structure of the DSP Program"
    ]
  },
  {
    "objectID": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "href": "FAQ.html#how-do-i-send-my-commentsquestions-feedback",
    "title": "Frequently Asked Questions",
    "section": "How do I send my comments/questions feedback?",
    "text": "How do I send my comments/questions feedback?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "href": "FAQ.html#i-want-to-include-a-dsp-module-in-my-course.-who-should-i-contact",
    "title": "Frequently Asked Questions",
    "section": "I want to include a DSP module in my course. Who should I contact?",
    "text": "I want to include a DSP module in my course. Who should I contact?",
    "crumbs": [
      "The DSP Program Handbook",
      "Frequently Asked Questions"
    ]
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html",
    "href": "DSPPH_SM_StartingModel.html",
    "title": "Statistical Modelling: Starting Model",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nlearn why you need a starting model to test your hypothesis\nlearn what model fitting means\nchoose and fit a starting model to start testing your hypothesis\ntake a first look at your fitted starting model"
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-whyNeed",
    "href": "DSPPH_SM_StartingModel.html#sec-whyNeed",
    "title": "Statistical Modelling: Starting Model",
    "section": "Why do you need a starting model?",
    "text": "Why do you need a starting model?\n\nImagine your research hypothesis is:\n\nResponse ~ Predictor + 1\n\nwhere you are hypothesizing that\n\nvariability in Response is explained by variability in Predictor.\n\nThis can be tested by determining the evidence for an effect of Predictor on your Response. An effect means that we get a change in Response when we observe a change in Predictor.\nIn fitting your model, you will be estimating the effect of Predictor on Response - the magnitude and direction of the effect, as well as an estimate of the uncertainty (error) in the effect.\n\n\n\n\n\n\nEffects are coefficients\n\n\n\n\n\nWhen we say that a predictor has “an effect” on a response, we are saying that a change in the predictor leads to a change in the response.\nThis change in the response that comes from a unit change in the predictor is estimated as a coefficient.\nThe coefficient of a continuous predictor is the slope. The slope describes the change in response that you get from a unit change in the predictor. For example, if your hypothesis is Growth ~ Temperature + 1, (and Temperature is continuous), the slope (coefficient) for Temperature will tell you the change in Growth you expect for a 1˚C change in Temperature.\nThe coefficient of a categorical predictor tells you how much the response will change when the categorical predictor changes from one category (level) to another. For example, if your hypothesis is Growth ~ Species + 1, (and Species is categorical with “Species A” and “Species B”), the coefficient for Species will tell you the change in Growth you expect when you change from one Species to another (e.g. “Species A” to “Species B”).\n\n\n\nYour starting model will let you estimate this effect (coefficient) of your predictor on your response. Your starting model will also let you estimate the error (uncertainty) around this effect (coefficient).\nOnce you have these estimates, you can test your hypothesis to see if the effect of the predictor on your response is meaningful (i.e. is the coefficient significantly different than zero?1)."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-WhatMeanFitModel",
    "href": "DSPPH_SM_StartingModel.html#sec-WhatMeanFitModel",
    "title": "Statistical Modelling: Starting Model",
    "section": "What does it mean to “fit a model”?",
    "text": "What does it mean to “fit a model”?\nLet’s take a step back and start by looking at the structure of a statistical model.\nA statistical model is a model of your hypothesis where the coefficients of the model (e.g. slope, intercept) are estimated from your data.\nA statistical model is a model that include both a deterministic part and a stochastic part.\n\nThe deterministic part represents your research hypothesis in math form - this describes how the predictors and response are related.\nThe stochastic part represents the error in your model. This includes error due to all the other possible factors or predictors that you have not been included in your hypothesis (called “process error”) as well as any error made when you made your observations (called “measurement error”).\n\n\n\n\n\n\n\nMore on statistical model form\n\n\n\n\n\nWe can represent a general statistical model as\n\\(E(Y_i) = Function(Pred1_i,  Pred2_i, ...) \\tag{Deterministic part}\\)\n\\(Resp_i \\sim Distribution(E(Y_i)) \\tag{Stochastic part}\\)\nwhere \\(Resp_i\\) is your response, \\(Pred1_i, Pred2_i\\) are your predictors for observation \\(i\\), and \\(E(Y_i)\\) is the expected value of your response.\nHere is an example for a case where variability in your response is explained by a continuous predictor:\n\nNote here that i) the shape assumption (deterministic part) is that the effect of the predictor on the response is linear, and ii) the error distribution assumption (stochastic part) is that the error is normal, meaning that your observations should be assumed to be normally distributed around the fitted value (\\(\\mu_i\\)).\n\n\n\nSo to choose (and eventually) fit your starting model, you need to choose both deterministic and stochastic assumptions. Happily, how we make our choices lies back in the biological world."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#choosing-your-starting-model",
    "href": "DSPPH_SM_StartingModel.html#choosing-your-starting-model",
    "title": "Statistical Modelling: Starting Model",
    "section": "Choosing your starting model",
    "text": "Choosing your starting model\nIt is important to note that there is more than one model you could use to test your hypothesis. This is because each model is a simplification and approximation of the real world, and there are a number of mathematical ways one can simplify and approximate the processes involved in your research hypothesis.\nIn this Handbook, you will learn about some very useful models, and how to choose among them. We will also discuss alternatives - this will give you other options, but will also help you communicate to other researchers that may choose different methods for their hypothesis testing.\nDespite the fact that there is more than one valid starting model, not all models are useful starting models. Here we will focus on finding a useful starting model.\nA useful starting model is one that reflects the mechanistic2 understanding underlying your research hypothesis and one that reflects the nature of your data (observations).\nNote: you will first be able to assess if your starting model is a useful one AFTER you have fit the model to the data3. At this stage you just need to pick an intelligent starting point - but what does that mean? And how do you do it?\n\n1. Choosing your error distribution assumption:\nTo choose your starting model, start by choosing the stochastic part of your model by choosing an error distribution assumption.\nThis describes how the data should be assumed to be distributed around your model fit (e.g. how to model the scatter of the data around the line in the figure above). Note I use the word “assumption” here - we are picking an existing mathematical form (data distribution4) that can approximate the behaviour of our observations. This is an assumption that we will test in the Validate section to come.\nHow do you choose?\n\nThink about theory: Note that the error in your statistical model (scatter in the plot above) is error around the response variable (i.e. on the y-axis). The key to choosing an error distribution assumption is then to look at your response variable.\n\n\nCan your response be a decimal (continuous) and positive or negative? Choose a normal error distribution assumption.\n\nCan your response be a decimal (continuous) but only positive? Start by using a Gamma error distribution assumption.\n\nCan your response only be a positive integer? Try a poisson error distribution assumption.\n\nCan your response only be one of two values? Start with a binomial error distribution assumption.\n\nPlot your data: Plotting your response variable can also help you determine the data distribution that would be the best starting point for your error distribution assumption.\n\n\n2. Choosing your shape assumption:\nThe next step in choosing your starting model is to choose your shape assumption. Your shape assumption specifies how your predictor(s) and your response are related to one another. This represents the deterministic part of your model and answers the question “what shape do I expect the relationship between my response and predictor to be?”.\nYour choices are linear (where a unit change in the predictor always leads to the same change in the response) or non-linear (where the effect of the predictor on the response depends on the value of the predictor). Note that you need to make a shape assumption choice for each of your predictors in your research hypothesis.\nHow do you choose?\nThink about theory: The first thing to consider is the nature of your predictor variable. Is it categorical? If yes, choose a linear shape assumption, as a non-linear shape assumption does not make sense for a categorical predictor (e.g. species).\nIs your predictor continuous? Then you need to think a bit more about the relationship between your predictor and response. Do you expect the predictor to always affect your response in the same way (i.e. a unit change in your response for a unit change in your predictor is expected to be the same over the range in your predictor)? Or do you expect that relationship to change as your predictor changes?\n\nPlot your data: Plotting your response vs. your predictor is another good exercise to get you thinking about what shape assumption will be appropriate. The GGally package has some good options for quickly plotting your data:\n\nlibrary(palmerpenguins) # loading palmer penguins data\nlibrary(GGally) # loading GGally package\nlibrary(dplyr) # loading dplyr package for select() \n\nmyDat&lt;-select(penguins, bill_length_mm, body_mass_g, species) # select a subset of columns to plot.  These would be your response and predictor columns\n\nggpairs(data=myDat, # your data \n        mapping=aes(col=species), # ggpairs will plot all columns in myDat, so we only need to tell it here any grouping variables we also want to include\n        upper=\"blank\" # keep the upper triangle of plots blank for this simple example.  Check ?ggpairs for more options.\n        )\n\n\n\n\n\n\n\n\nIn this course, we will primarily focus on statistical models assuming a linear shape assumption. You will see how flexible these can be, but we will also discuss what you should do if you want to assume non-linear5 relationships between your response and predictors.\n\n\nCommunicating your starting model\nTo communicate your starting model, report your research hypothesis, your response and predictors (main effects and interactions) along with descriptions of your error distribution and shape assumptions.\nFor each, describe how you arrived at your choices (e.g. “a poisson error distribution assumption was chosen as the response variable is count data…”). We will go over examples of all this in class."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-fitting",
    "href": "DSPPH_SM_StartingModel.html#sec-fitting",
    "title": "Statistical Modelling: Starting Model",
    "section": "Fitting your starting model",
    "text": "Fitting your starting model\nOnce you have chosen your starting model, you will fit the model to your data so that it can be used to test your hypothesis.\n As mentioned above, fitting your model means that you are going to use your data to estimate the value of the coefficients in your model (e.g. slope, intercept).\nThe best choices for the coefficient values are the coefficient values that give you the highest probability of observing your data. In other words, the best choices for the coefficients are values that are most likely given the data you have6. Fitting models this way is done using a method called maximum likelihood.\nFor example, think about how we would draw a “best-fit” linear line through this relationship:\n\nWhat we do intuitively is to find a line that minimizes the error7 in the model (i.e. the average difference between each observation and the fitted line). The coefficient values of this “best-fit” line have the maximum likelihood given our data.\nThe math involved when fitting your model follows the same logic to find the most likely values for your model coefficients. The actual math used will vary based on the type of model you are fitting (i.e. based on your error and shape assumptions), but in general, finding the right coefficients can be illustrated like this “gradient descent” illustration:\n\nThe most likely coefficient values are found by choosing a starting point for coefficient values and fitting the model. Then, the coefficient values are changed slightly and a new fit is made. The new fit is compared to the old fit to determine if the fit improved (i.e. the average error around the model decreased). This procedure is repeated8 until the error is reduced as much as possible. The resulting coefficient values are the most likely coefficient values for your model given your data.\nHere is another illustration showing gradient descent for two coefficients (e.g. two slopes):\n\nThe mathematical methods involved to fit our model will vary depending on our error distribution assumption and shape assumption. The math methods are behind the functions you will use to fit our models in R. Let’s look at one example of this now.\n\nGeneralized Linear Models (GLMs)\nGeneralized Linear Models or GLMs are statistical models that have a linear shape assumption but allow for a wide range of error distribution assumptions. The “generalized” in generalized linear model refers to the fact that the methods used to fit a GLM were developed from methods used to fit models that were restricted to a normal error distribution assumption (e.g. simple linear models and ANOVAs)9:\n\nSo we can use a GLM as our starting model when we have a shape assumption that is linear, and one of a variety of error distribution assumptions - see a little further along to find out which error distribution assumptions are supported by GLMs.\n\nHow to fit a GLM to data in R\n\nChoosing a link function:\nTo fit a GLM, you need to let R know what error distribution assumption you are using so that the math governing the stochastic part of your model can be “linked” to the original math developed for a normal error distribution assumption.\nSo, we need to choose a “link function” to use when we fit our model to data. This is not hard: There are canonical (default) link functions associated with each of the error distribution assumptions for your GLM. You can find them in R with ?family which will open up the help window to show:\n\nIn general, start with the default link function that matches the error distribution assumption you chose for your starting model.\n\n\n\n\n\n\n\nTransformations vs. GLMs\n\n\n\n\n\nYou may be familiar with the idea of transforming your response variable to be normally distributed for use in linear model. This idea stems from a time when methods were limited to those requiring a normal error distribution assumption. GLMs mean transformations are often no longer necessary as you can now choose an error distribution that reflects the nature of your data instead.\nWhile the idea of transformations is similar to the GLM’s link function, they are not the same. Transformations transform the response variable itself in order to try to constrain it to a normal error distribution assumption. In contrast, link functions “transform” the expected (fitted) value of the model while also indicating an error distribution that matches the original data.\nWhen possible, using a GLM with a link function is preferable to transforming your response variable. This is because transformations:\n\nchange the response variable itself making interpretation of modelled effects more difficult\ncan introduce bias in the coefficients,\ncan result in models that make predictions that are impossible, and\ncan be tricky to find an appropriate transformation.\n\nThat said, there may be times when you want to use a transformation of your response - e.g. to follow a method that is already established in your field. For these cases, you can still fit the model with the GLM strategy we describe here.\n\n\n\n\n\nFitting your GLM to your data:\nThe function for fitting a GLM in R is (helpfully) glm() and it is already installed in base R (no need to load another package).\nTo fit your model to the data you need to tell R your hypothesis10, your data, and your error distribution assumption:\n\nstartMod &lt;- glm(formula = Resp ~ Pred + 1, # your hypothesis\n                data = myDat, # your data\n                family = ...) # your error distribution assumption\n\nFor example, a GLM fit with a Gamma error distribution assumption would be:\n\nstartMod &lt;- glm(formula = Resp ~ Pred + 1, # your hypothesis\n                data = myDat, # your data\n                family = Gamma(link=\"inverse\")) # your error distribution assumption, with the canonical link \n\nAnd that’s it! You are now ready to fit a GLM as a starting model on the road to testing your research hypothesis!\n\n\n\n\nYour GLM model object\nBefore you can use your model to test your hypothesis, you need to validate your starting model. This will be the focus of the next section of your statistical modelling framework.\nFor now though, let’s take a first look at your starting model.\nFitting your GLM will produce a model object (called startMod above) - let’s explore this object. If you print information about the object itself, you will get something like:\n\nThis output includes11:\n\nA: a description of your starting model: This gives back information on the starting model you fit.\nB: estimates of the coefficients of your model: This gives you estimates of each coefficient in your model. Remember (as mentioned above) that the coefficients tell you the direction and magnitude of the effects of your predictor on your response. Coefficients in this output can be hard to interpret (e.g. they may be influenced by the error distribution assumption, and/or be complicated by many factor levels in a categorical predictor). We will be discussing how to get estimates of your coefficients in meaningful ways.\nC: Measures of how well your model performs: This section includes:\n\ndegrees of freedom for the null model (“Total” or “Null”, assuming no effects of your predictors on your response) and your starting model (“Residual”). Degrees of freedom are a measure of how complicated your model is and how much data you have.\ndeviance for the null model (assuming no effects of your predictors on your response) and remaining deviance after your starting model is applied. Deviance represents the variability in your response variable. It is this variability we are trying to explain. Comparing the Residual Deviance (remaining variability in your response after starting model is applied) with Null Deviance (original variability in your response that you were trying to explain) tells you how your model performs (i.e. how much variability in your response did you manage to explain).\nAIC stands for Akaike Information Criterion. AIC is another way of indicating model performance. It balances the explained variation with how complicated your starting model is. Your model complexity relates to how many predictors (and individual terms) are in your model as well as the shape of your model. We will talk much more about AIC in the Hypothesis Testing section of your model framework.\n\n\nYou can get a little more information about your model using the summary() command. Using summary(startMod) will lead to something like:\n\nSimilar to the output above, there are three sections produced:\n\nA: a description of your starting model\nB: estimates of the coefficients of your model: Notice that here you get more information about your coefficients. The summary() output also gives you information about the error around your coefficients as well as a test of significance of the coefficient. This test is a kind of hypothesis test, but the meaning behind the test and result will vary with your starting model structure. We will discuss this more in the Hypothesis Testing section coming up.\nC: Measures of how well your model performs: This section gives you similar information to the output above, but also includes the “Number of Fisher iterations”. This relates to how the model was fit. See the section on “Fitting your starting model”.\n\nIf you’re interested, here are some examples of GLM model objects. And if you’re not, skip to A first look at your starting model. As mentioned above, we will come back to coefficients again in the Reporting section of the Statistical Modelling Framework.\n\n\n\n\n\n\nGLM model object examples\n\n\n\n\n\nLet’s take a look at an examples of a GLM object in R.\nNOTE: the descriptions here are relevant for models with a normal error distribution assumption (i.e. using an “identity” link). We will generally not be using information from these objects in our Statistical Modelling Framework as it is easy to misinterpret the information (e.g. with error distribution assumptions other than normal).\n\nExample 1: Resp ~ ContPred + 1\nThe first example fits a GLM to test the hypothesis that\nResp ~ ContPred + 1\nwhere\n\nResp is your response variable,\nContPred is a continuous predictor,\nand your error distribution assumption is normal.\n\n\nstartMod.1 &lt;- glm(Resp ~ ContPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.1 object gives you:\n\nsummary(startMod.1)\n\n\nCall:\nglm(formula = Resp ~ ContPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4645.2  -2072.8     60.6   2060.8   4885.7  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20871.686   2176.542   9.589  &lt; 2e-16 ***\nContPred      -14.143      2.713  -5.212 2.74e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5551537)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance: 2764665463  on 498  degrees of freedom\nAIC: 9187.7\n\nNumber of Fisher Scoring iterations: 2\n\n\nAt the top of the output is the “Call”- the model you fit with the glm() function:\n\nsummary(startMod.1)$call\n\nglm(formula = Resp ~ ContPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\n\nBelow this are the coefficients. Notice there are two coefficients, and for each coefficient, you can see four values:\n\nthe coefficient estimate (Estimate),\nuncertainty (as standard error, Std.Error),\na t-statistic (t value, based on the estimate and error associated with the coefficient)12,\nand probability associated with the t-statistic (Pr(&gt;|t|)):\n\n\nsummary(startMod.1)$coefficients\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 20871.68582 2176.541631  9.589380 4.247405e-20\nContPred      -14.14253    2.713359 -5.212184 2.739251e-07\n\n\nThe t-statistic is used to test the null hypothesis that an estimate (a coefficient in this case) is not different than 0. The probability gives us the probability that you would get a t-statistic at least as large as you did even though the null hypothesis is in fact true. If the probability is very low, it is more likely our estimate is different than 0.\n\nNote that you have two coefficient estimates (rows) in the table above:\n\nThe first row of the coefficient table gives us the coefficient estimates associated with the intercept (2.087169^{4})\nThe second row is the coefficient estimate associated with ContPred. Note that as ContPred is a continuous variable, this estimate represents the slope of the linear effect of ContPred on Resp, i.e. for every unit change in ContPred, you get a -14.14 change in Resp.\n\n\n\nExample 2: Resp ~ CatPred + 1\nThe second example fits a GLM to test the hypothesis that\nResp ~ CatPred + 1\nwhere\n\nResp is your response variable,\nCatPred is a categorical predictor with three levels (“North”, “South”, “Central”),\nand your error distribution assumption is normal.\n\n\nstartMod.2 &lt;- glm(Resp ~ CatPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.2 object gives you:\n\nsummary(startMod.2)\n\n\nCall:\nglm(formula = Resp ~ CatPred + 1, family = gaussian(link = \"identity\"), \n    data = myDF)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2169.85   -731.45    -53.57    712.81   2912.31  \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6737.60      79.63   84.61   &lt;2e-16 ***\nCatPredSouth    2596.95     110.40   23.52   &lt;2e-16 ***\nCatPredCentral  5406.34     108.61   49.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 976451.4)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance:  485296363  on 497  degrees of freedom\nAIC: 8319.8\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nNote that we now have three coefficients in the table above. Recall from the Why do you need a starting model section that, with a categorical Predictor, fitting a model finds the mean predicted value of the Response at each category (level) of the categorical Predictor. The three coefficients are:\n\nThe coefficient labelled (Intercept) gives us the fitted value of our Resp when CatPred is “North” (6737.6). When you have a categorical predictor, R uses the Intercept coefficient to represent the predicted value of the response at one of the categories (levels).13 By default it chooses the first of the categories (levels) of your predictor (in this case, “North”)14\nThe coefficient labelled CatPredSouth gives you the difference between the predicted value of the response when CatPred = \"South\" and when CatPred = \"North\". So if you want to know the predicted value of Resp when CatPred = \"South\" you need to calculate 6737.6 + 2596.95 = 9334.55.\nThe coefficient labelled CatPredCentral gives you the difference between the predicted value of the response when CatPred = \"Central\" and when CatPred = \"North\". So if you want to know the predicted value of Resp when CatPred = \"Central\" you need to calculate 6737.6 + 5406.34 = 1.214394^{4}.\n\nYes, this is tedious way to calculate the coefficients in your model! And that is why use a different way when we come to the Reporting section of the Statistical Modelling Framework.\n\n\nExample 3: Resp ~ ContPred + CatPred + ContPred:CatPred + 1\nThe third example fits a GLM to test the hypothesis that\nResp ~ ContPred + CatPred + ContPred:CatPred + 1\nwhere\n\nResp is your response variable,\nContPred is a continuous predictor,\nCatPred is a categorical predictor with three levels (“North”, “South”, “Central”),\nContPred:CatPred indicates that you are including an interaction term representing a two-way interaction between your predictors,\nand your error distribution assumption is normal.\n\n\nstartMod.3 &lt;- glm(Resp ~ ContPred + CatPred + ContPred:CatPred + 1, # your hypothesis\n              data = myDF, # your data\n              family = gaussian(link = \"identity\")) # your error distribution assumption\n\nGetting a summary of startMod.3 gives you:\n\nsummary(startMod.3)\n\n\nCall:\nglm(formula = Resp ~ ContPred + CatPred + ContPred:CatPred + \n    1, family = gaussian(link = \"identity\"), data = myDF)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2204.12   -545.06     58.81    574.09   3003.42  \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             22335.803   1283.405  17.404  &lt; 2e-16 ***\nContPred                  -19.516      1.604 -12.169  &lt; 2e-16 ***\nCatPredSouth            -2665.492   1818.608  -1.466 0.143373    \nCatPredCentral           -964.233   1805.741  -0.534 0.593594    \nContPred:CatPredSouth       6.660      2.266   2.939 0.003444 ** \nContPred:CatPredCentral     7.985      2.255   3.541 0.000436 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 638994)\n\n    Null deviance: 2915483317  on 499  degrees of freedom\nResidual deviance:  315663026  on 494  degrees of freedom\nAIC: 8110.7\n\nNumber of Fisher Scoring iterations: 2\n\n\n\nNote that we now have six coefficients in the table above:\n\nThe coefficient labelled (Intercept) gives us the fitted value of our Resp when CatPred is “North” and ContPred is set to the mean value of ContPred.\nThe coefficient labelled ContPred gives us the coefficient (slope) associated with ContPred when CatPred = \"North\".\nThe coefficient labelled CatPredSouth gives you the difference between the predicted value of the response when CatPred = \"South\" and when CatPred = \"North\" (with ContPred is set to the mean value of ContPred). So, when CatPred = \"Central\", the coefficient (intercept) for CatPred is 2.23358^{4} + -2665.49 = 1.967031^{4}.\nThe coefficient labelled CatPredCentral gives you the difference between the predicted value of the response when CatPred = \"Central\" and when CatPred = \"North\" (with ContPred is set to the mean value of ContPred). So, when CatPred = \"Central\", the coefficient (intercept) for CatPred is 2.23358^{4} + -964.23 = 2.137157^{4}.\nThe coefficient labelled ContPred:CatPredSouth gives us the difference between coefficient (slope) associated with ContPred when CatPred = \"South\" vs. the ContPred slope when CatPred = \"North\". So, when CatPred = \"South\", the coefficient (slope) for ContPred is -19.52 + 6.66 = -12.86.\nThe coefficient labelled ContPred:CatPredCentral gives us the difference between coefficient (slope) associated with ContPred when CatPred = \"Central\" vs. the ContPred slope when CatPred = \"North\". So, when CatPred = \"Central\", the coefficient (slope) for ContPred is -19.52 + 7.99 = -11.53.\n\nYes, this is tedious way to calculate the coefficients in your model! And that is why use a different way when we come to the Reporting section of the Statistical Modelling Framework."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#sec-firstLookModel",
    "href": "DSPPH_SM_StartingModel.html#sec-firstLookModel",
    "title": "Statistical Modelling: Starting Model",
    "section": "A first look at your starting model",
    "text": "A first look at your starting model\nYou can use the visreg package to quickly visualize your modelled effects\n\nlibrary(visreg) # load visreg package\nlibrary(ggplot2) # load ggplot2\n\nvisreg(startMod.3, # model to visualize\n       scale = \"response\", # plot on the scale of the response\n       xvar = \"ContPred\", # predictor on x-axis\n       by = \"CatPred\", # predictor plotted as colour\n       overlay = TRUE, # to plot as overlay or panels \n       rug = FALSE, # to include a rug\n       gg = TRUE)+ # to plot as a ggplot\n  geom_point(data = myDF, # data\n             mapping = aes(x = ContPred, y = Resp, col = CatPred))+ # add your data to your plot\n  ylab(\"Response, (units)\")+ # change y-axis label\n  xlab(\"Cont, (units)\")+ # change x-axis label\n  theme_bw() # change ggplot theme\n\n\n\n\n\n\n\n\nRemember though: before you explore these modelled effects too closely, you have to validate your model."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#up-next",
    "href": "DSPPH_SM_StartingModel.html#up-next",
    "title": "Statistical Modelling: Starting Model",
    "section": "Up next",
    "text": "Up next\nNext we will discuss how you can make validate your model (make sure your starting model can be used to test your hypothesis), and then test your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_StartingModel.html#footnotes",
    "href": "DSPPH_SM_StartingModel.html#footnotes",
    "title": "Statistical Modelling: Starting Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore on this to come!↩︎\nI will keep mentioning mechanisms. In our statistical model building, we keep our focus on biologically meaningful or mechanistic explanations of variability in our response. This is because i) explaining the world through mechanisms is necessary for true understanding and to be able to show this understanding through prediction (e.g. the difference between correlation and causation). And ii) this is where the joy of being a biologist lies!↩︎\nmore on this to come↩︎\nA good time to review your notes on data distributions from earlier↩︎\nwhich might be non-linear of known shape, or non-linear of unknown shape↩︎\nParaphrased from Crawley 2013 pg. 451↩︎\nhere shown as Root Mean Square Error which is a method used when one has a normal error distribution assumption and linear shape assumption↩︎\neach repetition is called “an iteration”↩︎\nWe will discuss these other model types in upcoming classes↩︎\nfor a binomial error distribution assumption, you might have your response as the # successes in # of trials. In such cases, you would present your hypothesis as cbind(Success, Trials) ~ Predictor + 1↩︎\nnote that we will be discussing this more in depth when we get to the Reporting section of the Statistical Modelling Framework↩︎\nnote that the type of statistic shown will depend on the structure of your model including the error distribution assumption↩︎\nThis is called “dummy level coding”. You can avoid this with “level means coding”.↩︎\nNote that you can control this if needed.↩︎"
  },
  {
    "objectID": "handbookSMIntro.html",
    "href": "handbookSMIntro.html",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistics",
    "href": "handbookSMIntro.html#why-statistics",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#why-statistical-modelling",
    "href": "handbookSMIntro.html#why-statistical-modelling",
    "title": "Statistical Modelling Handbook",
    "section": "Why statistical modelling?",
    "text": "Why statistical modelling?\nYou can quantify how much variability you can explain with your research hypothesis through statistical modelling. Your statistical model represents your research hypothesis in a mathematical structure. This mathematical structure can be tested against your data to determine what evidence there is for your hypothesis:\n\ncan I explain the variability that I am seeing? (Can I reject my hypothesis?)\ngiven my hypothesis, how much variation in the observations can I explain?\ngiven my hypothesis, what would I expect (predict) to observe under different times or locations?\n\nYour job then is to explain observation variability in time and space by creating a “model” of what (you think) is going on - hence statistical modelling.\nIt is important to remember that any model is only an approximation of what is going on in the real world. As many have said before\n\nAll models are wrong but some are useful.\n\nWe will discuss how you can build useful models that you can use to test your hypotheses.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "href": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "title": "Statistical Modelling Handbook",
    "section": "Introducing a statistical modelling framework",
    "text": "Introducing a statistical modelling framework\n\nThe focus of this section of the handbook is statistics that can be applied to your work as a biologist. For that reason, the motivation for what we are going to do together comes directly from your biological research hypotheses. As a biologist, you have a research hypothesis that you want to test. The method presented in this handbook will help you test it. You will learn how to move from biological theory to hypothesis to the statistical modelling process you will use to test your hypothesis.\nYou will learn this process of statistical modelling by walking through a “Statistical Modelling Framework”. This is a set of steps that you can use to go from your research hypothesis to designing a model, testing your hypothesis and communicating the results.\nThis handbook will walk through the parts of this framework one by one. During DSP modules throughout your degree, you will do the same in class while you practice applying the framework to case studies. In this way, you should see how the framework is generally applicable but also flexible. And after you leave the course you will be able to apply the framework to help you in your statistical analysis in other courses, your thesis, and your future career - in every case, your process will take its starting point and focus from the hypothesis that is motivating you.\nAs we will discuss later in the handbook, working through this framework will also guide you in creating the “guts” of a paper or report. As well as clarifying how and why you made your analysis choices, it will guide you in describing your motivation behind your research question (why is it worthwhile to spend time explaining this variation?) as well as the mechanisms behind your research hypothesis (why do I think X is responsible for the variation I’m trying to explain?). Once through, you’ll have a solid draft that can be the basis of a report, thesis chapter or scientific paper. We will talk about how this works in section.\nA note about our Statistical Modelling Framework: The steps in the image to the right as a linear process but it is not actually a linear process. As you will see in the examples, sometimes you will need to make best guesses5 as to what model might be useful and only after confronting the model with your data will you know if your guesses were reasonable (and useful!) - i.e. the model is a valid one that you can use to test your hypothesis. We will talk about how to find a useful model, how to choose when there are multiple options, and how to communicate your choices.\n\nSteps in the statistical modelling framework:\n\n\nResponse(s)\nHere you will define your research question by identifying your response variable(s).\n\nWhat variability are you trying to explain?\nAnd why is it worth explaining? (your motivation)\n\nNote: we’ll begin by discussing how to model hypotheses with just one response variable before discussing multiple response variable(s).\n\n\nPredictor(s)\nHere you will choose your predictor variables.\n\nwhat could explain the variability in your response?\nwhat are the possible mechanisms behind your argument?\n\n\n\nHypothesis\nHere you will see how your response and predictor variables come together to define your research hypothesis. And we will discuss how to write this hypothesis to begin building your statistical modelling.\n\n\nStarting model\nHere you will choose and fit the starting model that will be used to test your hypothesis. You will do this by choosing and communicating two key assumptions that will help you pick a useful modelling starting point. Then you will fit your model to your data (i.e. confronting your model with your data).\n\n\nModel validation\nHere you will investigate whether your model will be a useful one to test your hypothesis. Your steps here will include considering if you have correlated predictors or problems with observation dependence.\nYou will also considering if your starting model assumptions were realistic. After this step, you will have a model that you can confidently use to test your hypothesis.\n\n\nHypothesis testing\nHere you will test your hypothesis by assessing the evidence supporting your model. We will discuss a number of different methods to do this, but will focus on the model selection method as a robust way to evaluate what your model is telling you about your hypothesis.\n\n\nReporting\nHere you will report the results of your hypothesis testing.\nYou will report:\n\nyour best-specified model identified in the hypothesis testing\nthe effects (patterns) described by your model (including visualizing your model effects)\nhow well your model explains variability in your response.\n\n\n\nPredicting\nHere you will use your model to make predictions of your response under different conditions (while considering prediction limits).\n\n\n\nWhere we will begin: generalized linear models (GLMs)\nTo begin with, we will be discussing generalized linear models (GLMs) as models that can be useful to test many different hypotheses. Also, understanding how GLMs can be used to test your hypothesis will help you understand other, including more advanced, statistical models (Pongpipat_et_al_PracticalExtensionStatisticsForPsychology).\nRemember: the model you choose is just an approximation of the real world. This means that often times alternative models would be possible (models like t-tests, ANOVAs, ANCOVAs, etc.6). In fact, you may be collaborating with someone who wants to model your hypothesis with a different method. In this handbook, we compare GLMs to alternative model types here. And remember: the steps in the statistical modelling framework are generally applicable. Regardless of the method you apply, you need to ground your choices in good biological and statistical theory.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#the-examples",
    "href": "handbookSMIntro.html#the-examples",
    "title": "Statistical Modelling Handbook",
    "section": "The examples",
    "text": "The examples\nHere we’ve gathered examples following our statistical modelling framework structure. You can request/contribute new examples here",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "href": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "title": "Statistical Modelling Handbook",
    "section": "From statistical modelling to scientific report writing",
    "text": "From statistical modelling to scientific report writing\nHere you can see how you can use the steps in the Statistical Modelling Framework to outline your communication of your hypothesis testing in reports and paper.",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#where-to-from-here",
    "href": "handbookSMIntro.html#where-to-from-here",
    "title": "Statistical Modelling Handbook",
    "section": "Where to from here?",
    "text": "Where to from here?\nTBA: - boosted regression trees - Bayesian negative bionomial regression mixed models - nested design in mixed models - random forest vs. GAMs - meta-analysis - complex models - classification (categorical response) and regression (continous response) trees - boosted regression trees - boosted regression trees iteratively fit models gradually increasing emphasis on observations that were initially poorly fit - ADMB and TMB\n\nmultivariate statistics\nexploratoryordination/clustering\ncross validation\n\n[[Principle Component Analysis]] linearly transforms multivariate data into a new coordinate system where the majority of the variation in the data is captured with fewer dimensions than the initial data - Li et al. 2023\n[[Principle Component Analysis|PCA]] constructs a “map” of the samples where samples that are more similar are closer together\n\nAcknowledgements\nTBA",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "handbookSMIntro.html#footnotes",
    "href": "handbookSMIntro.html#footnotes",
    "title": "Statistical Modelling Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhere, we can also use the term “research hypothesis” instead of explanation and you’ll see we quickly switch to using this term↩︎\nMath skills have often been underemphasized in many Biology educations. This has not been helpful or necessary. Biologists need math and are very capable at applying math to solve problems, but the math needs to be useful; that is, math that you can apply to your own needs as you research biology. The DSP Program aims at providing statistical modelling tools that will be useful to you as a biologist. It is a happy coincidence that the same skills can be applied to a lot of other situations as well. The programming and statistics skills you are learning here will be useful to you in the future - in your future courses, thesis-writing and a wide range of careers.↩︎\nmore on causal vs. correlative explanations coming soon↩︎\nuseful predictions will always include uncertainty↩︎\nand these will be educated guesses!↩︎\nDon’t worry if these terms don’t mean anything to you yet - more to come!↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Statistical Modelling Handbook"
    ]
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Contacts and Feedback for the DSP Program",
    "section": "",
    "text": "The DSP Program is a joint venture by colleagues across AU’s Department of Biology.\nProgram development is led by the DSP Program Taskforce:\n\nAnna B. Neuheimer (Aquatic Biology, Taskforce head)\nRobert Buitenwerf (Ecoinformatics and Biodiversity)\nAlejandro Ordonez Gloria (Ecoinformatics and Biodiversity)\nTove Hedegaard Jørgensen (Genetics, Ecology and Evolution & Centre for Educational Development)\nIan Marshall (Microbiology)\nBirgit Olesen (Aquatic Biology & Arctic Research Centre)\nPeter Teglberg Madsen (Zoophysiology)\nJesper Givskov Sørensen (Genetics, Ecology and Evolution)\n\n\n\nSend your questions and feedback to the DSP Program Taskforce\nWe welcome your questions and feedback. Please submit them using this form or send an email to abneuheimer@bio.au.dk."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html",
    "href": "DSPPH_SM_HypothesisTesting.html",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "",
    "text": "In this section you will learn:\n\n\n\n\n\n\nhow statistical models can be used to test your hypothesis by judging the evidence for your model\nabout methods to judge the evidence for your model\nto use the model selection method to judge the evidence for your model and test your hypothesis"
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#what-is-a-p-value",
    "href": "DSPPH_SM_HypothesisTesting.html#what-is-a-p-value",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "What is a P-value",
    "text": "What is a P-value\nThe P-value is used for null-hypothesis significance testing (NHST) (@Muff2022). The “P” in P-value stands for probability - the probability of observing an outcome given that the null hypothesis is true (@Muff2022; @Popovic2024). Remember that null-hypothesis tests assume that the tested effect is zero. In the case of hypothesis testing, the null hypothesis test assumes a coefficient describing the effect of a predictor on your response is zero.\nIn the case of hypothesis testing, the null hypothesis you are testing against is that a predictor’s coefficient is zero. So, the P-value associated with the hypothesis testing tells you the probability of getting a coefficient of the value you got even though the coefficient is in fact zero.\nWhen the P-value is very low, we say that there is evidence that the coefficient is not zero, i.e. evidence that your predictor has an effect on your response. By convention, we say a P-value is low if P &lt; 0.05; meaning that the evidence comes with a 5% probability that the coefficient is actually zero.\nFirst, let’s describe how this works in general, and then look at an example:\nTo determine a P-value associated with a model coefficient, the null-hypothesis testing estimates something called a test statistic based on the coefficient’s estimate and the error around it. This test statistic is assumed to come from a certain data distribution (the exact distribution will vary based on your model structure)\nLet’s look at an example using our model fit to the hypothesis WtChange ~ Prey + 1. By using summary() on our model, we get\n\nsummary(startMod) # look at our validated starting model\n\n\nCall:\nglm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.85062  -0.23669  -0.04888   0.25828   0.83052  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.996353   0.158745  -44.07   &lt;2e-16 ***\nPrey         0.079912   0.002557   31.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1414749)\n\n    Null deviance: 147.8242  on 69  degrees of freedom\nResidual deviance:   9.6203  on 68  degrees of freedom\nAIC: 65.728\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe coefficients table shows us that the Intercept was estimated as -7 ± 0.16 g and the slope associated with Prey10 is 0.08 ± 0.0026 \\(g \\cdot m^{3}\\cdot num^{-1}\\).\nFor each coefficient, you can see t-statistic (called t value in the table) and P-value (called Pr(&gt;|t\\) in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g. for the intercept in the example, -7/0.16 = -44.07). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the P-value. When P-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero11, and that the predictor associated with the coefficient can be included in our model (i.e. the predictor is explaining a significant amount of our response variability).\nHow to estimate P-values for your model\nThe output from the summary() function quickly becomes limiting when you have more than one predictor. Instead, you can use the anova() function to estimate the P-values associated with each model term.\nHere’s an example for our model testing WtChange \\(\\sim Prey + 1\\):\n\nanova(startMod, # model object\n      test = \"F\") # type of null hypothesis test to perform\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: WtChange\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev      F    Pr(&gt;F)    \nNULL                    69     147.82                     \nPrey  1    138.2        68       9.62 976.88 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote here that you need to indicate what type of null hypothesis testing you want:\n\nuse the F-test for error distribution assumptions like normal or gamma (i.e. distributions where the scale parameter is estimated)\nuse the Chi-square test for error distribution assumptions like poisson or binomial (i.e. distributions where the scale parameter is fixed)\n\nThe result is a table where each predictor has a row to report the results of the null hypothesis test. Here we see that there is strong evidence the coefficient associated with Prey is not zero (P &lt; \\(2.2 \\cdot 10^{-16}\\)).\nTwo more notes about using P-values:\n\nnote in the table above that it says “Terms added sequentially (first to last)”. This indicates that the coefficients of the predictors are tested by adding each predictor one at a time to the model, estimating the coefficient associated with the predictor, and testing the null hypothesis that the coefficient is not different than zero. This process is problematic when you have even a moderate amount of predictor collinearity. This is a big reason to prefer the model selection method of hypothesis testing that we outline below.\nBecause of the issues interpreting P-values, it is better to talk about what P-values tell you about the evidence for your hypothesis, rather than a strict idea of rejecting or not your hypothesis. Here is an illustration of how to interpret your P-values12:\n\n\n(from @Muff2022)"
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#limitations-of-p-values",
    "href": "DSPPH_SM_HypothesisTesting.html#limitations-of-p-values",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Limitations of P-values",
    "text": "Limitations of P-values\nAs mentioned in the previous section, problem with the P-value method of testing your research hypothesis comes when you have more than one predictor in your hypothesis. Correlation among your predictors13 means that it is difficult to trust your coefficient estimates. This means that you can not use the P-values as a way to determine which coefficients are significantly different than zero when you have correlated predictors. Said another way, your assessment of whether a predictor is useful in your model will be uncertain if you have correlated predictors. And correlated predictors are very common. For this reason, we will be hypothesis testing using model selection."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#what-is-model-selection",
    "href": "DSPPH_SM_HypothesisTesting.html#what-is-model-selection",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "What is model selection",
    "text": "What is model selection\nCompare the following two models:\n\nWtChange \\(\\sim \\beta_1\\cdot Prey + \\beta_0 + error\\)\nWtChange \\(\\sim \\beta_0 + error\\)\n\nNote that model 2 is obtained by making the coefficient of Prey (\\(\\beta_1\\)) equal to 0 (i.e. if the effect of Prey on WtChange was zero). If you determined which of these two models better fits your data, you will know if \\(\\beta_1\\) is likely to be 0 and, thus, whether or not you have evidence that Prey can explain variation in WtChange."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#how-do-you-use-hypothesis-testing-for-model-selection",
    "href": "DSPPH_SM_HypothesisTesting.html#how-do-you-use-hypothesis-testing-for-model-selection",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "How do you use hypothesis testing for model selection",
    "text": "How do you use hypothesis testing for model selection\nThe steps involved in testing your hypothesis using model selection is\n\nform your candidate model set\nfit and rank models in your candidate model set\nchoose your best-specified model(s)\n\nLet’s walk through these now.\n\nForm your candidate model set\nYour candidate model set contains models with all possible predictor combinations14. So the candidate model set for WtChange \\(\\sim Prey + 1\\) is:\nWtChange \\(\\sim Prey + 1\\)\nWtChange \\(\\sim 1\\)\n\n\n\n\n\n\nAnother example\n\n\n\n\n\nHere is another example:\nif your hypothesis is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\\(Resp \\sim Cov1 + Cov2 + 1\\)\n\\(Resp \\sim Cov1 + 1\\)\n\\(Resp \\sim Cov2 + 1\\)\n\\(Resp \\sim 1\\)\nNote that the more predictors you have in your model, the bigger your candidate model set.\n\n\n\nHopefully you are starting to see that the difference among models in the candidate model set can be described by setting the coefficient associated with a predictor to zero. In this way, fitting and comparing the models in your candidate model set is a way of assessing the evidence for your hypothesis. This method is more robust to issues like predictor collinearity because you are assessing the evidence for a predictor’s effect on your response when each predictor is in a model alone and when it is in a model with other predictors.\nOne last note about your candidate model set: you must remember the biology when you form your candidate model set. There may be a biological reason why a certain model must not be included in your candidate model set (i.e. a model that defies biological logic). These should be excluded from your candidate model set. (@BurnhamAnderson2002).\n\n\nFit and rank models in your candidate model set\nEach model in the candidate model set is graded based on an estimate of the model’s “cost” vs. its “benefit”.\nThe model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) 15.\nThe model’s benefit is how well the model fits your data - i.e. how much of the variability in your response the model explains. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.16\n\n\n\n\n\n\nThe Principle of Parsimony\n\n\n\n\n\nThe principle of parsimony means that, when in doubt, you will choose the simpler explanation. This means that:\n\nmodels should have as few parameters as possible\nlinear models are preferred to non-linear models\nmodels with fewer assumptions are better\n\nThis said, there are times when you might choose a more complicated explanation over a simpler explanation. One example of this is when you prioritize a model’s ability to predict a future response vs. getting an accurate understanding of the underlying mechanisms. We will discuss this more in the upcoming section on Prediction.\n\n\n\nYou can fit and rank your models quickly using a function called dredge() in the MuMIn package17. The dredge() function fits and ranks models representing all possible predictor combinations based on your starting model - i.e. your default candidate model set. The output is a table ranking the models in your candidate model set.\nLet’s explore this now.\n\nlibrary(MuMIn) # load MuMIn package\n\noptions(na.action = \"na.fail\") # to avoid illegal model fitting\n\ndredgeOut &lt;- dredge(startMod) # create model selection table for validated starting model\n\nprint(dredgeOut)\n\nGlobal model call: glm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n---\nModel selection table \n  (Intrc)    Prey df   logLik  AICc  delta weight\n2  -6.996 0.07991  3  -29.864  66.1   0.00      1\n1  -2.238          2 -125.489 255.2 189.07      0\nModels ranked by AICc(x) \n\n\nNote the line:\n\noptions(na.action = \"na.fail\") # to avoid illegal model fitting\n\nThis is included because you need to make sure the data used to fit every model in your candidate model set stays the same. This could be violated if you have missing values in some of your predictor columns. This options() statement makes sure your model selection is following the rules.\nThe output of the dredge() function gives us\n\nthe Global model call (our original hypothesis), and\na Model selection table\n\nThe Model selection table contains one row for each model in our candidate model set. Let’s explore this now:\nFind the column called “(Intrc)”. This column tells us when the intercept is included in the model. If there is a number in that column, the associated model in your candidate model set (row) contains an intercept. Note that by default all models will contain an intercept18.\nFind the column called “Prey”. This column tells us when the Prey predictor is in the model. Notice the first row contains a number in the Prey column, while the second row is blank. This means that Prey is a predictor in the model reported in the first row but is missing from the model in the second. Note also that a number is recorded in the Prey column, row 1 (0.0799). This is the coefficient associated with the Prey predictor. Since we have a normal error distribution assumption, this coefficient can be considered the slope of a line19. If the predictor was a categorical predictor (vs. continuous predictor), a “+” would appear in the Model selection table when the categorical predictor was in the model.\nSo, in our example above, the model in the first row contains an intercept and the Prey - i.e. the first row is the model WtChange ~ Prey + 1. The model in the second row contains only an intercept - i.e. the second row is the model WtChange ~ 1.\nThe rest of the columns in the Model selection table contain information that help us rank the models.\n\nthe “df” column reports the number of model coefficients. Models that are more complicated (e.g. more predictors) will have a higher df as they require more coefficients to fit. Models with more terms are more “costly”. In the first row (WtChange ~ Prey + 1), df is 3 because the model fitting estimates a coefficient for Prey, for the intercept, and for the normal error distribution assumption (standard deviation). In the second row (WtChange ~ 1), df is 2 because the model fitting estimates a coefficient for the intercept, and for the normal error distribution assumption (standard deviation). So the model in the first row is more costly than the second row.\nthe “logLik” column reports the log-Likelihood of the model fit. The absolute value of this estimate will depend on the type of data you are modelling, but in general, the logLik is related to how much variation in your response the model explains. It can be used to compare models fit to the same data. This can be seen as a measure of the “benefit” of the model.\nthe “AICc” column reports information criteria for your models. Information criteria balances the cost (complexity) and benefit (explained variation) for your model. An example of information criterion is the Akaike Information Criterion (AIC). The AIC is estimated as:\n\n\\(AIC = 2\\cdot k - 2 \\cdot ln(L)\\)\nwhere \\(k\\) is the cost of the model (number of coefficients, like df above), and \\(L\\) is the maximum likelihood estimate made when the model was fit.\nThere are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes). The AICc is reported by default here, but you can control that in the dredge() function. In all cases, lower information criterion means more support for the model.\n\nthe “delta” (\\(\\Delta\\)) column is a convenient way to see how different each model’s AICc is from the model with the lowest AICc (\\(\\Delta AIC_i\\) is the change in AIC for model i vs. the model with the lowest AIC.)\nthe “weight” column reports Akaike weights for the model. The Akaike weights are a measure of the relative likelihood of the models. The sum of all the Akaike weights is 1, so we can get a relative estimate for the support for each model.\n\n\n\n\n\n\n\nAkaike weights\n\n\n\n\n\nHere is the equation to estimate the Akaike weights:\n\\[\nw_i = \\frac{exp(-\\frac{1}{2} \\cdot \\Delta AIC_i)}{\\sum_{r=1}^{R}exp(-\\frac{1}{2} \\cdot \\Delta AIC_r)}\n\\]\nwhere\n\n\\(w_i\\) is the Akaike weight for model i,\n\\(\\Delta AIC_i\\) is the change in AIC for model i vs. the model with the lowest AIC\n\\(\\Delta AIC_r\\) is the change in AIC for model r vs. the model with the lowest AIC. This is estimated for all models in the candidate model set (R models).\n\n@BurnhamAnderson2002\n\n\n\n\n\nChoose your best-specified model(s)\nUsing the model selection table, you can choose your best-specified model(s) and find out what it tells you about your hypothesis.\nIn general, your best-specified model will be the model with the lowest information criterion (e.g. AIC)20. This will be the model at the top of the model selection table.\nThat said, notice I write “best-specified model(s)” - possibly plural. This is because you might have models where the AIC estimates are very close to one another. A good rule of thumb is to report all models where the AIC is within 2 of the lowest AIC model (i.e. delta &lt; 2). Following @BurnhamAnderson2002,\n\n\n\nfor models where delta is\nthere is … for the model\n\n\n\n\n0-2\nsubstantial support\n\n\n4-7\nconsiderably less support\n\n\n&gt; 10\nessentially no support\n\n\n\nWith our example above:\n\nprint(dredgeOut)\n\nGlobal model call: glm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n---\nModel selection table \n  (Intrc)    Prey df   logLik  AICc  delta weight\n2  -6.996 0.07991  3  -29.864  66.1   0.00      1\n1  -2.238          2 -125.489 255.2 189.07      0\nModels ranked by AICc(x) \n\n\nwe have one best-specified model (model with substantial support):\nWtChange ~ Prey + 1 (AICc = 66.1)\nand essentially no support for the null model:\nWtChange ~ 1 (AICc = 255.2; delta = 189.1)\nWe can conclude that there is evidence that Prey explains variability in WtChange.\n\n\nWhat does your best-specified model(s) say about your hypothesis?\nModel selection is a way of hypothesis testing. So what does your best-specified model say about your hypothesis? By comparing your best-specified model to your starting model, you can see where there is evidence for the effects of each predictor, and where the effects are estimated to be zero.\nAs our best-specified model is\nWtChange ~ Prey + 1\nWe can conclude that there is evidence that Prey explains variability in WtChange.\n\n\n\n\n\n\nMore examples\n\n\n\n\n\nif our starting hypothesis was:\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\nA best-specified model of \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\) would indicate that we have evidence that there are effects of Cov1 and Cov2 on Resp and that the effect of Cov1 on Resp depends on Cov2 (an interaction).\nA best-specified model of \\(Resp \\sim Cov1 + Cov2 + 1\\) would indicate that we have evidence that there are effects of Cov1 and Cov2 on Resp but no evidence of an interaction effect.\nA best-specified model of \\(Resp \\sim Cov1 + 1\\) would indicate that we have evidence that there is an effect of Cov1 but not Cov2 on Resp.\nA best-specified model of \\(Resp \\sim 1\\) (i.e. the null hypothesis) would indicate that we have no evidence for effects of Cov1 or Cov2 on Resp. This is also a valid scientific result!\n\n\n\n\nIn the next section (on Reporting), we will discuss further how to communicate what your hypothesis testing results say about your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-p-values-1",
    "href": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-p-values-1",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Hypothesis testing using p-values",
    "text": "Hypothesis testing using p-values\nYou’ll remember from last week that you can use summary() on your starting model object to see the coefficients that were fit, along with their uncertainty (standard error) in the coefficients and a p-value. Let’s take another look with an example:\n\nIn the example, we are trying to explain variability in WtChange (g) with Prey \\(num \\cdot m^{-3}\\) by fitting a model to the hypothesis WtChange ~ Prey + 121 with a normal error distribution assumption and linear shape assumption. The coefficients table shows us that the Intercept was estimated as -10.2 +/- 3.6 g and the slope associated with Prey is 0.12 +/- 0.04 \\(g \\cdot m^{3}\\cdot num{-1}\\).\nFor each coefficient, you can see t-statistic (called t value in the table) and p-value (called Pr(&gt;|t\\) in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g. for the intercept in the example, -10.2/3.6 = -2.8). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the p-value. When p-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero, and that the covariate associated with the coefficient can be included in our model (i.e. the covariate is explaining a significant amount of our response variability).\n\nLimitations of p-values\nA problem with this method of testing your research hypothesis comes when you have more than one covariate in your hypothesis. As with Assumption #1 above, any correlation among your covariates will mean that you can not trust your coefficient estimates. This means that you can’t use the p-values as a way to determine which coefficients are significant when you have correlated covariates. Said another way, your assessment of whether a covariate is useful in your model will change if you have correlated covariates. And correlated covariates are very common."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-model-selection-1",
    "href": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-model-selection-1",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Hypothesis testing using model selection",
    "text": "Hypothesis testing using model selection\nAs mentioned above, an alternative method of testing your hypothesis is through model selection. Compare the following two models:\n\nWtChange \\(\\sim \\beta_1\\cdot Prey + \\beta_0 + error\\)\nWtChange \\(\\sim \\beta_0 + error\\)\n\nNote that model 2 is obtained by making \\(\\beta_1 = 0\\). If you determined which of these two models better fits our data, you will know if \\(\\beta_1\\) is likely to be 0 and, thus, whether or not Prey can explain variation in your response.\n\nThe candidate model set\nWe expand this idea from two models to all models in your “candidate model set”. This set are the models representing all possible covariate combinations. So if your hypothesis is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\\(Resp \\sim Cov1 + Cov2 + 1\\)\n\\(Resp \\sim Cov1 + 1\\)\n\\(Resp \\sim Cov2 + 1\\)\n\\(Resp \\sim 1\\)\nModel selection for hypothesis testing therefore involves:\n\nFirst, each model in your candidate model set is fit to your data.\nThen, each model is “graded” based on how well it is fit to the data and how complicated it is (more below).\nFinally, the models are ranked and you can choose the best-specified model.\n\n\n\nInformation criterion\nEach model in the candidate model set is graded based on an estimate of the model’s “cost” vs. “benefit”. The model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) 22, and the benefit is how well the model fits your data. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.\nThe cost-benefit information is combined to give each model a grade through a metric called “Information Criterion”. For example, Akaike Information Criterion23 is estimated as:\n\\(AIC = 2\\cdot k - 2 \\cdot ln(L)\\)\nwhere \\(k\\) is the cost of the model (number of parameters), and \\(L\\) is the maximum likelihood estimate made when the model was fit.24\nIn all cases, the model with the lowest AIC is our “best-specified” model, though we will discuss what should be done if you have two or more models that are equally “good” (i.e. within 2 AIC of the lowest AIC). Note that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.\n\n\nChoosing you best-specified model\n\nFor example, if your starting model is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is:\n1: \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n2: \\(Resp \\sim Cov1 + Cov2 + 1\\)\n3: \\(Resp \\sim Cov1 + 1\\)\n4: \\(Resp \\sim Cov2 + 1\\)\n5: \\(Resp \\sim 1\\).\nYou will fit each of these models and estimate their AIC, for example:\n1: \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\) (AIC = 60)\n2: \\(Resp \\sim Cov1 + Cov2 + 1\\) (AIC = 78)\n3: \\(Resp \\sim Cov1 + 1\\) (AIC = 20)\n4: \\(Resp \\sim Cov2 + 1\\) (AIC = 140)\n5: \\(Resp \\sim 1\\) (AIC = 234).\nYou then use this information to pick your best-specified model as the model with the lowest AIC25. Your best-specified model tells you how to interpret the evidence for your hypothesis. With the example above, you would conclude that the 3rd model (\\(Resp \\sim Cov1 + 1\\)) is your best-specified model (it has by far the lowest AIC), indicating that \\(Cov1\\) explains variablity in \\(Resp\\), but that there is no evidence that \\(Cov2\\) explains variability in \\(Resp\\) (as \\(Cov2\\) doesn’t appear in your best-specified model).\nModel selection like this can be easily done using the dredge() function in the MuMIn package. We’ll practice this in class."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#footnotes",
    "href": "DSPPH_SM_HypothesisTesting.html#footnotes",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ninference is the conclusion you make based on reasoning and evidence↩︎\nvs. categorical↩︎\nif you’re confused why we are choosing these assumptions, read the course notes section on Starting Model↩︎\nthese estimates are called parameters↩︎\nnote that some write this as “P value” and some as “p value” and some as “p-value”. There is no one rule. Just pick one and make it consistent through your text. I’ll try to do that here.↩︎\nfield↩︎\nsee more in your notes on Data curation and collection↩︎\nSee your notes on Model Validation for more on predictor collinearity↩︎\nbecause it is generally applicable↩︎\ni.e. the effect of Prey on WtChange↩︎\nfor example, a P value of 0.006 means that there is a 0.6% chance we would estimate the effect of Prey to be 0.12 \\(g \\cdot m^{3}\\cdot num{-1}\\) when it was in fact 0↩︎\nwe’ll come back to this in the Reporting section↩︎\nSee your notes on Model Validation for more on predictor collinearity↩︎\nthese are also called “nested” models as each model is “nested” in one of the other models when it only differs by one predictor. “Nested” is also used in experimental design to mean something totally different, so we will avoid using the term here.↩︎\nsee “The Principle of Parsimony” below↩︎\nsee the section on Starting Model↩︎\nnote the spelling and capitalization of this package name!↩︎\nindeed, your null model only contains an intercept↩︎\nand it is the same number given in the summary() output above. More on this coming up in the Reporting section!↩︎\nNote that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.↩︎\nrecall that the + 1 is to indicate that there is an intercept in our model. The +1 can be left out of the model formula and R will still estimate an intercept, but I will write it here for clarity↩︎\nsee section 9.3 and the “Principle of Parsimony”↩︎\nSection 9.17↩︎\nThere are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes).↩︎\nmodels within 2 of the lowest AIC are all chosen as the best-specified model↩︎"
  },
  {
    "objectID": "handbookIntro.html",
    "href": "handbookIntro.html",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.\n\n\n\nThere are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.\n\n\n\nThe ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is.",
    "href": "handbookIntro.html#what-this-handbook-is.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The DSP Program Handbook contains information on the DSP Program as well as resources for data collection, analysis and statistical modelling tasks.\nThe information in this handbook is provided as a resource for the AU Biology community including both those wanting to apply the skills in their own work, and those wanting to design data skills exercises consistent with the DSP Program.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#what-this-handbook-is-not.",
    "href": "handbookIntro.html#what-this-handbook-is-not.",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "There are many, many resources available for help with both programming and statistical modelling. Our intention is not to “reinvent the wheel”. Instead we aim to connect relevant data analysis and hypothesis testing strategies to your work as a biologist - both in class and out.\nThe methods contained in this handbook are not your only options. Where possible, we will give links to further information that can help if you would like to delve deeper on a subject.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#dsp-program-tools",
    "href": "handbookIntro.html#dsp-program-tools",
    "title": "Introduction to the Handbook",
    "section": "",
    "text": "The ideas and tools developed through the DSP Program are universal and not tied to a particular programming language. That said, most of our teaching takes place through the use of a scripted programming language.\n\n\nThe benefits of using a scripted programming language vs. ‘point & click’ programs (e.g. Excel, but see ### below) is that programming languages help make sure:\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.\n\nWith a programming language, your original data remains unchanged during your analysis and your work-flow is documented as a complete “recipe” of what you have done. This helps you and your colleagues understand and track what you are doing, it promotes experimentation and exploration, and reduces the potential for errors in the analysis. It will also allow you to learn from one project to another where you can often transfer your code to tackle new problems.\nIt is not enough for you to trust your own work. You have to work in a way that others can trust in your work as well. Programming languages help you do that.\n\n\nYour first language should be one that is\n\nrelevant (one that matches your immediate needs),\ncommon (one that is used by your community), and\nfree (one that doesn’t require an expensive license)\n\n\n\n\nOur starting point will be the R Programming Language.\nR is a scripted programming language and an environment for statistical computing and graphics. R provides a wide variety of statistical and graphical techniques, and can be extended to meet all sorts of needs. R is available for Windows, MacOS, and Linux.\nWe will start with R as i) R is heavily used in biological research already, ii) R is flexible and applicable to many tasks, iii) R is open-source and free, iv) R has an extensive community supporting new learners, and v) R is already taught in a number of AU courses.\nRegardless of the language chosen, the skills you gain learning your first programming language will help you learn any other languages you want to learn in the future. This is because learning a programming language involves learning (computational thinking, or how to break down a task into steps and communicate this to a computer)1. These skills are universal to all programming languages, as well as many of the tasks you need to pursue your biological research goals.\n\n\n\nOur advice is to learn one language deeply as it is much easier to switch languages after you have developed your computational skills. If you find yourself needing a more general purpose language, try Python or Julia.\n\n\n\n\nYou will quickly note that you will not only be learning R in the DSP Program. We will also go over skills for correctly using a spreadsheet editor (e.g. Microsoft’s Excel) in your work. This is because Biologists still use Excel for a large number of tasks (e.g. designing an experiment, data collection, budgeting), and many Biology graduates need to use Excel in some aspect of their future careers. Even though Excel is not a programming language, we will still be using best practices to ensure that\n\nyour analysis is kept separate from your data, and\nyour analysis is explicit and documented.",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "handbookIntro.html#footnotes",
    "href": "handbookIntro.html#footnotes",
    "title": "Introduction to the Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComputational thinking includes skills in decomposition (breaking down tasks into small steps), pattern recognition (observing patterns in tasks and data), abstraction (identifying and extracting relevant information, ignoring or removing unnecessary information), algorithms (creating an ordered set of instructions for solving a problem), modelling and simulation (statistical modelling for hypothesis testing, imitating processes and problems), and evaluation (determining the effectiveness of a solution, generalizing to apply the solution to a new problem) - adapted from digitalcareers.csiro.au.↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Introduction to the Handbook"
    ]
  },
  {
    "objectID": "DSPPH_DA_Merge.html",
    "href": "DSPPH_DA_Merge.html",
    "title": "So you want to: merge your data sets",
    "section": "",
    "text": "Often times we are interested in exploring connections among variables from different sources. Merging your data files can be a way of collecting all the variables of interest so you can explore research questions and hypotheses about the data.\nIn addition, merging can be used to label your observations. We go through examples below."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "href": "DSPPH_DA_Merge.html#in-the-base-package-using-merge",
    "title": "So you want to: merge your data sets",
    "section": "in the base package using merge()",
    "text": "in the base package using merge()\n\nmDat&lt;-merge(Dat1, # data frame to merge\n             Dat2, # other data frame to merge\n             by = c(\"ID\", \"Day\")) # merge by variable(s)\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n\n\nNote that you don’t need to have the same number of observations (rows) in your two data frames to merge. Merging can be a great way of labelling your data. Here’s an example:\nConsider a data frame with strain information for each of your organism IDs:\n\n## ID: organism ID\n## Strain: strain of organism\n\nstr(Dat3)\n\n'data.frame':   20 obs. of  2 variables:\n $ ID    : chr  \"id24\" \"id30\" \"id33\" \"id26\" ...\n $ Strain: chr  \"A\" \"C\" \"B\" \"C\" ...\n\n\nNote that Dat1 and Dat2 each contained 40 observations - one observation for each of 20 IDs made on each of 2 days.\nIn contrast Dat3 only has 20 observations - information about the strain for each of 20 IDs.\nBy using merge(), we can add the strain information to mDat:\n\nallDat &lt;- merge(mDat, # one data frame\n                Dat3, # the other data frame\n                by = \"ID\") # variables to merge by\n\nstr(allDat)\n\n'data.frame':   40 obs. of  5 variables:\n $ ID    : chr  \"id20\" \"id20\" \"id21\" \"id21\" ...\n $ Day   : num  1 2 1 2 1 2 1 2 1 2 ...\n $ Length: num  195 139 144 180 110 97 149 184 139 143 ...\n $ Temp  : num  6.1 7.2 8.7 5.3 6.9 7.5 7.2 7.1 8.1 7.1 ...\n $ Strain: chr  \"C\" \"C\" \"A\" \"A\" ...\n\n\nNow we have our observations labelled by the strain information!\nSome things to note:\n\nif you leave out the by = function totally, R will look for column names that are similar between the two data frames and use that for the merge.\nyou can designate that the “merge by” variables have different column names in the two data frames. This is done with the by.x = and by.y = arguments. Check ?merge for more.\nyou can control what happens to unmatched columns (e.g. if an ID appeared in only one of the two data frames). This is done with the all =, all.x =, and all.y = arguments. Check ?merge for more."
  },
  {
    "objectID": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "href": "DSPPH_DA_Merge.html#in-the-dplyr-package-using-full_join",
    "title": "So you want to: merge your data sets",
    "section": "in the dplyr package using full_join()",
    "text": "in the dplyr package using full_join()\nThe dplyr package includes the full_join() function as another way to merge your data frames\n\nlibrary(dplyr) # load dplyr package\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmDat&lt;-full_join(Dat1, # data frame to merge\n                Dat2, # other data frame to merge\n                by = join_by(ID, Day)) # merge by column\n\nstr(mDat) # take a look at the object I made\n\n'data.frame':   40 obs. of  4 variables:\n $ ID    : chr  \"id29\" \"id30\" \"id22\" \"id31\" ...\n $ Day   : num  1 2 1 2 1 1 2 1 2 2 ...\n $ Length: num  150 164 110 134 155 125 144 144 139 167 ...\n $ Temp  : num  7.4 7.3 6.9 7.5 5.6 7.8 7.8 6.3 7.9 7.1 ...\n\n\nSome things to note:\n\nThe full_join() function keeps all observations appearing in either data frame.\nThe left_join() function keeps all observations in the first data frame (Dat1) but you will lose any unmatched observations in the second data frame (Dat2).\nThe right_join() function keeps all observations in the second data frame (Dat2) but you will lose any unmatched observations in the first data frame (Dat1)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.\n\n\n\nThe Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions\n\n\n\n\n\n\n\nModules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#purpose",
    "href": "intro.html#purpose",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The goal of the DSP Program is to help students become more capable, confident and employable:\nCapable: The skills at the heart of the DSP Program represent those needed to pursue quantitative biological research. By developing these skills in our students, students will be more capable of successfully pursuing research projects both in their studies and post-graduate careers. Moreover, these skills are those required for data analysis tasks across fields. By developing their data skills during the program, students will be deemed more capable and attractive to a variety of data-focused careers. By focusing skills in an open-sourced programming language, students will be able to implement the tools they learnt after their degree is finished.\nConfident: A major focus of the DSP program is to make the students aware of the skills they are learning (the “why” and the “how”) and the general applicability of these skills across research projects, fields and careers. This includes repeated practice to a variety of biological-based research questions. By empowering students to see their abilities in these areas, we empower them to promote themselves when they pursue future research or career opportunities.\nEmployable: Students that are more capable and confident are more employable. The DSP program aims at increasing the skills, awareness and confidence of AU’s Biology students to increase employability, and in particular, allowing our students to access data-based careers that were previously off their, and the employers’, “radar”.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#learning-objectives",
    "href": "intro.html#learning-objectives",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "The Data Skills Portfolio participants will gain training in:\n\nComputational thinking - breaking down complex problems (decomposition), looking for similarities within and among problems (pattern recognition), identifying relevant information (abstraction), developing step-by-step solutions to a problem (algorithms)\nData handling and management - data acquisition, manipulation, exploration, visualization, and storage\nResearch ethics and transparency - data skills to support ethical research practices and transparent science (e.g. documenting science); communication of analysis choices and results, including standard graphical forms\nExperimental design - robust, ethical experimental design\nHypothesis testing - identifying the hypothesis, designing a model to test the hypothesis, assessing and communicating model fit and results.\nSkills marketing - communicating data skills, and marketing skills to a wide-range of career positions",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "intro.html#interested-in-contributing-to-the-dsp-program",
    "href": "intro.html#interested-in-contributing-to-the-dsp-program",
    "title": "What is the DSP Program?",
    "section": "",
    "text": "Modules are being designed to fit into existing course activities (and ECTS). Please contact the DSP Program Taskforce if you would like to co-develop a DSP module for your course.",
    "crumbs": [
      "What is the DSP Program?"
    ]
  },
  {
    "objectID": "handbookDAIntro.html",
    "href": "handbookDAIntro.html",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.\n\n\n\n\n\nA bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.\n\n\n\nHere we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.\n\n\n\n\nHere we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-tools",
    "href": "handbookDAIntro.html#a-note-on-tools",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "In general we will be encouraging you to do all data analysis skills using a scripted programming language like R. See ### here for more discussion about this, but in short, a scripted program language will allow you to:\n\nkeep your analysis separate from the data (and thereby safeguard the data),\nkeep a record of all your analysis steps (and thereby more easily find errors, make your methods reproducible and transparent), and\nallow you to use powerful tools to accomplish your analysis tasks.\n\nThat said, there may be times where you need to do analysis in a spreadsheet editor, like Excel. You will find the ideas presented below are still relevant. And if you do need to analyse data in a program like Excel, remember to:\n\nkeep your analysis separate from your data (e.g. in a separate sheet), and\nkeep a record of all your analysis steps. This is done by:\n\ncarefully and completely adding comments explaining your steps,\nand ensuring each cell contains only one step/piece of information.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "href": "handbookDAIntro.html#a-note-on-learning-your-first-programming-language",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "A bit on strategy when learning any programming language (we’ll also cover more strategies in class): it’s important to struggle but not for too long. Learning programming means learning computational thinking, or the logic behind breaking a problem down for a computer to solve. Struggling helps us learn this logic (ensuring we truly see the patterns in the code) but struggling too long can be an energy- and time-waster and may make us lose motivation for the process. The most successful path forward is a middle way: Read through this document, try to reproduce the examples and try the exercises, but if you’ve been staring at a problem for hours, it’s time to ask for help. Ask google, ask another R user, ask me, and if you don’t understand the answers you are given, ask again.\nDon’t worry about memorizing the details of this document or our discussions in class. You will always have reference material available to you (e.g. this document, the class notes, R’s help files, the internet). You can let memorization happen organically: Depending on your individual research adventures, you will use some of these tools more often than others and they will likely become committed to memory. Other tools will prove less useful to you. Memorizing this latter group would be a waste of time.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#the-basics",
    "href": "handbookDAIntro.html#the-basics",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will go over essential topics when you’re first learning a programming language.. Here we cover:\n\ninstalling R\nscripts\nbasic syntax\ngetting help in R (how to read help files!)\nobjects and data structures\netc.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "handbookDAIntro.html#so-you-want-to",
    "href": "handbookDAIntro.html#so-you-want-to",
    "title": "Data Analysis Handbook",
    "section": "",
    "text": "Here we will provide examples of how to accomplish common data analysis tasks.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Analysis Handbook"
    ]
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html",
    "href": "DSPPH_SM_Hypothesis.html",
    "title": "Statistical Modelling: Hypothesis",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nidentify your model terms (main effects and interactions)\npresent your research hypothesis in words and formula"
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html#hypotheses-with-more-than-one-predictor---including-interactions",
    "href": "DSPPH_SM_Hypothesis.html#hypotheses-with-more-than-one-predictor---including-interactions",
    "title": "Statistical Modelling: Hypothesis",
    "section": "Hypotheses with more than one predictor - including interactions",
    "text": "Hypotheses with more than one predictor - including interactions\nWhen you are considering more than one predictor, you need to consider the possibility of both main effects and interactions.\nA main effect represents the direct independent effect of the predictor on the response.\nAn interaction represents that the effect of one predictor on the response depends on the value of the other predictor.\nIf your research hypothesis states that\n\nVariability in the response (Resp) is explained by independent effects of two predictors (Pred1 and Pred2)1\n\nyour hypothesis in R syntax would be:\n\nResp ~ Pred1 + Pred2 + 1\n\nIf your research hypothesis states that\n\nVariability in the response (Resp) is explained by effects of two predictors (Pred1 and Pred2 with the effect of Pred1 depending on the value of Pred2)\n\nyour hypothesis in R syntax would be:\n\nResp ~ Pred1 + Pred2 + Pred1:Pred2 + 1\n\nNote that : is used to denote an interaction. In this case, it is a two-way interaction between Pred1 and Pred2.2\n\n\n\n\n\n\nTip\n\n\n\n\n\nR has a number of shortcuts for representing formulas in shortform. For example:\nResp ~ Pred1 + Pred2 + Pred1:Pred2 + 1\ncan be written as\nResp ~ Pred1*Pred2\ni.e. * means to include all main effects and all possible interactions.\nAnother example:\nResp ~ Pred1 + Pred2 + Pred3 + Pred1:Pred2 + Pred2:Pred3 + Pred1:Pred3 + 1\ncan be written as\nResp ~ (Pred1 + Pred2 + Pred3)^2\nwhich tells R to include all main effects and all possible two-way interactions between the three predictors."
  },
  {
    "objectID": "DSPPH_SM_Hypothesis.html#footnotes",
    "href": "DSPPH_SM_Hypothesis.html#footnotes",
    "title": "Statistical Modelling: Hypothesis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome jargon here: when the effects of each predictor is independent of other predictors, you say the model is “additive”↩︎\nNote: to include an interaction all predictors involved in the interaction have to have a main effect also included in the model↩︎"
  },
  {
    "objectID": "DSPPH_SM_Predictors.html",
    "href": "DSPPH_SM_Predictors.html",
    "title": "Statistical Modelling: Predictor(s)",
    "section": "",
    "text": "In this section you will:\n\n\n\n\n\n\nDefine your predictor variables (what could explain the variability in your response?)\nPresent possible mechanisms behind your argument (why might your predictor variables explain variability in your response?\n\n\n\n\n\nOften very quickly, you will start to have ideas about what mechanistically might be responsible for the variation in your response. This is exciting! This is where you can apply biological theory to form your research hypothesis of why observations are as they appear. This step needs you to be curious, creative, and tap into your foundation of biological theory. And this step is where you identify the predictors in your statistical model.\nHere your focus is on the biological mechanisms (or processes) that you expect affect change in your response variable. An example of a mechanism affecting plant height might be temperature-dependent growth as temperature controls the rate of enzymatic reactions involved in plant growth.\nOnce you have identified a possible mechanism, you can identify a measurable factor that can be used to quantify that mechanism. This is a predictor. To follow our example, a corresponding predictor to measure an effect of temperature-dependent growth is ambient temperature.\nIt is necessary also to spend some time thinking about how you measure your predictor - what measure is relevant to your response variable? Here think about how measures of your predictor can be relevant to the time and space resolution of your response variable. To complete our example, you will want to measure ambient temperature quite close to each plant, and need to consider not just the temperature on the day the plant height was measured, but throughout the growing period of the plant (e.g. by considering average or integrated temperature measures).\nSo your response variable is the variability you are trying to explain, and your predictor(s) is what you think is causing the variability. In this course, we will use the term “predictors” but note that they are also known as “covariates”, “factors”, “independent variables”, “explanatory variables”, or “x variables”.\nIt is important that you let yourself think freely when you are considering what might by causing the variability in your response variable. At this early stage, do not restrict yourself to what you will be able to measure and test - let your curiosity and ideas range freely (called “blue-sky thinking”). Think first about all the mechanisms that may be responsible for the response variability. Then think about all the ways observations may be limited (e.g. limitations in our ability to measure certain variables or access data from certain places or times). And take lots of notes! As you move on in the framework, you will quickly simplify your hypothesis into what is measured and what is testable, but all your exciting ideas will be used in to communicate the scope of your study, motivate your predictor choice (in your Introduction and Methods), to put your results into context of greater biological theory, as well as direct future study efforts to focus on variation in your response that remains unexplained (in your Discussion section). Spending some time allowing yourself to brainstorm at this point is time well spent.\nFinally, notice that throughout this section, we emphasized mechanisms. You want to develop and test a hypothesis that is grounded in biological mechanisms."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html",
    "href": "DSPPH_SM_ModelValidation.html",
    "title": "Statistical Modelling: Validation",
    "section": "",
    "text": "In this section you will learn:\n\n\n\n\n\n\nwhy model validation is necessary\nhow to determine if your starting model can be used to test your hypothesis\nwhat to do if your model is misspecified"
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#why-do-you-need-to-validate-your-starting-model",
    "href": "DSPPH_SM_ModelValidation.html#why-do-you-need-to-validate-your-starting-model",
    "title": "Statistical Modelling: Validation",
    "section": "Why do you need to validate your starting model?",
    "text": "Why do you need to validate your starting model?\nAs we discussed before “All models are wrong, but some are useful”. So how do you tell if your starting model is a useful one; that is, one that can be used to test your hypothesis?\nAs discussed in the last section, when you choose your starting model you make an educated guess as to what a useful starting model might be, but you can only validate that it is a useful starting model after you have fit the model to your data.\nA useful model is one that reflects the mechanistic understanding of your research hypothesis (the deterministic part of your model - your shape assumption) as well as the nature of your observations (the stochastic part of your model - your error distribution assumption)1.\nIn addition, a useful model is one that conforms to the assumptions of the method you use to test your model (e.g. GLM). The assumptions of the GLM include that your predictors are not correlated with one another, and that your observations are independent of one another.\nIn this section, we will explore each of these assumptions by considering if your:\n\npredictors are correlated with one another,\nobservations are independent of one another,\nerror distribution assumption was adequate, and\nshape distribution assumption was adequate.\n\nBy considering these four points, you can determine if your model is “well-specified” to test your hypothesis.\nIn this section, we will go over tools that will help you determine if your model is well-specified and what to do if you find yourself with a misspecified starting model.\n\n\n\n\n\n\n“A well-specified model”\n\n\n\n\n\nNote that you are trying to find “a well-specified model”. This terminology reflects the fact that more than one model2 is likely appropriate to test your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#a-useful-residual---the-scaled-residual",
    "href": "DSPPH_SM_ModelValidation.html#a-useful-residual---the-scaled-residual",
    "title": "Statistical Modelling: Validation",
    "section": "A useful residual - the scaled residual",
    "text": "A useful residual - the scaled residual\nAs mentioned above, inspecting scaled residuals gives us a method of validating our model that is generally applicable - GREAT - but it also is a method that is intuitive: The scaled residual method checks to see if your model is useful (valid) by seeing if it can even produce data that looks like the data you used to fit your model (i.e. your observations). A model that is well-specified will be able to simulate data that looks like the data used to fix it.\nWe will estimate and explore scaled residuals using functions in the DHARMa package.\n\n\n\n\n\n\nDHARMa’s scaled residuals\n\n\n\n\n\n\nThe DHARMa package uses a simulation-based approach to estimate scaled residuals. These scaled residuals are standardized to a uniform distribution regardless of the model structure.\nHere’s how it works:\n\nDHARMa uses your starting model to simulate new response data for each observation (each row in your data frame). The default is that it simulates 250 new data sets from your model6.\nDHARMa uses the simulated values at each observation to calculate the empirical cumulative density function (ECDF) of the simulated values at that observation.\nThe scaled residual for observed data point i is then defined as the value of the ECDF at the value of the observed data i (see figure to the right).\nEstimated this way, if your model is wellspecified, the scaled residuals will always follow a uniform distribution, regardless of your starting model structure. Put another way: if the observed data were created from the same data-generating process of your starting model, all values of the cumulative distribution should appear with equal probability and the DHARMa residuals will be distributed uniformly.\n\nYou can explore more about the DHARMa package here."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#types-of-observation-dependence-and-what-you-can-do-about-them",
    "href": "DSPPH_SM_ModelValidation.html#types-of-observation-dependence-and-what-you-can-do-about-them",
    "title": "Statistical Modelling: Validation",
    "section": "Types of observation dependence and what you can do about them",
    "text": "Types of observation dependence and what you can do about them\nIt is relatively common to find violations of this assumption14. Violations can happen from:\n\ngrouping of your observations by a variable not in your hypothesis15\nobservations that are made at different times (temporal autocorrelation)\nobservations that are made at different locations (spatial autocorrelation)\n\n\nGrouping - a missing predictor\nGrouping occurs when you have observations dependent on one another due to some other variable not in your hypothesis. This may be a site variable (e.g. garden plots), or a variable that impacts your measurement (e.g. observer, or gear type), or could be related to repeated measurements (e.g. sampling the same individual multiple times).\n\nAn example\n\nAnother example: you might be exploring the effect of added fertilizer on plant height with your hypothesis being Height ~ Fertilizer, with Fertilizer being a categorical predictor with levels of “Fertilized” or “Control”. In making your observations, you measure 24 plant heights growing in a Fertilized or Control site. Oh, and these sites happened to be organized across six experimental plots.\nGiven your hypothesis that Height ~ Fertilizer, each measurement of plant height will be treated as an individual observation (replicate) with the assumption that, other than the Fertilizer treatment, these observations are independent of one another. However, this assumption is violated as the plants from the same plots may be more similar than plants from different plots (e.g. sunlight differences across plots may influence plant height, or water drainage differences across plots may influence the effect of the fertilizer). The Plot variable may be grouping your observations.\n\n#### How you know if you have a problem with observations dependent based on a grouping variable\nYou can find out if observation dependence is influencing your model by inspecting the residuals of your starting model. Here, you plot your residuals vs. variables not in your model that may be causing dependence in the observations. If you have a problem with observation dependence, there will be a pattern in your residuals when plotted against the offending variable.\nHere is an example of how to do this with our generic example started above. You can see how the residuals differ across levels of the Other variable also in our data set using violin plots:\n\nggplot()+ # start ggplot\n  geom_violin(data = myDat,\n              mapping = aes(x = Other, y = Resid))+ # add observations as a violin\n  geom_point(data = myDat,\n             mapping = aes(x = Other, y = Resid))+ # add observations as points\n  xlab(\"Other variable\")+ # y-axis label\n  ylab(\"Scaled residual\")+ # x-axis label\n  labs(caption = \"Figure 3: A comparison of model residuals vs. other variable\")+ # figure caption\n  theme_bw()+ # change theme of plot\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n\nor with a points plot, by colouring the residuals based on the level in Other16:\n\nggplot()+ # start ggplot\n  geom_point(data = myDat, # the data frame\n             mapping = aes(x = Cont1, y = Resid, col = Other), # add observations as points\n             size = 3)+ # change the size of the points\n  xlab(\"Cont1\")+ # y-axis label\n  ylab(\"Scaled residual\")+ # x-axis label\n  labs(caption = \"Figure 3: A comparison of model residuals vs. other variable\")+ # figure caption\n  theme_bw()+ # change theme of plot\n  theme(plot.caption = element_text(hjust=0)) # move figure legend (caption) to left alignment. Use hjust = 0.5 to align in the center.\n\n\n\n\n\n\n\n\nNote that the spread of the residuals in each level (category) of Other is fairly equal in the violin plots and the colours are spread across the points plot. This indicates that Other is not causing much structure in the residuals, and likely is not causing the model to violate the assumption of independence.\nFinally, if you quantitative evidence that your observations are dependent on your grouping variable, you can test to see if residuals among the different groups have similar variances. This can be done with a Levene test for the homogeneity of variances through functions in the DHARMa package, e.g. \n\nlibrary(DHARMa) # load package\n\ntestCategorical(simOut, # the residuals \n                catPred = myDat$Other)$homogeneity # the grouping variable of concern\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   4  0.1112 0.9785\n      185               \n\nplotResiduals(simulationOutput = simOut, # compare simulated data to \n              form = myDat$Other, # our observations\n              asFactor = TRUE) # whether the variable plotted is a factor\n\n\n\n\n\n\n\n\nWhen your P-value is large (as it is here: P = 0.978), you would say that there is no evidence that the residual variance depends on the levels in Other (i.e. no concern for observations being dependent on Other).\n\n\nWhat can you do if your observations are grouped by something not in your hypothesis\nIf you find evidence of observation dependence, you can:\nAdd the grouping variable to your model as a (fixed effect) predictor: The assumption of observation independence means that your observations are independent of one another in all ways except for the predictors already included in your hypothesis. Therefore, an easy way to address observation dependence is to include the variable in your hypothesis, e.g. changing your original model:\nHeight ~ Fertilizer + 1\nto\nHeight ~ Fertilizer + Plot + Fertilizer:Plot + 1\nwould account for variability in plant height that was due to the plants growing in different plots (the main effect of Plot), as well as the influence of plot on the effect fertilizer has on the plants (the interaction Fertilizer:Plot).\nNote that adding Plot in this way is adding Plot as a “fixed effect” (vs. “random effect” - more on this below). In fact, every predictor we have been discussing up until now is in our model as something called a fixed effect.\nA couple of things to note when you add new fixed effects to your model:\n\nFixed effects influence the predicted mean of the model prediction, and so they are formerly part of your research hypothesis. This means that your hypothesis changes every time you add or remove a fixed effect. Your original hypothesis was\n\nHeight ~ Fertilizer + 1\nor that variability in plant height is explained by fertilizer addition. If you add Plot to your hypothesis to deal with observation dependence, your research hypothesis becomes:\nHeight ~ Fertilizer + Plot + Fertilizer:Plot + 1\nor that variability in plant height is explained by fertilizer addition, plot ID and the interaction between the two.\n\nWhen you add new fixed effects to your hypothesis, your model gets more “expensive” to fit. In the section on Hypothesis Testing, we will discuss how we can quantify the “benefit” of the model (increased explained deviance related to the likelihood of the model) vs. the cost of the model (how many coefficients have to be estimated). Adding new fixed effects increases the cost of fitting your model as more coefficients need to be estimated. This increased cost means you need bigger datasets (more observations) to fit the model. When you add a new continuous predictor to your hypothesis, there is one more coefficient to estimate (in a linear model, this is a slope). When you add a new categorical predictor to your hypothesis, you need to estimate another coefficient for each level in your new predictor. In our example, you would need to estimate 6 - 1 = 517 new coefficients to add the main effect of the Plot variable to the model, and another 6 - 1 = 5 coefficients to add the Fertilizer:Plot interaction to the model.\n\nBoth points above (that your hypothesis will change, and that your model will be more expensive to fit) means that just adding your grouping variable to deal with observation dependence is not always a great option. Instead you could,\nAdd the grouping variable to your model as a random effect using a mixed model: Many turn to mixed modelling as a way to deal with observation dependence. Mixed modelling18 is called “mixed” modelling because it includes a mix of both fixed effects (predictors that influence the model predicted mean) and random effects (predictors that influence the model predicted variance). Fitting mixed models is beyond the scope of this course, but I wanted you to have an idea of what mixed modelling is, and how to get started with mixed modelling, in case you want to use these methods in the future.\nTo fit mixed models in R, you need to adjust your hypothesis formula to tell R which predictors should be treated as fixed and which should be random. Recall that your model with only fixed effects:\nHeight ~ Fertilizer + Plot + Fertilizer:Plot + 1\nindicates that the predicted mean height is affected by fertilizer, plot ID and the interaction between the two, with separate coefficients associated with each plot ID. In mixed modelling, you could instead include Plot with:\nHeight ~ Fertilizer + (1|Plot) + (Fertilizer|Plot) + 1\nThe (1|Plot) term will estimate one (not 5) extra coefficient describing how the variance in average height (the intercept) varies when you move from plot to plot. The (Fertilizer|Plot) term will estimate one (not 5) extra coefficient describing how the variance in the effect of the fertilizer varies by plot.\nSo mixed modelling offers a way to address dependence in your observations that does not change your research hypothesis (as you are not adding fixed effects) and is cheaper (requiring less coefficient estimates as the random effects influence the modelled variance, not the mean).\nYou can fit mixed models in the lme4 package using the glmer() function (which stands for Generalized Linear Mixed Effects Models, or GLMM), with syntax that is very similar to what we have been using for GLMs, e.g.\n\nstartMod &lt;- glmer(formula = Height ~ Fertilizer + (1|Plot) + (Fertilizer|Plot) + 1,\n                  data = myDat,\n                  family = Gamma(link = \"inverse\"))\n\nWe won’t explore this further here.\nOne final thing to note is that random effects will always be categorical. If you have a continuous variable that is causing dependence in your observations, this variable can not be included as a random effect and must be included as a fixed effect.\nHere is a table if you are trying to determine if a variable causing dependence in your model should be included as a fixed or random effect:\n\n\n\n\n\n\n\nYour situation\nYour choice\n\n\n\n\n- the variable causing dependence is continuous- if you are interested in effect sizes of the variable on your response- if the factor levels are informative (vs. just numeric labels)\nfixed effect\n\n\n- the levels in the variable are only samples from a population of levels- if you have enough levels (at least 5) to estimate the variance of the effect due to your variable\nrandom effect\n\n\n\n(adapted from Crawley2013TheRBook)\n\n\n\nTemporal autocorrelation\nTemporal autocorrelation occurs when you measure your observations at different points in time.\nObservations collected closer together in time will be more similar than those collected further apart in time, and this could be happening independent of the mechanisms underlying your hypothesis. This topic is beyond the scope of this course, but I add a short description here so the idea is “on your radar” as you move forward in biology.\n\nAn example\nSuppose you want to test the hypothesis that variability in growth rate of newly hatched cod (torsk) in the Kattegat is explained by prey availability (Growth ~ Prey). The observations you are using to test this hypothesis have been collected over ~15 years, and it is likely that measurements made closer to each other in time are more similar than those made further apart from each other in time. This may be because the physical or biological environment was more similar in years that are close to each other in time (e.g. the parent population was similar, the temperature was similar) However, there is nothing in your hypothesis (Growth ~ Prey) to let R know which observations are close to each other in time. Thus, you violate the assumption of observation independence.\n\nWhen a model is fit to data and the observations are dependent on their sampling time, similar values closer in time are given too much weight on the model coefficients and the model fit is biased (compare lines in the plot on the right).\n\n\nHow you know if you have a problem with temporal autocorrelation\nYou can find out if observation dependence due to temporal autocorrelation is influencing your model by plotting your model residuals vs. time. If you have a problem with observation dependence, there will be a pattern in your residuals when plotted against the offending variable - in this case time.\nYou can also determine how large the problem of temporal autocorrelation is by estimating the autocorrelation function19 and the Durbin-Watson test. An easy way to test the latter is with the testTemporalAutocorrelation() function in the DHARMa package.\n\n\nWhat you do if your observations are influenced by temporal autocorrelation\nAs above, the assumption of observation independence means that your observations are independent of one another in all ways except for the predictors included in your hypothesis. We need to tell R about this temporal dependence in your model. This can be done by including time as a predictor in your model. Note that time will need to be a fixed effect as it is continuous, and will need to be modelled with a non-linear shape assumption20 to deal with the complicated form of the temporal autocorrelation.\nAlternatively, you can tell R about the correlation structure of the data directly (i.e. how similarity in observations changes as observations are further and further away from each other).\n### Spatial autocorrelation\n\nSimilar to the previous section, spatial autocorrelation describes the dependence among observations that are collected at different spatial locations. Observations measured close to each other in space are more (or less!) likely to be similar to one another than those measured further apart. In the plot on the right you can see examples of observation dependence on space in the dispersed and clustered drawings. In the dispersed example, observations closer to each other in space are less likely to resemble each other than one would expect if observations were distributed randomly in space (random example). In the clustered example, observations closer to each other in space are more likely to resemble each other than one would expect if observations were distributed randomly in space.\n\n#### An example\nFor example, you might be interested in how abundance of a species changes with mean environmental temperature and intend on testing the hypothesis that Abundance ~ Temperature. Measuring abundance over a large spatial area, you find that observations made closer to each other in space are more similar than those measured farther apart and part of this is due to effects other than temperature (e.g. other aspects of the environment such as food availability are more similar to each other for sites that are closer together). Without telling R information about where in space the observations were collected (remember, the hypothesis only includes Temperature), you violate the assumption of observation independence.\n\n\nHow you know if you have a problem with spatial autocorrelation\nYou can find out if observation dependence due to spatial autocorrelation is influencing your modelling by plotting your model residuals vs. location. As location is measured in two dimensions, you could try a bubble plot21 or variogram22 to help you look for patterns in your residuals with space. You can also estimate how big the problem of spatial autocorrelation is with Moran’s I test. The last can be done with the testSpatialAutocorrelation() function in the DHARMa package.\n\n\nWhat you do if your observations are influenced by spatial autocorrelation\nSimilarly to our discussion about temporal autocorrelation, the location of the observations (e.g. latitude and longitude) can be included in your model to account for the spatial autocorrelation23. Alternatively, the dependence among observations due to proximity can be included in the model as a spatial autocorrelation structure. \n\n\n\nA final point about observation independence\nRemember that you only have to worry about your observations being dependent based on variables not already in your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#footnotes",
    "href": "DSPPH_SM_ModelValidation.html#footnotes",
    "title": "Statistical Modelling: Validation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee course notes from last week if this is unclear↩︎\nbut not all models↩︎\nnotice I say “can be defined as…”. This is because there are many different definitions of a residual that have been made to deal with different model structures. We are going to use a very useful one - the scaled residuals - that will let you explore residuals for a wide range of models. More on this below!↩︎\nincluding those predictors inside your model, and possible predictors not included in your model. More on this to come!↩︎\nincluding those predictors inside your model, and possible predictors not included in your model. More on this to come!↩︎\nthis can be adjusted if needed↩︎\nsometimes also called multicollinearity↩︎\nremember, these are the modelled effects of your predictors on your response↩︎\nyour predictors are so correlated, in fact, we say they are “aliased” with one another↩︎\nthe error (variance) around the coefficient estimates is getting bigger (inflation)↩︎\nthe function we will use to calculate this will choose the correct estimate - either VIF or GVIF - depending on your model.↩︎\nthe model can not have interactions when estimating VIFs because an interaction term will always be correlated with the main effect predictor terms involved in the interaction. Ask in class if you have questions about this.↩︎\nin your own work, you will choose the predictor to remove based on your study goals and particular situation↩︎\nalso called pseudoreplication↩︎\nalso called nested sampling↩︎\nhere I plot the residuals vs. Cont1 but it could also be vs. the fitted values↩︎\nremember with categorical predictors, one factor level is included in the intercept↩︎\nalso called “multi-level modelling” or “hierarchical modelling”↩︎\ne.g. using the acf() function in the base stats package in R↩︎\nwe’ll talk about non-linear models in a couple of weeks↩︎\ncheck the sp package for more↩︎\ncheck the sgeotest package for more↩︎\nthis would be done with a non-linear model↩︎\nA quantile defines a particular part of a data set, e.g. the 90% quantile indicates the value where 90% of the values are less than the 90% quantile value. See your notes DSPH_SM_DataDistributions.html for more↩︎\n“Deviation n.s.” on the plot means that there is no significant deviation from expected↩︎\nNote that zero-inflated data (more zeroes than expected) appears similarly↩︎\nfrom the DHARMA package vignette↩︎"
  },
  {
    "objectID": "handbookDCIntro.html",
    "href": "handbookDCIntro.html",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Data curation is the collection and on-going management of data.\nGood data curation helps maintain data quality, and allows you and others to discover, access, and re-use your data. Data curation skills then are essential for ethical, transparent and reproducible science, and are an important part of your contribution to the field of biology.\n\n\nDepending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.\n\n\n\nThe individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns R can handle that, but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)\n\n\n\n\nFor more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "href": "handbookDCIntro.html#experimental-vs.-observational-studies",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "Depending on your research question, hypothesis and methods available, you may collect data (observations) from an experimental (e.g. in a lab) or observational study (e.g. in the field).\nBoth strategies have advantages and disadvantages. For example, experimental studies allow you to control many of the factors that may influence your study outside your research hypothesis. Experimental studies also allow you to more easily test every combination of predictors allowing you to better disentangle effects of multiple predictors on your response. In contrast, experimental studies are limited in their ability to represent the “real world” while field studies allow you to explore your hypothesis in situ.",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#data-collection",
    "href": "handbookDCIntro.html#data-collection",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "The individual needs of your project will vary, but here we will present a standard way of structuring your data and data collection that will support robust and ethical research. Specifically we will cover:\n\nChoosing what you record\nChoosing how you record it\nNaming your columns and data files\nIncluding a “ReadMe” document\nAsking questions\nBacking up your data\n\nA note on tools: You will most likely be collecting your data in a spreadsheet editor like Excel1, but these guidelines are generally applicable for however you are collecting your data.\n\n\nYou will need to record observations of each predictor and response. You should include comments made during the data collection noting unusual conditions (e.g. weather, equipment issues) that might affect your observations (as a comments column). When in doubt record, record, record.\n\n\n\nBest practices is to record your data in “unique record format”(also called “long format”). Unique record format makes it easier to track how you have collected the data, it makes it easier for two or more researchers to collaborate on the data collection, and it reduces the chance of errors in your data collection.\nRecording your data in unique record format means that:\n\nevery column is a variable with one format (either numeric or character),\nevery row is an observation,\nevery cell holds only one piece of information (i.e. a single value), and\nyou record your data in a rectangular table: this means that all columns have the same number of rows. If needed, you can fill missing information with a value to show it is missing (e.g. using NA which is what R uses to indicate a missing value)\n\nIn addition - you should make sure there is no extra white space (e.g. blank rows) above or to the left of your data.\n\nit is most flexible to record dates as separate columns for year, month and day, and, similarly, record times as separate columns of hours and minutes. The handbook goes over working with with complicated dates columns R can handle that, but having separate columns for each variable is easiest.\nrecord a decimal using “.” not “,” as this will be most consistent for the data analysis tools you will be using. Again, there are ways of working around this in R if your data does use commas as decimals.\n\n\n\n\nMake your variable (column) names simple but informative.\nAvoid spaces in your column names - instead you can include multiple words in your column names by using “snake_case” (e.g. total_length).\nAvoid having units in your column names (use a readme file instead, see below)\n\n\n\n\nInclude a useful name for your data file that describes the type of data in the file and the project or person related to the project, e.g. “ChristensenMSc_EnviroData.csv”\nNote that you will likely record your data in a spreadsheet editor resulting in a file with e.g. .xlsx extension. That said, you may want to save your data as a plain text file (e.g. with .txt or .csv extension). This is because a plain text file is the most “portable” across platforms (e.g. Windows vs. Mac) and into the future. Also, this format avoids Excel reformatting data columns that can cause confusion with interpretation. You can change how your file is saved using “Save as” in the file menu.\nHere’s an example of best-practice data collection practice (i.e. unique record format) - it is a file called Pedersen_ExampleData.csv:\n\n\n\n\n\nNote that:\n1a) There is no white space above or to the left of the data.\n1b) Every column is a variable that is either a numeric or a character.\n1c) Each row is a unique observation.\n1d) Each cell holds only a single piece of information.\n1e) The data are recorded in a regular table (no missing rows or columns).\nIn contrast, here is an example of what NOT to do:\n Note in this not-great-practice example:\n2a) There is white space above the data.\n2b) Some columns contain a mix of numeric and character data. Note that it is better to give information on units (e.g. g) in the ReadMe file (see next section).\n2c) Rows contain more than one observation.\n2d) Some cells hold more than one piece of information.\n2e) The data are not recorded in a regular table (there are missing rows and columns).\n2f) A variable is not in its own column.\n2g) A column name contains white space.\n\n\n\n\nFor each data file, include a separate “ReadMe” document that describes the data. The ReadMe file documents (briefly) what the data are, where they came from, and how they can be reused. The document allows you to communicate your data to your colleagues and your future self.\nThis document should:\n\nbe a plain text or pdf document.\ninclude the name of the data file associated with the ReadMe document.\ninclude a description of where the data came from.\ninclude information on how the data can or can not be reused.\ninclude a contact person for the data and their contact details (e.g. email).\ninclude descriptions of each column, the type of data and the format of the data, including units.\ninclude a description of how missing values are recorded (e.g. NA).\n\nHere is an example of a ReadMe document called Pedersen_ExampleData_ReadMe.pdf to accompany the Pedersen_ExampleData.csv above:\n\n\n\n\n\n\n\n\nRemember to ask questions! Asking your questions is one of the most valuable contributions you can make to science. Asking your questions will help us clarify methods, develop better strategies and stop accidents before they happen.\n\nIf you are unsure how to do something - stop and ask!\nIf you are unsure why you should do something - stop and ask!\nIf something does not feel safe and comfortable - stop and ask!\n\nmore here\n\n\n\nRemember to back up your data on multiple sources. (more TBA)",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#where-to-go-from-here",
    "href": "handbookDCIntro.html#where-to-go-from-here",
    "title": "Data Collection & Curation Handbook",
    "section": "",
    "text": "For more ideas on best practices regarding data curation, try:\nWickham, H. . (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "handbookDCIntro.html#footnotes",
    "href": "handbookDCIntro.html#footnotes",
    "title": "Data Collection & Curation Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthough you may have projects where you are working with databases↩︎",
    "crumbs": [
      "The DSP Program Handbook",
      "Data Collection & Curation Handbook"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "The Data Skills Portfolio (DSP) Program is based at Aarhus University’s Department of Biology.\nThe DSP Program is designed to help our Biology students develop robust and useful quantitative skills for biological research and many fields beyond.\nHere you will find a description of the program along with DSP handbook materials.\nExplore and enjoy!\nAnd remember: your feedback is welcome!\n\n\n\nto the Data Skills Portfolio Program Handbook for\n\nData Collection & Curation\nData Analysis\nStatistical Modelling"
  },
  {
    "objectID": "index.html#draft-website",
    "href": "index.html#draft-website",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "The Data Skills Portfolio (DSP) Program is based at Aarhus University’s Department of Biology.\nThe DSP Program is designed to help our Biology students develop robust and useful quantitative skills for biological research and many fields beyond.\nHere you will find a description of the program along with DSP handbook materials.\nExplore and enjoy!\nAnd remember: your feedback is welcome!"
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Data Skills Portfolio Program",
    "section": "",
    "text": "to the Data Skills Portfolio Program Handbook for\n\nData Collection & Curation\nData Analysis\nStatistical Modelling"
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html",
    "href": "DSPPH_DA_DatesTimes.html",
    "title": "So you want to: work with dates and times",
    "section": "",
    "text": "You will often have a time component to your biological research and research hypotheses - i.e. you might want to explain variability over time.. As mentioned in the Data Collection and Curation section, it is a good idea to record the time of your observation as separate columns of year, month, day, etc. But you will also need to work with data collected by others where date (and sometimes time) information is together one column (variable)."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "href": "DSPPH_DA_DatesTimes.html#in-the-base-package",
    "title": "So you want to: work with dates and times",
    "section": "In the base package:",
    "text": "In the base package:\nIn the base package which is installed along with R1, you can use the as.Date() function to format your data as a date:\nConsider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nmyDat$Time &lt;- as.Date(x = myDat$Time, # the date and time column\n                      format = \"%Y/%m/%d %H:%M\") # describing the format of the date and time column\n\nhead(myDat) # examine the first few rows\n\n        Time Value\n1 2024-10-03    36\n2 2020-06-09    35\n3 2020-10-07    57\n4 2021-10-09    43\n5 2020-06-04    44\n6 2021-12-02    36\n\n\nYou can learn more about the formatting syntax with ?strptime. Note that the code above replaces the myDat$Time column with the new, formatted date and time information.\n\nYou can extract parts of the date and time column with functions like months() for months, years() for years, etc.\n\nFor example:\n\nmyDat$Months &lt;- months(myDat$Time) # extract only the months\n\nstr(myDat) # structure of the data frame\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : Date, format: \"2024-10-03\" \"2020-06-09\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: chr  \"October\" \"June\" \"October\" \"October\" ..."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "href": "DSPPH_DA_DatesTimes.html#using-the-lubridate-package",
    "title": "So you want to: work with dates and times",
    "section": "Using the lubridate package:",
    "text": "Using the lubridate package:\nThe lubridate package was created to make working with dates and times easier. There are still two steps to the process. You can repeat the steps above but now with the lubridate package.\nAgain, consider the data:\n\nmyDat &lt;- read.csv(\"DTData.csv\") # load in the data\n\nhead(myDat) # examine the first few rows\n\n             Time Value\n1 2024/10/3 10:32    36\n2   2020/6/9 0:17    35\n3 2020/10/7 18:38    57\n4 2021/10/9 11:42    43\n5   2020/6/4 1:40    44\n6 2021/12/2 20:42    36\n\n\nwhere the Dat$Time column gives the year, month, day, hour and minutes of an observation given in the Dat$Value column.\n\nYou can format the Dat$Time column as a date and time with:\n\n\nlibrary(lubridate) # load the lubridate package\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nmyDat$Time &lt;- ymd_hm(myDat$Time) # the date and time column\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  2 variables:\n $ Time : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value: int  36 35 57 43 44 36 47 39 38 50 ...\n\n\n\nYou can extract parts of the date and time column with functions like month() for months, year() for years, etc. Notice that the function names are not plural with the lubridate package.2\n\nFor example:\n\nmyDat$Months &lt;- month(myDat$Time) # extract only the months\n\nstr(myDat) # examine the structure of the data\n\n'data.frame':   15 obs. of  3 variables:\n $ Time  : POSIXct, format: \"2024-10-03 10:32:00\" \"2020-06-09 00:17:00\" ...\n $ Value : int  36 35 57 43 44 36 47 39 38 50 ...\n $ Months: num  10 6 10 10 6 12 3 4 11 11 ...\n\n\nMuch more is available in the lubridate packages, including determining durations and dealing with time-zones. Check the lubridate package “cheat sheet” for more information."
  },
  {
    "objectID": "DSPPH_DA_DatesTimes.html#footnotes",
    "href": "DSPPH_DA_DatesTimes.html#footnotes",
    "title": "So you want to: work with dates and times",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nno need to load a separate package↩︎\nBoth months() and month() will work with lubridate, but months() is also used to denote a duration of a month. See the cheat sheet for more.↩︎"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html",
    "href": "DSPPH_DA_MissingValues.html",
    "title": "So you want to: deal with missing values",
    "section": "",
    "text": "Missing values are important and interesting, and they can affect how functions can be used on your data."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#apply-functions-to-data-containing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Apply functions to data containing missing values",
    "text": "Apply functions to data containing missing values\nWe can apply functions to objects or their components when missing values are present by letting R know what we want to do with the missing values. For example, getting the overall mean of the chlorophyll column without telling R how to handle missing values:\n\nmean(ChlData$Chl)\n\n[1] NA\n\n\nvs. telling R to remove them with the na.rm = ... argument:\n\nmean(ChlData$Chl, na.rm = TRUE)\n\n[1] 32.42375\n\n\nNote that the data frame itself remains unchanged, but R ignores the NAs when calculating the mean. We can find out more about how a particular function handles missing values by looking at the function’s help file (e.g. ?mean)."
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "href": "DSPPH_DA_MissingValues.html#locating-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Locating missing values",
    "text": "Locating missing values\nWe can also find missing values using the is.na() function:\n\nis.na(ChlData$Chl)\n\n [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nand identify locations of missing values with:\n\nwhich(is.na(ChlData$Chl) == TRUE)\n\n[1] 5 8 9"
  },
  {
    "objectID": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "href": "DSPPH_DA_MissingValues.html#removing-missing-values",
    "title": "So you want to: deal with missing values",
    "section": "Removing missing values",
    "text": "Removing missing values\nFinally, we can remove all rows that are incomplete (i.e. containing any missing values) with the na.omit() function:\n\nhead(ChlData) # Original data frame\n\n  Station Year Month   Chl\n1     HL2 2007     2 16.07\n2     S27 2005    10 31.31\n3     HL2 2002    NA 40.38\n4     HL2 2001     2 32.00\n5     HL2 2001    10    NA\n6     S27 2007    10 29.71\n\nChlData &lt;- na.omit(ChlData) # Remove the NAs\n\nhead(ChlData) # Data frame without NAs\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nNote that above I reassign the output of the na.omit() function back to the name ChlData. This replaces the original data frame with the new data frame without missing values. I could also save it as a new object (with a new name) so the original is not overwritten.\n\n\n\n\n\n\nTip\n\n\n\n\n\nTake a look at the numbers that print out to the left of the data frame:\n\nhead(ChlData)\n\n   Station Year Month   Chl\n1      HL2 2007     2 16.07\n2      S27 2005    10 31.31\n4      HL2 2001     2 32.00\n6      S27 2007    10 29.71\n7      S27 2006     7 59.20\n10     HL2 2003     5 26.00\n\n\nThese are row names that were assigned when the data were read in. We can ignore row names but I wanted to to explain them as they can be distracting when one starts manipulating data frames. Unless we specify otherwise, rows are named by their original position when the data are read in to R, e.g. initially row #3 was assigned the name “3”, and row #4 was assigned the name “4”, etc. Since we’ve removed some rows with na.omit() above, the row names now skip from e.g. 2 to 4 (row 3 has been removed), but note that the 3rd row in the data frame can still be accessed with:\n\nChlData[3,]\n\n  Station Year Month Chl\n4     HL2 2001     2  32\n\n\nYou can choose your own row names with the rownames() function (or column names with the colnames() function) as well as with arguments in e.g. read.csv()."
  }
]
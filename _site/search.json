[
  {
    "objectID": "DSPPH_SM_ModelValidation.html",
    "href": "DSPPH_SM_ModelValidation.html",
    "title": "Statistical Modelling: Validation",
    "section": "",
    "text": "In this section you will learn:\n\n\n\n\n\n\nwhy model validation is necessary\nhow to determine if your starting model can be used to test your hypothesis\nwhat to do if your model is misspecified"
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#why-do-you-need-to-validate-your-starting-model",
    "href": "DSPPH_SM_ModelValidation.html#why-do-you-need-to-validate-your-starting-model",
    "title": "Statistical Modelling: Validation",
    "section": "Why do you need to validate your starting model?",
    "text": "Why do you need to validate your starting model?\nAs we discussed before “All models are wrong, but some are useful”. So how do you tell if your starting model is a useful one; that is, one that can be used to test your hypothesis?\nAs discussed in the last section, when you choose your starting model you make an educated guess as to what a useful starting model might be, but you can only validate that it is a useful starting model after you have fit the model to your data.\nA useful model is one that reflects the mechanistic understanding of your research hypothesis (the deterministic part of your model - your shape assumption) as well as the nature of your observations (the stochastic part of your model - your error distribution assumption)1.\nIn addition, a useful model is one that conforms to the assumptions of the method you use to test your model (e.g. GLM). The assumptions of the GLM include that your predictors are not correlated with one another, and that your observations are independent of one another.\nIn this section, we will explore each of these assumptions by considering if your:\n\npredictors are correlated with one another,\nobservations are independent of one another,\nerror distribution assumption was adequate, and\nshape distribution assumption was adequate.\n\nBy considering these four points, you can determine if your model is “well-specified” to test your hypothesis.\nIn this section, we will go over tools that will help you determine if your model is well-specified and what to do if you find yourself with a misspecified starting model.\n\n\n\n\n\n\n“A well-specified model”\n\n\n\n\n\nNote that you are trying to find “a well-specified model”. This terminology reflects the fact that more than one model2 is likely appropriate to test your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#a-useful-residual---the-scaled-residual",
    "href": "DSPPH_SM_ModelValidation.html#a-useful-residual---the-scaled-residual",
    "title": "Statistical Modelling: Validation",
    "section": "A useful residual - the scaled residual",
    "text": "A useful residual - the scaled residual\nAs mentioned above, inspecting scaled residuals gives us a method of validating our model that is generally applicable - GREAT - but it also is a method that is intuitive: The scaled residual method checks to see if your model is useful (valid) by seeing if it can even produce data that looks like the data you used to fit your model (i.e. your observations). A model that is well-specified will be able to simulate data that looks like the data used to fix it.\nWe will estimate and explore scaled residuals using functions in the DHARMa package.\n\n\n\n\n\n\nDHARMa’s scaled residuals\n\n\n\n\n\n\nThe DHARMa package uses a simulation-based approach to estimate scaled residuals. These scaled residuals are standardized to a uniform distribution regardless of the model structure.\nHere’s how it works:\n\nDHARMa uses your starting model to simulate new response data for each observation (each row in your data frame). The default is that it simulates 250 new data sets from your model6.\nDHARMa uses the simulated values at each observation to calculate the empirical cumulative density function (ECDF) of the simulated values at that observation.\nThe scaled residual for observed data point i is then defined as the value of the ECDF at the value of the observed data i (see figure to the right).\nEstimated this way, if your model is wellspecified, the scaled residuals will always follow a uniform distribution, regardless of your starting model structure. Put another way: if the observed data were created from the same data-generating process of your starting model, all values of the cumulative distribution should appear with equal probability and the DHARMa residuals will be distributed uniformly.\n\nYou can explore more about the DHARMa package here."
  },
  {
    "objectID": "DSPPH_SM_ModelValidation.html#footnotes",
    "href": "DSPPH_SM_ModelValidation.html#footnotes",
    "title": "Statistical Modelling: Validation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee course notes from last week if this is unclear↩︎\nbut not all models↩︎\nnotice I say “can be defined as…”. This is because there are many different definitions of a residual that have been made to deal with different model structures. We are going to use a very useful one - the scaled residuals - that will let you explore residuals for a wide range of models. More on this below!↩︎\nincluding those predictors inside your model, and possible predictors not included in your model. More on this to come!↩︎\nincluding those predictors inside your model, and possible predictors not included in your model. More on this to come!↩︎\nthis can be adjusted if needed↩︎\nsometimes also called multicollinearity↩︎\nremember, these are the modelled effects of your predictors on your response↩︎\nyour predictors are so correlated, in fact, we say they are “aliased” with one another↩︎\nthe error (variance) around the coefficient estimates is getting bigger (inflation)↩︎\nthe function we will use to calculate this will choose the correct estimate - either VIF or GVIF - depending on your model.↩︎\nthe model can not have interactions when estimating VIFs because an interaction term will always be correlated with the main effect predictor terms involved in the interaction. Ask in class if you have questions about this.↩︎"
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html",
    "href": "DSPPH_SM_HypothesisTesting.html",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "",
    "text": "In this section you will learn:\n\n\n\n\n\n\nhow statistical models can be used to test your hypothesis by judging the evidence for your model\nabout methods to judge the evidence for your model\nto use the model selection method to judge the evidence for your model and test your hypothesis"
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#what-is-a-p-value",
    "href": "DSPPH_SM_HypothesisTesting.html#what-is-a-p-value",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "What is a P-value",
    "text": "What is a P-value\nThe P-value is used for null-hypothesis significance testing (NHST) (@Muff2022). The “P” in P-value stands for probability - the probability of observing an outcome given that the null hypothesis is true (@Muff2022; @Popovic2024). Remember that null-hypothesis tests assume that the tested effect is zero. In the case of hypothesis testing, the null hypothesis test assumes a coefficient describing the effect of a predictor on your response is zero.\nIn the case of hypothesis testing, the null hypothesis you are testing against is that a predictor’s coefficient is zero. So, the P-value associated with the hypothesis testing tells you the probability of getting a coefficient of the value you got even though the coefficient is in fact zero.\nWhen the P-value is very low, we say that there is evidence that the coefficient is not zero, i.e. evidence that your predictor has an effect on your response. By convention, we say a P-value is low if P &lt; 0.05; meaning that the evidence comes with a 5% probability that the coefficient is actually zero.\nFirst, let’s describe how this works in general, and then look at an example:\nTo determine a P-value associated with a model coefficient, the null-hypothesis testing estimates something called a test statistic based on the coefficient’s estimate and the error around it. This test statistic is assumed to come from a certain data distribution (the exact distribution will vary based on your model structure)\nLet’s look at an example using our model fit to the hypothesis WtChange ~ Prey + 1. By using summary() on our model, we get\n\nsummary(startMod) # look at our validated starting model\n\n\nCall:\nglm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.996353   0.158745  -44.07   &lt;2e-16 ***\nPrey         0.079912   0.002557   31.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1414749)\n\n    Null deviance: 147.8242  on 69  degrees of freedom\nResidual deviance:   9.6203  on 68  degrees of freedom\nAIC: 65.728\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe coefficients table shows us that the Intercept was estimated as -7 ± 0.16 g and the slope associated with Prey10 is 0.08 ± 0.0026 \\(g \\cdot m^{3}\\cdot num^{-1}\\).\nFor each coefficient, you can see t-statistic (called t value in the table) and P-value (called Pr(&gt;|t\\) in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g. for the intercept in the example, -7/0.16 = -44.07). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the P-value. When P-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero11, and that the predictor associated with the coefficient can be included in our model (i.e. the predictor is explaining a significant amount of our response variability).\nHow to estimate P-values for your model\nThe output from the summary() function quickly becomes limiting when you have more than one predictor. Instead, you can use the anova() function to estimate the P-values associated with each model term.\nHere’s an example for our model testing WtChange \\(\\sim Prey + 1\\):\n\nanova(startMod, # model object\n      test = \"F\") # type of null hypothesis test to perform\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: WtChange\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev      F    Pr(&gt;F)    \nNULL                    69     147.82                     \nPrey  1    138.2        68       9.62 976.88 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote here that you need to indicate what type of null hypothesis testing you want:\n\nuse the F-test for error distribution assumptions like normal or gamma (i.e. distributions where the scale parameter is estimated)\nuse the Chi-square test for error distribution assumptions like poisson or binomial (i.e. distributions where the scale parameter is fixed)\n\nThe result is a table where each predictor has a row to report the results of the null hypothesis test. Here we see that there is strong evidence the coefficient associated with Prey is not zero (P &lt; \\(2.2 \\cdot 10^{-16}\\)).\nTwo more notes about using P-values:\n\nnote in the table above that it says “Terms added sequentially (first to last)”. This indicates that the coefficients of the predictors are tested by adding each predictor one at a time to the model, estimating the coefficient associated with the predictor, and testing the null hypothesis that the coefficient is not different than zero. This process is problematic when you have even a moderate amount of predictor collinearity. This is a big reason to prefer the model selection method of hypothesis testing that we outline below.\nBecause of the issues interpreting P-values, it is better to talk about what P-values tell you about the evidence for your hypothesis, rather than a strict idea of rejecting or not your hypothesis. Here is an illustration of how to interpret your P-values12:\n\n\n(from @Muff2022)"
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#limitations-of-p-values",
    "href": "DSPPH_SM_HypothesisTesting.html#limitations-of-p-values",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Limitations of P-values",
    "text": "Limitations of P-values\nAs mentioned in the previous section, problem with the P-value method of testing your research hypothesis comes when you have more than one predictor in your hypothesis. Correlation among your predictors13 means that it is difficult to trust your coefficient estimates. This means that you can not use the P-values as a way to determine which coefficients are significantly different than zero when you have correlated predictors. Said another way, your assessment of whether a predictor is useful in your model will be uncertain if you have correlated predictors. And correlated predictors are very common. For this reason, we will be hypothesis testing using model selection."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#what-is-model-selection",
    "href": "DSPPH_SM_HypothesisTesting.html#what-is-model-selection",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "What is model selection",
    "text": "What is model selection\nCompare the following two models:\n\nWtChange \\(\\sim \\beta_1\\cdot Prey + \\beta_0 + error\\)\nWtChange \\(\\sim \\beta_0 + error\\)\n\nNote that model 2 is obtained by making the coefficient of Prey (\\(\\beta_1\\)) equal to 0 (i.e. if the effect of Prey on WtChange was zero). If you determined which of these two models better fits your data, you will know if \\(\\beta_1\\) is likely to be 0 and, thus, whether or not you have evidence that Prey can explain variation in WtChange."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#how-do-you-use-hypothesis-testing-for-model-selection",
    "href": "DSPPH_SM_HypothesisTesting.html#how-do-you-use-hypothesis-testing-for-model-selection",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "How do you use hypothesis testing for model selection",
    "text": "How do you use hypothesis testing for model selection\nThe steps involved in testing your hypothesis using model selection is\n\nform your candidate model set\nfit and rank models in your candidate model set\nchoose your best-specified model(s)\n\nLet’s walk through these now.\n\nForm your candidate model set\nYour candidate model set contains models with all possible predictor combinations14. So the candidate model set for WtChange \\(\\sim Prey + 1\\) is:\nWtChange \\(\\sim Prey + 1\\)\nWtChange \\(\\sim 1\\)\n\n\n\n\n\n\nAnother example\n\n\n\n\n\nHere is another example:\nif your hypothesis is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\\(Resp \\sim Cov1 + Cov2 + 1\\)\n\\(Resp \\sim Cov1 + 1\\)\n\\(Resp \\sim Cov2 + 1\\)\n\\(Resp \\sim 1\\)\nNote that the more predictors you have in your model, the bigger your candidate model set.\n\n\n\nHopefully you are starting to see that the difference among models in the candidate model set can be described by setting the coefficient associated with a predictor to zero. In this way, fitting and comparing the models in your candidate model set is a way of assessing the evidence for your hypothesis. This method is more robust to issues like predictor collinearity because you are assessing the evidence for a predictor’s effect on your response when each predictor is in a model alone and when it is in a model with other predictors.\nOne last note about your candidate model set: you must remember the biology when you form your candidate model set. There may be a biological reason why a certain model must not be included in your candidate model set (i.e. a model that defies biological logic). These should be excluded from your candidate model set. (@BurnhamAnderson2002).\n\n\nFit and rank models in your candidate model set\nEach model in the candidate model set is graded based on an estimate of the model’s “cost” vs. its “benefit”.\nThe model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) 15.\nThe model’s benefit is how well the model fits your data - i.e. how much of the variability in your response the model explains. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.16\n\n\n\n\n\n\nThe Principle of Parsimony\n\n\n\n\n\nThe principle of parsimony means that, when in doubt, you will choose the simpler explanation. This means that:\n\nmodels should have as few parameters as possible\nlinear models are preferred to non-linear models\nmodels with fewer assumptions are better\n\nThis said, there are times when you might choose a more complicated explanation over a simpler explanation. One example of this is when you prioritize a model’s ability to predict a future response vs. getting an accurate understanding of the underlying mechanisms. We will discuss this more in the upcoming section on Prediction.\n\n\n\nYou can fit and rank your models quickly using a function called dredge() in the MuMIn package17. The dredge() function fits and ranks models representing all possible predictor combinations based on your starting model - i.e. your default candidate model set. The output is a table ranking the models in your candidate model set.\nLet’s explore this now.\n\nlibrary(MuMIn) # load MuMIn package\n\noptions(na.action = \"na.fail\") # to avoid illegal model fitting\n\ndredgeOut &lt;- dredge(startMod) # create model selection table for validated starting model\n\nprint(dredgeOut)\n\nGlobal model call: glm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n---\nModel selection table \n  (Intrc)    Prey df   logLik  AICc  delta weight\n2  -6.996 0.07991  3  -29.864  66.1   0.00      1\n1  -2.238          2 -125.489 255.2 189.07      0\nModels ranked by AICc(x) \n\n\nNote the line:\n\noptions(na.action = \"na.fail\") # to avoid illegal model fitting\n\nThis is included because you need to make sure the data used to fit every model in your candidate model set stays the same. This could be violated if you have missing values in some of your predictor columns. This options() statement makes sure your model selection is following the rules.\nThe output of the dredge() function gives us\n\nthe Global model call (our original hypothesis), and\na Model selection table\n\nThe Model selection table contains one row for each model in our candidate model set. Let’s explore this now:\nFind the column called “(Intrc)”. This column tells us when the intercept is included in the model. If there is a number in that column, the associated model in your candidate model set (row) contains an intercept. Note that by default all models will contain an intercept18.\nFind the column called “Prey”. This column tells us when the Prey predictor is in the model. Notice the first row contains a number in the Prey column, while the second row is blank. This means that Prey is a predictor in the model reported in the first row but is missing from the model in the second. Note also that a number is recorded in the Prey column, row 1 (0.0799). This is the coefficient associated with the Prey predictor. Since we have a normal error distribution assumption, this coefficient can be considered the slope of a line19. If the predictor was a categorical predictor (vs. continuous predictor), a “+” would appear in the Model selection table when the categorical predictor was in the model.\nSo, in our example above, the model in the first row contains an intercept and the Prey - i.e. the first row is the model WtChange ~ Prey + 1. The model in the second row contains only an intercept - i.e. the second row is the model WtChange ~ 1.\nThe rest of the columns in the Model selection table contain information that help us rank the models.\n\nthe “df” column reports the number of model coefficients. Models that are more complicated (e.g. more predictors) will have a higher df as they require more coefficients to fit. Models with more terms are more “costly”. In the first row (WtChange ~ Prey + 1), df is 3 because the model fitting estimates a coefficient for Prey, for the intercept, and for the normal error distribution assumption (standard deviation). In the second row (WtChange ~ 1), df is 2 because the model fitting estimates a coefficient for the intercept, and for the normal error distribution assumption (standard deviation). So the model in the first row is more costly than the second row.\nthe “logLik” column reports the log-Likelihood of the model fit. The absolute value of this estimate will depend on the type of data you are modelling, but in general, the logLik is related to how much variation in your response the model explains. It can be used to compare models fit to the same data. This can be seen as a measure of the “benefit” of the model.\nthe “AICc” column reports information criteria for your models. Information criteria balances the cost (complexity) and benefit (explained variation) for your model. An example of information criterion is the Akaike Information Criterion (AIC). The AIC is estimated as:\n\n\\(AIC = 2\\cdot k - 2 \\cdot ln(L)\\)\nwhere \\(k\\) is the cost of the model (number of coefficients, like df above), and \\(L\\) is the maximum likelihood estimate made when the model was fit.\nThere are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes). The AICc is reported by default here, but you can control that in the dredge() function. In all cases, lower information criterion means more support for the model.\n\nthe “delta” (\\(\\Delta\\)) column is a convenient way to see how different each model’s AICc is from the model with the lowest AICc (\\(\\Delta AIC_i\\) is the change in AIC for model i vs. the model with the lowest AIC.)\nthe “weight” column reports Akaike weights for the model. The Akaike weights are a measure of the relative likelihood of the models. The sum of all the Akaike weights is 1, so we can get a relative estimate for the support for each model.\n\n\n\n\n\n\n\nAkaike weights\n\n\n\n\n\nHere is the equation to estimate the Akaike weights:\n\\[\nw_i = \\frac{exp(-\\frac{1}{2} \\cdot \\Delta AIC_i)}{\\sum_{r=1}^{R}exp(-\\frac{1}{2} \\cdot \\Delta AIC_r)}\n\\]\nwhere\n\n\\(w_i\\) is the Akaike weight for model i,\n\\(\\Delta AIC_i\\) is the change in AIC for model i vs. the model with the lowest AIC\n\\(\\Delta AIC_r\\) is the change in AIC for model r vs. the model with the lowest AIC. This is estimated for all models in the candidate model set (R models).\n\n@BurnhamAnderson2002\n\n\n\n\n\nChoose your best-specified model(s)\nUsing the model selection table, you can choose your best-specified model(s) and find out what it tells you about your hypothesis.\nIn general, your best-specified model will be the model with the lowest information criterion (e.g. AIC)20. This will be the model at the top of the model selection table.\nThat said, notice I write “best-specified model(s)” - possibly plural. This is because you might have models where the AIC estimates are very close to one another. A good rule of thumb is to report all models where the AIC is within 2 of the lowest AIC model (i.e. delta &lt; 2). Following @BurnhamAnderson2002,\n\n\n\nfor models where delta is\nthere is … for the model\n\n\n\n\n0-2\nsubstantial support\n\n\n4-7\nconsiderably less support\n\n\n&gt; 10\nessentially no support\n\n\n\nWith our example above:\n\nprint(dredgeOut)\n\nGlobal model call: glm(formula = WtChange ~ Prey, family = gaussian(link = \"identity\"), \n    data = myDat)\n---\nModel selection table \n  (Intrc)    Prey df   logLik  AICc  delta weight\n2  -6.996 0.07991  3  -29.864  66.1   0.00      1\n1  -2.238          2 -125.489 255.2 189.07      0\nModels ranked by AICc(x) \n\n\nwe have one best-specified model (model with substantial support):\nWtChange ~ Prey + 1 (AICc = 66.1)\nand essentially no support for the null model:\nWtChange ~ 1 (AICc = 255.2; delta = 189.1)\nWe can conclude that there is evidence that Prey explains variability in WtChange.\n\n\nWhat does your best-specified model(s) say about your hypothesis?\nModel selection is a way of hypothesis testing. So what does your best-specified model say about your hypothesis? By comparing your best-specified model to your starting model, you can see where there is evidence for the effects of each predictor, and where the effects are estimated to be zero.\nAs our best-specified model is\nWtChange ~ Prey + 1\nWe can conclude that there is evidence that Prey explains variability in WtChange.\n\n\n\n\n\n\nMore examples\n\n\n\n\n\nif our starting hypothesis was:\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\nA best-specified model of \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\) would indicate that we have evidence that there are effects of Cov1 and Cov2 on Resp and that the effect of Cov1 on Resp depends on Cov2 (an interaction).\nA best-specified model of \\(Resp \\sim Cov1 + Cov2 + 1\\) would indicate that we have evidence that there are effects of Cov1 and Cov2 on Resp but no evidence of an interaction effect.\nA best-specified model of \\(Resp \\sim Cov1 + 1\\) would indicate that we have evidence that there is an effect of Cov1 but not Cov2 on Resp.\nA best-specified model of \\(Resp \\sim 1\\) (i.e. the null hypothesis) would indicate that we have no evidence for effects of Cov1 or Cov2 on Resp. This is also a valid scientific result!\n\n\n\n\nIn the next section (on Reporting), we will discuss further how to communicate what your hypothesis testing results say about your hypothesis."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-p-values-1",
    "href": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-p-values-1",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Hypothesis testing using p-values",
    "text": "Hypothesis testing using p-values\nYou’ll remember from last week that you can use summary() on your starting model object to see the coefficients that were fit, along with their uncertainty (standard error) in the coefficients and a p-value. Let’s take another look with an example:\n\nIn the example, we are trying to explain variability in WtChange (g) with Prey \\(num \\cdot m^{-3}\\) by fitting a model to the hypothesis WtChange ~ Prey + 121 with a normal error distribution assumption and linear shape assumption. The coefficients table shows us that the Intercept was estimated as -10.2 +/- 3.6 g and the slope associated with Prey is 0.12 +/- 0.04 \\(g \\cdot m^{3}\\cdot num{-1}\\).\nFor each coefficient, you can see t-statistic (called t value in the table) and p-value (called Pr(&gt;|t\\) in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g. for the intercept in the example, -10.2/3.6 = -2.8). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the p-value. When p-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero, and that the covariate associated with the coefficient can be included in our model (i.e. the covariate is explaining a significant amount of our response variability).\n\nLimitations of p-values\nA problem with this method of testing your research hypothesis comes when you have more than one covariate in your hypothesis. As with Assumption #1 above, any correlation among your covariates will mean that you can not trust your coefficient estimates. This means that you can’t use the p-values as a way to determine which coefficients are significant when you have correlated covariates. Said another way, your assessment of whether a covariate is useful in your model will change if you have correlated covariates. And correlated covariates are very common."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-model-selection-1",
    "href": "DSPPH_SM_HypothesisTesting.html#hypothesis-testing-using-model-selection-1",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Hypothesis testing using model selection",
    "text": "Hypothesis testing using model selection\nAs mentioned above, an alternative method of testing your hypothesis is through model selection. Compare the following two models:\n\nWtChange \\(\\sim \\beta_1\\cdot Prey + \\beta_0 + error\\)\nWtChange \\(\\sim \\beta_0 + error\\)\n\nNote that model 2 is obtained by making \\(\\beta_1 = 0\\). If you determined which of these two models better fits our data, you will know if \\(\\beta_1\\) is likely to be 0 and, thus, whether or not Prey can explain variation in your response.\n\nThe candidate model set\nWe expand this idea from two models to all models in your “candidate model set”. This set are the models representing all possible covariate combinations. So if your hypothesis is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n\\(Resp \\sim Cov1 + Cov2 + 1\\)\n\\(Resp \\sim Cov1 + 1\\)\n\\(Resp \\sim Cov2 + 1\\)\n\\(Resp \\sim 1\\)\nModel selection for hypothesis testing therefore involves:\n\nFirst, each model in your candidate model set is fit to your data.\nThen, each model is “graded” based on how well it is fit to the data and how complicated it is (more below).\nFinally, the models are ranked and you can choose the best-specified model.\n\n\n\nInformation criterion\nEach model in the candidate model set is graded based on an estimate of the model’s “cost” vs. “benefit”. The model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) 22, and the benefit is how well the model fits your data. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.\nThe cost-benefit information is combined to give each model a grade through a metric called “Information Criterion”. For example, Akaike Information Criterion23 is estimated as:\n\\(AIC = 2\\cdot k - 2 \\cdot ln(L)\\)\nwhere \\(k\\) is the cost of the model (number of parameters), and \\(L\\) is the maximum likelihood estimate made when the model was fit.24\nIn all cases, the model with the lowest AIC is our “best-specified” model, though we will discuss what should be done if you have two or more models that are equally “good” (i.e. within 2 AIC of the lowest AIC). Note that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.\n\n\nChoosing you best-specified model\n\nFor example, if your starting model is\n\\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\nyour candidate model set is:\n1: \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\)\n2: \\(Resp \\sim Cov1 + Cov2 + 1\\)\n3: \\(Resp \\sim Cov1 + 1\\)\n4: \\(Resp \\sim Cov2 + 1\\)\n5: \\(Resp \\sim 1\\).\nYou will fit each of these models and estimate their AIC, for example:\n1: \\(Resp \\sim Cov1 + Cov2 + Cov1:Cov2 + 1\\) (AIC = 60)\n2: \\(Resp \\sim Cov1 + Cov2 + 1\\) (AIC = 78)\n3: \\(Resp \\sim Cov1 + 1\\) (AIC = 20)\n4: \\(Resp \\sim Cov2 + 1\\) (AIC = 140)\n5: \\(Resp \\sim 1\\) (AIC = 234).\nYou then use this information to pick your best-specified model as the model with the lowest AIC25. Your best-specified model tells you how to interpret the evidence for your hypothesis. With the example above, you would conclude that the 3rd model (\\(Resp \\sim Cov1 + 1\\)) is your best-specified model (it has by far the lowest AIC), indicating that \\(Cov1\\) explains variablity in \\(Resp\\), but that there is no evidence that \\(Cov2\\) explains variability in \\(Resp\\) (as \\(Cov2\\) doesn’t appear in your best-specified model).\nModel selection like this can be easily done using the dredge() function in the MuMIn package. We’ll practice this in class."
  },
  {
    "objectID": "DSPPH_SM_HypothesisTesting.html#footnotes",
    "href": "DSPPH_SM_HypothesisTesting.html#footnotes",
    "title": "Statistical Modelling: Hypothesis Testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ninference is the conclusion you make based on reasoning and evidence↩︎\nvs. categorical↩︎\nif you’re confused why we are choosing these assumptions, read the course notes section on Starting Model↩︎\nthese estimates are called parameters↩︎\nnote that some write this as “P value” and some as “p value” and some as “p-value”. There is no one rule. Just pick one and make it consistent through your text. I’ll try to do that here.↩︎\nfield↩︎\nsee more in your notes on Data curation and collection↩︎\nSee your notes on Model Validation for more on predictor collinearity↩︎\nbecause it is generally applicable↩︎\ni.e. the effect of Prey on WtChange↩︎\nfor example, a P value of 0.006 means that there is a 0.6% chance we would estimate the effect of Prey to be 0.12 \\(g \\cdot m^{3}\\cdot num{-1}\\) when it was in fact 0↩︎\nwe’ll come back to this in the Reporting section↩︎\nSee your notes on Model Validation for more on predictor collinearity↩︎\nthese are also called “nested” models as each model is “nested” in one of the other models when it only differs by one predictor. “Nested” is also used in experimental design to mean something totally different, so we will avoid using the term here.↩︎\nsee “The Principle of Parsimony” below↩︎\nsee the section on Starting Model↩︎\nnote the spelling and capitalization of this package name!↩︎\nindeed, your null model only contains an intercept↩︎\nand it is the same number given in the summary() output above. More on this coming up in the Reporting section!↩︎\nNote that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.↩︎\nrecall that the + 1 is to indicate that there is an intercept in our model. The +1 can be left out of the model formula and R will still estimate an intercept, but I will write it here for clarity↩︎\nsee section 9.3 and the “Principle of Parsimony”↩︎\nSection 9.17↩︎\nThere are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes).↩︎\nmodels within 2 of the lowest AIC are all chosen as the best-specified model↩︎"
  },
  {
    "objectID": "handbookSMIntro.html",
    "href": "handbookSMIntro.html",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?"
  },
  {
    "objectID": "handbookSMIntro.html#why-statistics",
    "href": "handbookSMIntro.html#why-statistics",
    "title": "Statistical Modelling Handbook",
    "section": "",
    "text": "All science is explaining variability - explaining why observations are changing in time and/or space. These explanations1 lead us to mechanistic understandings of why the world is as we observe it.\nAs biologists, the variability that you are interested in relates to the biological world, but your job is really no different from other scientists/researchers/data analysts, etc. - you are all explaining variability.\nYou need your explanations of variability to be quantitative in order to:\n\ncommunicate how certain you are with your explanation,\ncommunicate how much variability still remains unexplained, and\nmake useful predictions about the biological world.\n\nStatistics provides the mathematical tools2 to accomplish these tasks. Statistics help you determine the evidence for causal3 mechanisms. And statistics help you make useful predictions4 about how a biological system might behave at a different time or location.\nStatistics help us answer:\n\ncan you explain the variability that you are seeing?\n\ngiven your hypothesis, how much variation can you explain?\ngiven your hypothesis, what would you predict to observe?"
  },
  {
    "objectID": "handbookSMIntro.html#why-statistical-modelling",
    "href": "handbookSMIntro.html#why-statistical-modelling",
    "title": "Statistical Modelling Handbook",
    "section": "Why statistical modelling?",
    "text": "Why statistical modelling?\nYou can quantify how much variability you can explain with your research hypothesis through statistical modelling. Your statistical model represents your research hypothesis in a mathematical structure. This mathematical structure can be tested against your data to determine what evidence there is for your hypothesis:\n\ncan I explain the variability that I am seeing? (Can I reject my hypothesis?)\ngiven my hypothesis, how much variation in the observations can I explain?\ngiven my hypothesis, what would I expect (predict) to observe under different times or locations?\n\nYour job then is to explain observation variability in time and space by creating a “model” of what (you think) is going on - hence statistical modelling.\nIt is important to remember that any model is only an approximation of what is going on in the real world. As many have said before\n\nAll models are wrong but some are useful.\n\nWe will discuss how you can build useful models that you can use to test your hypotheses."
  },
  {
    "objectID": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "href": "handbookSMIntro.html#introducing-a-statistical-modelling-framework",
    "title": "Statistical Modelling Handbook",
    "section": "Introducing a statistical modelling framework",
    "text": "Introducing a statistical modelling framework\n\nThe focus of this section of the handbook is statistics that can be applied to your work as a biologist. For that reason, the motivation for what we are going to do together comes directly from your biological research hypotheses. As a biologist, you have a research hypothesis that you want to test. The method presented in this handbook will help you test it. You will learn how to move from biological theory to hypothesis to the statistical modelling process you will use to test your hypothesis.\nYou will learn this process of statistical modelling by walking through a “Statistical Modelling Framework”. This is a set of steps that you can use to go from your research hypothesis to designing a model, testing your hypothesis and communicating the results.\nThis handbook will walk through the parts of this framework one by one. During DSP modules throughout your degree, you will do the same in class while you practice applying the framework to case studies. In this way, you should see how the framework is generally applicable but also flexible. And after you leave the course you will be able to apply the framework to help you in your statistical analysis in other courses, your thesis, and your future career - in every case, your process will take its starting point and focus from the hypothesis that is motivating you.\nAs we will discuss later in the handbook, working through this framework will also guide you in creating the “guts” of a paper or report. As well as clarifying how and why you made your analysis choices, it will guide you in describing your motivation behind your research question (why is it worthwhile to spend time explaining this variation?) as well as the mechanisms behind your research hypothesis (why do I think X is responsible for the variation I’m trying to explain?). Once through, you’ll have a solid draft that can be the basis of a report, thesis chapter or scientific paper. We will talk about how this works in section.\nA note about our Statistical Modelling Framework: The steps in the image to the right as a linear process but it is not actually a linear process. As you will see in the examples, sometimes you will need to make best guesses5 as to what model might be useful and only after confronting the model with your data will you know if your guesses were reasonable (and useful!) - i.e. the model is a valid one that you can use to test your hypothesis. We will talk about how to find a useful model, how to choose when there are multiple options, and how to communicate your choices.\n\nSteps in the statistical modelling framework:\n\n\nResponse(s)\nHere you will define your research question by identifying your response variable(s).\n\nWhat variability are you trying to explain?\nAnd why is it worth explaining? (your motivation)\n\nNote: we’ll begin by discussing how to model hypotheses with just one response variable before discussing multiple response variable(s).\n\n\nPredictor(s)\nHere you will choose your predictor variables.\n\nwhat could explain the variability in your response?\nwhat are the possible mechanisms behind your argument?\n\n\n\nHypothesis\nHere you will see how your response and predictor variables come together to define your research hypothesis. And we will discuss how to write this hypothesis to begin building your statistical modelling.\n\n\nStarting model\nHere you will choose and fit the starting model that will be used to test your hypothesis. You will do this by choosing and communicating two key assumptions that will help you pick a useful modelling starting point. Then you will fit your model to your data (i.e. confronting your model with your data).\n\n\nModel validation\nHere you will investigate whether your model will be a useful one to test your hypothesis. Your steps here will include considering if you have correlated predictors or problems with observation dependence.\nYou will also considering if your starting model assumptions were realistic. After this step, you will have a model that you can confidently use to test your hypothesis.\n\n\nHypothesis testing\nHere you will test your hypothesis by assessing the evidence supporting your model. We will discuss a number of different methods to do this, but will focus on the model selection method as a robust way to evaluate what your model is telling you about your hypothesis.\n\n\nReporting\nHere you will report the results of your hypothesis testing.\nYou will report:\n\nyour best-specified model identified in the hypothesis testing\nthe effects (patterns) described by your model (including visualizing your model effects)\nhow well your model explains variability in your response.\n\n\n\nPredicting\nHere you will use your model to make predictions of your response under different conditions (while considering prediction limits).\n\n\n\nWhere we will begin: generalized linear models (GLMs)\nTo begin with, we will be discussing generalized linear models (GLMs) as models that can be useful to test many different hypotheses. Also, understanding how GLMs can be used to test your hypothesis will help you understand other, including more advanced, statistical models (Pongpipat_et_al_PracticalExtensionStatisticsForPsychology).\nRemember: the model you choose is just an approximation of the real world. This means that often times alternative models would be possible (models like t-tests, ANOVAs, ANCOVAs, etc.6). In fact, you may be collaborating with someone who wants to model your hypothesis with a different method. In this handbook, we compare GLMs to alternative model types here. And remember: the steps in the statistical modelling framework are generally applicable. Regardless of the method you apply, you need to ground your choices in good biological and statistical theory."
  },
  {
    "objectID": "handbookSMIntro.html#the-examples",
    "href": "handbookSMIntro.html#the-examples",
    "title": "Statistical Modelling Handbook",
    "section": "The examples",
    "text": "The examples\nHere we’ve gathered examples following our statistical modelling framework structure. You can request/contribute new examples here"
  },
  {
    "objectID": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "href": "handbookSMIntro.html#from-statistical-modelling-to-scientific-report-writing",
    "title": "Statistical Modelling Handbook",
    "section": "From statistical modelling to scientific report writing",
    "text": "From statistical modelling to scientific report writing\nHere you can see how you can use the steps in the Statistical Modelling Framework to outline your communication of your hypothesis testing in reports and paper."
  },
  {
    "objectID": "handbookSMIntro.html#where-to-from-here",
    "href": "handbookSMIntro.html#where-to-from-here",
    "title": "Statistical Modelling Handbook",
    "section": "Where to from here?",
    "text": "Where to from here?\nTBA: - boosted regression trees - Bayesian negative bionomial regression mixed models - nested design in mixed models - random forest vs. GAMs - meta-analysis - complex models - classification (categorical response) and regression (continous response) trees - boosted regression trees - boosted regression trees iteratively fit models gradually increasing emphasis on observations that were initially poorly fit - ADMB and TMB\n\nmultivariate statistics\nexploratoryordination/clustering\ncross validation\n\n[[Principle Component Analysis]] linearly transforms multivariate data into a new coordinate system where the majority of the variation in the data is captured with fewer dimensions than the initial data - Li et al. 2023\n[[Principle Component Analysis|PCA]] constructs a “map” of the samples where samples that are more similar are closer together\n\nAcknowledgements\nTBA"
  },
  {
    "objectID": "handbookSMIntro.html#footnotes",
    "href": "handbookSMIntro.html#footnotes",
    "title": "Statistical Modelling Handbook",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhere, we can also use the term “research hypothesis” instead of explanation and you’ll see we quickly switch to using this term↩︎\nMath skills have often been underemphasized in many Biology educations. This has not been helpful or necessary. Biologists need math and are very capable at applying math to solve problems, but the math needs to be useful; that is, math that you can apply to your own needs as you research biology. The DSP Program aims at providing statistical modelling tools that will be useful to you as a biologist. It is a happy coincidence that the same skills can be applied to a lot of other situations as well. The programming and statistics skills you are learning here will be useful to you in the future - in your future courses, thesis-writing and a wide range of careers.↩︎\nmore on causal vs. correlative explanations coming soon↩︎\nuseful predictions will always include uncertainty↩︎\nand these will be educated guesses!↩︎\nDon’t worry if these terms don’t mean anything to you yet - more to come!↩︎"
  }
]
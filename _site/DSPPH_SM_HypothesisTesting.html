<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Skills Portfolio Program - Statistical Modelling: Hypothesis Testing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Data Skills Portfolio Program</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./intro.html" rel="" target="">
 <span class="menu-text">What is the DSP Program?</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./handbookIntro.html" rel="" target="">
 <span class="menu-text">The DSP Program Handbook</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./feedback.html" rel="" target="">
 <span class="menu-text">Feedback</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-hypothesis-testing" id="toc-what-is-hypothesis-testing" class="nav-link active" data-scroll-target="#what-is-hypothesis-testing">What is hypothesis testing?</a></li>
  <li><a href="#hypothesis-testing-using-p-values" id="toc-hypothesis-testing-using-p-values" class="nav-link" data-scroll-target="#hypothesis-testing-using-p-values">Hypothesis testing using P-values</a>
  <ul class="collapse">
  <li><a href="#what-is-a-p-value" id="toc-what-is-a-p-value" class="nav-link" data-scroll-target="#what-is-a-p-value">What is a P-value</a></li>
  <li><a href="#limitations-of-p-values" id="toc-limitations-of-p-values" class="nav-link" data-scroll-target="#limitations-of-p-values">Limitations of P-values</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing-using-model-selection" id="toc-hypothesis-testing-using-model-selection" class="nav-link" data-scroll-target="#hypothesis-testing-using-model-selection">Hypothesis testing using model selection</a>
  <ul class="collapse">
  <li><a href="#what-is-model-selection" id="toc-what-is-model-selection" class="nav-link" data-scroll-target="#what-is-model-selection">What is model selection</a></li>
  <li><a href="#how-do-you-use-hypothesis-testing-for-model-selection" id="toc-how-do-you-use-hypothesis-testing-for-model-selection" class="nav-link" data-scroll-target="#how-do-you-use-hypothesis-testing-for-model-selection">How do you use hypothesis testing for model selection</a>
  <ul class="collapse">
  <li><a href="#form-your-candidate-model-set" id="toc-form-your-candidate-model-set" class="nav-link" data-scroll-target="#form-your-candidate-model-set">Form your candidate model set</a></li>
  <li><a href="#fit-and-rank-models-in-your-candidate-model-set" id="toc-fit-and-rank-models-in-your-candidate-model-set" class="nav-link" data-scroll-target="#fit-and-rank-models-in-your-candidate-model-set">Fit and rank models in your candidate model set</a></li>
  <li><a href="#choose-your-best-specified-models" id="toc-choose-your-best-specified-models" class="nav-link" data-scroll-target="#choose-your-best-specified-models">Choose your best-specified model(s)</a></li>
  <li><a href="#what-does-your-best-specified-models-say-about-your-hypothesis" id="toc-what-does-your-best-specified-models-say-about-your-hypothesis" class="nav-link" data-scroll-target="#what-does-your-best-specified-models-say-about-your-hypothesis">What does your best-specified model(s) say about your hypothesis?</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#up-next" id="toc-up-next" class="nav-link" data-scroll-target="#up-next">Up next</a>
  <ul class="collapse">
  <li><a href="#hypothesis-testing-using-p-values-1" id="toc-hypothesis-testing-using-p-values-1" class="nav-link" data-scroll-target="#hypothesis-testing-using-p-values-1">Hypothesis testing using p-values</a>
  <ul class="collapse">
  <li><a href="#limitations-of-p-values-1" id="toc-limitations-of-p-values-1" class="nav-link" data-scroll-target="#limitations-of-p-values-1">Limitations of p-values</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing-using-model-selection-1" id="toc-hypothesis-testing-using-model-selection-1" class="nav-link" data-scroll-target="#hypothesis-testing-using-model-selection-1">Hypothesis testing using model selection</a>
  <ul class="collapse">
  <li><a href="#the-candidate-model-set" id="toc-the-candidate-model-set" class="nav-link" data-scroll-target="#the-candidate-model-set">The candidate model set</a></li>
  <li><a href="#information-criterion" id="toc-information-criterion" class="nav-link" data-scroll-target="#information-criterion">Information criterion</a></li>
  <li><a href="#choosing-you-best-specified-model" id="toc-choosing-you-best-specified-model" class="nav-link" data-scroll-target="#choosing-you-best-specified-model">Choosing you best-specified model</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Statistical Modelling: Hypothesis Testing</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In this section you will learn:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><p>how statistical models can be used to test your hypothesis by judging the evidence for your model</p></li>
<li><p>about methods to judge the evidence for your model</p></li>
<li><p>to use the model selection method to judge the evidence for your model and test your hypothesis</p></li>
</ul>
</div>
</div>
</div>
<section id="what-is-hypothesis-testing" class="level1">
<h1>What is hypothesis testing?</h1>
<p>Once you have confidence in your starting model, you can finally arrive at the reason for your statistical journey: testing your model to see what evidence there is for your research hypothesis. <strong>Finally - the science!</strong> By testing your model, you will find out what inference<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> can be made from your modelled effects.</p>
<p>Recall that fitting your starting model meant that you estimated the parameter for each coefficient associated with your predictors.</p>
<p>Let’s take an example where we want to explain variability in change in weight (<code>WtChange</code>, g) and believe it is due to prey density (<code>Prey</code>, <span class="math inline">\(num \cdot m^{-3}\)</span>). Here, we have <code>Prey</code> as a continuous<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> predictor.</p>
<p>We will test the hypothesis <code>WtChange ~ Prey</code> by first fitting a model with a normal error distribution assumption and a linear shape assumption<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. If our research hypothesis is</p>
<p><em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span></p>
<p>our starting model will fit:</p>
<p><em>WtChange</em> <span class="math inline">\(\sim \beta_1\cdot Prey + \beta_0 + error\)</span></p>
<p>where the coefficients are the slope (<span class="math inline">\(\beta_1\)</span>) and intercept (<span class="math inline">\(\beta_0\)</span>), and the error is based on a normal error distribution.</p>
<p>When you test your hypothesis, you are focusing on the estimates of your model coefficients<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> and whether or not these estimates are different than zero.</p>
<p>For example, if <span class="math inline">\(\beta_1 \approx 0\)</span>, the effect of <code>Prey</code> on <code>WtChange</code> would be zero, the <code>Prey</code> predictor would fall out of the model, and you would conclude that there is little evidence that <code>Prey</code> explains variability in <code>WtChange</code>.</p>
<p>So, to test your research hypothesis, you need to determine if each of the coefficients are significantly different than zero.</p>
<p>This can be done using (at least) two methods:</p>
<ul>
<li><p>the P-value<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> method: The first method estimates the probability (P-value) your coefficient (e.g.&nbsp;<span class="math inline">\(\beta_1\)</span>) would be estimated at the value it is even though the “real” value of your coefficient is 0. More on P-values below.</p></li>
<li><p>the model selection method: The second method involves comparing models with different predictor combinations (model selection). Here, you will consider the evidence for models with and without each of your predictors to determine the evidence that each coefficient is different than zero.</p></li>
</ul>
<p>The method that you can use to test your hypothesis depends on the design of the study you use to explore your research hypothesis - i.e.&nbsp;experimental vs.&nbsp;observational<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> studies<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. With experimental studies where you are able to control (to a good extent) the collinearity among your predictors<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, you can use either the P-value or model selection methods. For observational studies, the model selection method is a more robust way to test your hypothesis. For this reason<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, in this course, you will test your research hypothesis through model selection but let us first discuss the P-value method (what it is, and its limitations).</p>
</section>
<section id="hypothesis-testing-using-p-values" class="level1">
<h1>Hypothesis testing using P-values</h1>
<section id="what-is-a-p-value" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-p-value">What is a P-value</h2>
<p>The P-value is used for null-hypothesis significance testing (NHST) (<span class="citation" data-cites="Muff2022">@Muff2022</span>). The “P” in P-value stands for probability - the probability of observing an outcome given that the null hypothesis is true (<span class="citation" data-cites="Muff2022">@Muff2022</span>; <span class="citation" data-cites="Popovic2024">@Popovic2024</span>). Remember that null-hypothesis tests assume that the tested effect is zero. In the case of hypothesis testing, the null hypothesis test assumes a coefficient describing the effect of a predictor on your response is zero.</p>
<p>In the case of hypothesis testing, the null hypothesis you are testing against is that a predictor’s coefficient is zero. So, the P-value associated with the hypothesis testing tells you the probability of getting a coefficient of the value you got even though the coefficient is in fact zero.</p>
<p>When the P-value is very low, we say that there is evidence that the coefficient is not zero, i.e.&nbsp;evidence that your predictor has an effect on your response. By convention, we say a P-value is low if P &lt; 0.05; meaning that the evidence comes with a 5% probability that the coefficient is actually zero.</p>
<p>First, let’s describe how this works in general, and then look at an example:</p>
<p>To determine a P-value associated with a model coefficient, the null-hypothesis testing estimates something called a test statistic based on the coefficient’s estimate and the error around it. This test statistic is assumed to come from a certain data distribution (the exact distribution will vary based on your model structure)</p>
<p>Let’s look at an example using our model fit to the hypothesis <code>WtChange ~ Prey + 1</code>. By using <code>summary()</code> on our model, we get</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(startMod) <span class="co"># look at our validated starting model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = WtChange ~ Prey, family = gaussian(link = "identity"), 
    data = myDat)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -6.996353   0.158745  -44.07   &lt;2e-16 ***
Prey         0.079912   0.002557   31.25   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 0.1414749)

    Null deviance: 147.8242  on 69  degrees of freedom
Residual deviance:   9.6203  on 68  degrees of freedom
AIC: 65.728

Number of Fisher Scoring iterations: 2</code></pre>
</div>
</div>
<p>The coefficients table shows us that the Intercept was estimated as -7 ± 0.16 g and the slope associated with <code>Prey</code><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> is 0.08 ± 0.0026 <span class="math inline">\(g \cdot m^{3}\cdot num^{-1}\)</span>.</p>
<p>For each coefficient, you can see t-statistic (called <code>t value</code> in the table) and P-value (called <code>Pr(&gt;|t\)</code> in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g.&nbsp;for the intercept in the example, -7/0.16 = -44.07). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the P-value. When P-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>, and that the predictor associated with the coefficient can be included in our model (i.e.&nbsp;the predictor is explaining a significant amount of our response variability).</p>
<p><strong>How to estimate P-values for your model</strong></p>
<p>The output from the <code>summary()</code> function quickly becomes limiting when you have more than one predictor. Instead, you can use the <code>anova()</code> function to estimate the P-values associated with each model term.</p>
<p>Here’s an example for our model testing <em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(startMod, <span class="co"># model object</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>      <span class="at">test =</span> <span class="st">"F"</span>) <span class="co"># type of null hypothesis test to perform</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model: gaussian, link: identity

Response: WtChange

Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev      F    Pr(&gt;F)    
NULL                    69     147.82                     
Prey  1    138.2        68       9.62 976.88 &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>Note here that you need to indicate what type of null hypothesis testing you want:</p>
<ul>
<li><p>use the F-test for error distribution assumptions like normal or gamma (i.e.&nbsp;distributions where the scale parameter is estimated)</p></li>
<li><p>use the Chi-square test for error distribution assumptions like poisson or binomial (i.e.&nbsp;distributions where the scale parameter is fixed)</p></li>
</ul>
<p>The result is a table where each predictor has a row to report the results of the null hypothesis test. Here we see that there is strong evidence the coefficient associated with <code>Prey</code> is not zero (P &lt; <span class="math inline">\(2.2 \cdot 10^{-16}\)</span>).</p>
<p>Two more notes about using P-values:</p>
<ol type="1">
<li><p>note in the table above that it says “Terms added sequentially (first to last)”. This indicates that the coefficients of the predictors are tested by adding each predictor one at a time to the model, estimating the coefficient associated with the predictor, and testing the null hypothesis that the coefficient is not different than zero. <strong>This process is problematic when you have even a moderate amount of predictor collinearity.</strong> This is a big reason to prefer the model selection method of hypothesis testing that we outline below.</p></li>
<li><p>Because of the issues interpreting P-values, it is better to talk about what P-values tell you about the evidence for your hypothesis, rather than a strict idea of rejecting or not your hypothesis. Here is an illustration of how to interpret your P-values<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>:</p></li>
</ol>
<p><img src="./MuffPValues.png" align="center" width="400px"></p>
<p>(from <span class="citation" data-cites="Muff2022">@Muff2022</span>)</p>
</section>
<section id="limitations-of-p-values" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-p-values">Limitations of P-values</h2>
<p>As mentioned in the previous section, problem with the P-value method of testing your research hypothesis comes when you have more than one predictor in your hypothesis. Correlation among your predictors<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> means that it is difficult to trust your coefficient estimates. This means that you can not use the P-values as a way to determine which coefficients are significantly different than zero when you have correlated predictors. Said another way, your assessment of whether a predictor is useful in your model will be uncertain if you have correlated predictors. And correlated predictors are very common. For this reason, we will be hypothesis testing using model selection.</p>
</section>
</section>
<section id="hypothesis-testing-using-model-selection" class="level1">
<h1>Hypothesis testing using model selection</h1>
<p>An alternative method of testing your hypothesis is through model selection. This method is more robust to issues like predictor collinearity and can be generally applied regardless of the structure of your experiment or model. For this reason, we focus on this method of hypothesis testing.</p>
<section id="what-is-model-selection" class="level2">
<h2 class="anchored" data-anchor-id="what-is-model-selection">What is model selection</h2>
<p>Compare the following two models:</p>
<ol type="1">
<li><p><em>WtChange</em> <span class="math inline">\(\sim \beta_1\cdot Prey + \beta_0 + error\)</span></p></li>
<li><p><em>WtChange</em> <span class="math inline">\(\sim \beta_0 + error\)</span></p></li>
</ol>
<p>Note that model 2 is obtained by making the coefficient of <code>Prey</code> (<span class="math inline">\(\beta_1\)</span>) equal to 0 (i.e.&nbsp;if the effect of <code>Prey</code> on <code>WtChange</code> was zero). If you determined which of these two models better fits your data, you will know if <span class="math inline">\(\beta_1\)</span> is likely to be 0 and, thus, whether or not you have evidence that <code>Prey</code> can explain variation in <code>WtChange</code>.</p>
</section>
<section id="how-do-you-use-hypothesis-testing-for-model-selection" class="level2">
<h2 class="anchored" data-anchor-id="how-do-you-use-hypothesis-testing-for-model-selection">How do you use hypothesis testing for model selection</h2>
<p>The steps involved in testing your hypothesis using model selection is</p>
<ul>
<li><p>form your candidate model set</p></li>
<li><p>fit and rank models in your candidate model set</p></li>
<li><p>choose your best-specified model(s)</p></li>
</ul>
<p>Let’s walk through these now.</p>
<section id="form-your-candidate-model-set" class="level3">
<h3 class="anchored" data-anchor-id="form-your-candidate-model-set">Form your candidate model set</h3>
<p>Your candidate model set contains models with all possible predictor combinations<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. So the candidate model set for <em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span> is:</p>
<p><em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span></p>
<p><em>WtChange</em> <span class="math inline">\(\sim 1\)</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="Another example">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Another example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here is another example:</p>
<p>if your hypothesis is</p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span></p>
<p>your candidate model set is</p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Cov1 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Cov2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim 1\)</span></p>
<p>Note that the more predictors you have in your model, the bigger your candidate model set.</p>
</div>
</div>
</div>
<p>Hopefully you are starting to see that the difference among models in the candidate model set can be described by setting the coefficient associated with a predictor to zero. In this way, fitting and comparing the models in your candidate model set is a way of assessing the evidence for your hypothesis. This method is more robust to issues like predictor collinearity because you are assessing the evidence for a predictor’s effect on your response when each predictor is in a model alone and when it is in a model with other predictors.</p>
<p>One last note about your candidate model set: you must remember the biology when you form your candidate model set. There may be a biological reason why a certain model must not be included in your candidate model set (i.e.&nbsp;a model that defies biological logic). These should be excluded from your candidate model set. (<span class="citation" data-cites="BurnhamAnderson2002">@BurnhamAnderson2002</span>).</p>
</section>
<section id="fit-and-rank-models-in-your-candidate-model-set" class="level3">
<h3 class="anchored" data-anchor-id="fit-and-rank-models-in-your-candidate-model-set">Fit and rank models in your candidate model set</h3>
<p>Each model in the candidate model set is graded based on an estimate of the model’s “cost” vs.&nbsp;its “benefit”.</p>
<p>The model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>.</p>
<p>The model’s benefit is how well the model fits your data - i.e.&nbsp;how much of the variability in your response the model explains. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<div class="callout callout-style-default callout-tip callout-titled" title="The Principle of Parsimony">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Principle of Parsimony
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The principle of parsimony means that, when in doubt, you will choose the simpler explanation. This means that:</p>
<ul>
<li><p>models should have as few parameters as possible</p></li>
<li><p>linear models are preferred to non-linear models</p></li>
<li><p>models with fewer assumptions are better</p></li>
</ul>
<p>This said, there are times when you might choose a more complicated explanation over a simpler explanation. One example of this is when you prioritize a model’s ability to predict a future response vs.&nbsp;getting an accurate understanding of the underlying mechanisms. We will discuss this more in the upcoming section on Prediction.</p>
</div>
</div>
</div>
<p>You can fit and rank your models quickly using a function called <code>dredge()</code> in the MuMIn package<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>. The <code>dredge()</code> function fits and ranks models representing all possible predictor combinations based on your starting model - i.e.&nbsp;your default candidate model set. The output is a table ranking the models in your candidate model set.</p>
<p>Let’s explore this now.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MuMIn) <span class="co"># load MuMIn package</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">na.action =</span> <span class="st">"na.fail"</span>) <span class="co"># to avoid illegal model fitting</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>dredgeOut <span class="ot">&lt;-</span> <span class="fu">dredge</span>(startMod) <span class="co"># create model selection table for validated starting model</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(dredgeOut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Global model call: glm(formula = WtChange ~ Prey, family = gaussian(link = "identity"), 
    data = myDat)
---
Model selection table 
  (Intrc)    Prey df   logLik  AICc  delta weight
2  -6.996 0.07991  3  -29.864  66.1   0.00      1
1  -2.238          2 -125.489 255.2 189.07      0
Models ranked by AICc(x) </code></pre>
</div>
</div>
<p>Note the line:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">na.action =</span> <span class="st">"na.fail"</span>) <span class="co"># to avoid illegal model fitting</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This is included because you need to make sure the data used to fit every model in your candidate model set stays the same. This could be violated if you have missing values in some of your predictor columns. This <code>options()</code> statement makes sure your model selection is following the rules.</p>
<p>The output of the <code>dredge()</code> function gives us</p>
<ul>
<li>the Global model call (our original hypothesis), and</li>
<li>a Model selection table</li>
</ul>
<p>The Model selection table contains one row for each model in our candidate model set. Let’s explore this now:</p>
<p><em>Find the column called “(Intrc)”</em>. This column tells us when the intercept is included in the model. If there is a number in that column, the associated model in your candidate model set (row) contains an intercept. Note that by default all models will contain an intercept<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>.</p>
<p><em>Find the column called “Prey”</em>. This column tells us when the <code>Prey</code> predictor is in the model. Notice the first row contains a number in the Prey column, while the second row is blank. This means that <code>Prey</code> is a predictor in the model reported in the first row but is missing from the model in the second. Note also that a number is recorded in the Prey column, row 1 (0.0799). This is the coefficient associated with the <code>Prey</code> predictor. Since we have a normal error distribution assumption, this coefficient can be considered the slope of a line<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. If the predictor was a categorical predictor (vs.&nbsp;continuous predictor), a “+” would appear in the Model selection table when the categorical predictor was in the model.</p>
<p>So, in our example above, the model in the first row contains an intercept and the <code>Prey</code> - i.e.&nbsp;the first row is the model <code>WtChange ~ Prey + 1</code>. The model in the second row contains only an intercept - i.e.&nbsp;the second row is the model <code>WtChange ~ 1</code>.</p>
<p>The rest of the columns in the Model selection table contain information that help us rank the models.</p>
<ul>
<li><p>the “df” column reports the number of model coefficients. Models that are more complicated (e.g.&nbsp;more predictors) will have a higher df as they require more coefficients to fit. Models with more terms are more “costly”. In the first row (<code>WtChange ~ Prey + 1</code>), df is 3 because the model fitting estimates a coefficient for <code>Prey</code>, for the intercept, and for the normal error distribution assumption (standard deviation). In the second row (<code>WtChange ~ 1</code>), df is 2 because the model fitting estimates a coefficient for the intercept, and for the normal error distribution assumption (standard deviation). So the model in the first row is more costly than the second row.</p></li>
<li><p>the “logLik” column reports the log-Likelihood of the model fit. The absolute value of this estimate will depend on the type of data you are modelling, but in general, the logLik is related to how much variation in your response the model explains. It can be used to compare models fit to the same data. This can be seen as a measure of the “benefit” of the model.</p></li>
<li><p>the “AICc” column reports information criteria for your models. Information criteria balances the cost (complexity) and benefit (explained variation) for your model. An example of information criterion is the Akaike Information Criterion (AIC). The AIC is estimated as:</p></li>
</ul>
<p><span class="math inline">\(AIC = 2\cdot k - 2 \cdot ln(L)\)</span></p>
<p>where <span class="math inline">\(k\)</span> is the cost of the model (number of coefficients, like df above), and <span class="math inline">\(L\)</span> is the maximum likelihood estimate made when the model was fit.</p>
<p>There are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes). The AICc is reported by default here, but you can control that in the <code>dredge()</code> function. <strong>In all cases, lower information criterion means more support for the model.</strong></p>
<ul>
<li><p>the “delta” (<span class="math inline">\(\Delta\)</span>) column is a convenient way to see how different each model’s AICc is from the model with the lowest AICc (<span class="math inline">\(\Delta AIC_i\)</span> is the change in AIC for model <em>i</em> vs.&nbsp;the model with the lowest AIC.)</p></li>
<li><p>the “weight” column reports Akaike weights for the model. The Akaike weights are a measure of the relative likelihood of the models. The sum of all the Akaike weights is 1, so we can get a relative estimate for the support for each model.</p></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Akaike weights">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Akaike weights
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here is the equation to estimate the Akaike weights:</p>
<p><span class="math display">\[
w_i = \frac{exp(-\frac{1}{2} \cdot \Delta AIC_i)}{\sum_{r=1}^{R}exp(-\frac{1}{2} \cdot \Delta AIC_r)}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(w_i\)</span> is the Akaike weight for model <em>i</em>,</li>
<li><span class="math inline">\(\Delta AIC_i\)</span> is the change in AIC for model <em>i</em> vs.&nbsp;the model with the lowest AIC</li>
<li><span class="math inline">\(\Delta AIC_r\)</span> is the change in AIC for model <em>r</em> vs.&nbsp;the model with the lowest AIC. This is estimated for all models in the candidate model set (<em>R</em> models).</li>
</ul>
<p><span class="citation" data-cites="BurnhamAnderson2002">@BurnhamAnderson2002</span></p>
</div>
</div>
</div>
</section>
<section id="choose-your-best-specified-models" class="level3">
<h3 class="anchored" data-anchor-id="choose-your-best-specified-models">Choose your best-specified model(s)</h3>
<p>Using the model selection table, you can choose your best-specified model(s) and find out what it tells you about your hypothesis.</p>
<p>In general, your best-specified model will be the model with the lowest information criterion (e.g.&nbsp;AIC)<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>. This will be the model at the top of the model selection table.</p>
<p>That said, notice I write “best-specified model(s)” - possibly plural. This is because you might have models where the AIC estimates are very close to one another. A good rule of thumb is to report all models where the AIC is within 2 of the lowest AIC model (i.e.&nbsp;delta &lt; 2). Following <span class="citation" data-cites="BurnhamAnderson2002">@BurnhamAnderson2002</span>,</p>
<table class="table">
<thead>
<tr class="header">
<th>for models where delta is</th>
<th>there is … for the model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0-2</td>
<td>substantial support</td>
</tr>
<tr class="even">
<td>4-7</td>
<td>considerably less support</td>
</tr>
<tr class="odd">
<td>&gt; 10</td>
<td>essentially no support</td>
</tr>
</tbody>
</table>
<p>With our example above:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(dredgeOut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Global model call: glm(formula = WtChange ~ Prey, family = gaussian(link = "identity"), 
    data = myDat)
---
Model selection table 
  (Intrc)    Prey df   logLik  AICc  delta weight
2  -6.996 0.07991  3  -29.864  66.1   0.00      1
1  -2.238          2 -125.489 255.2 189.07      0
Models ranked by AICc(x) </code></pre>
</div>
</div>
<p>we have one best-specified model (model with substantial support):</p>
<p><code>WtChange ~ Prey + 1</code> (AICc = 66.1)</p>
<p>and essentially no support for the null model:</p>
<p><code>WtChange ~ 1</code> (AICc = 255.2; delta = 189.1)</p>
<p>We can conclude that there is evidence that <code>Prey</code> explains variability in <code>WtChange</code>.</p>
</section>
<section id="what-does-your-best-specified-models-say-about-your-hypothesis" class="level3">
<h3 class="anchored" data-anchor-id="what-does-your-best-specified-models-say-about-your-hypothesis">What does your best-specified model(s) say about your hypothesis?</h3>
<p>Model selection is a way of hypothesis testing. So what does your best-specified model say about your hypothesis? By comparing your best-specified model to your starting model, you can see where there is evidence for the effects of each predictor, and where the effects are estimated to be zero.</p>
<p>As our best-specified model is</p>
<p><code>WtChange ~ Prey + 1</code></p>
<p>We can conclude that there is evidence that <code>Prey</code> explains variability in <code>WtChange</code>.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="More examples">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
More examples
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>if our starting hypothesis was:</p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span></p>
<ul>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span> would indicate that we have evidence that there are effects of <code>Cov1</code> and <code>Cov2</code> on <code>Resp</code> and that the effect of <code>Cov1</code> on <code>Resp</code> depends on <code>Cov2</code> (an interaction).</p></li>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim Cov1 + Cov2 + 1\)</span> would indicate that we have evidence that there are effects of <code>Cov1</code> and <code>Cov2</code> on <code>Resp</code> but no evidence of an interaction effect.</p></li>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim Cov1 + 1\)</span> would indicate that we have evidence that there is an effect of <code>Cov1</code> but not <code>Cov2</code> on <code>Resp</code>.</p></li>
<li><p>A best-specified model of <span class="math inline">\(Resp \sim 1\)</span> (i.e.&nbsp;the null hypothesis) would indicate that we have no evidence for effects of <code>Cov1</code> or <code>Cov2</code> on <code>Resp</code>. This is also a valid scientific result!</p></li>
</ul>
</div>
</div>
</div>
<p>In the next section (on Reporting), we will discuss further how to communicate what your hypothesis testing results say about your hypothesis.</p>
</section>
</section>
</section>
<section id="up-next" class="level1">
<h1>Up next</h1>
<p>In our next class (after the Easter break), we will learn how to report (including visualize) what your hypothesis testing results are saying about your hypothesis.</p>
<pre><code>- Other methods - Test your hypothesis by the model selection method to choose your best-specified model    - </code></pre>
<!---

P-value is the probability of observing the outcome when the null hypothesis is true.

experimental vs. observational studies can handle different model selection procedures
experimental can use null-hypothesis testing and p-values
observational can use model selection


Burnham_Anderson_Chapter2_InformationCriterionLikelihoodTheory

P-values are continuous measures of statistical evidence Muff et al. 2021


refinements of your hypothesis after you collect the data will result in a hypothesis that reflects the sample, not the population


--->
<p>Form your candidate model set (how to do this smartly) - nested and non-nested models Fit and grade each model Rank and choose your best-specified model(s)</p>
<p>Once you have confidence in your starting model, you can finally arrive at the reason for your statistical journey: testing your model to see what evidence there is for your research hypothesis.</p>
<p>Recall that fitting your starting model meant that you estimated the coefficients for each parameter associated with your covariates. Let’s take an example where we want to explain variability in change in weight (WtChange, g) and believe it is due to prey density (Prey, <span class="math inline">\(num \cdot m^{-3}\)</span>). We will test the hypothesis <code>WtChange ~ Prey</code> by first fitting a model with a normal error distribution assumption and a linear shape assumption.</p>
<p>If our research hypothesis is</p>
<p><em>WtChange</em> <span class="math inline">\(\sim Prey + 1\)</span></p>
<p>our starting model will fit:</p>
<p><em>WtChange</em> <span class="math inline">\(\sim \beta_1\cdot Prey + \beta_0 + error\)</span></p>
<p>where the coefficients are the slope (<span class="math inline">\(\beta_1\)</span>) and intercept (<span class="math inline">\(\beta_0\)</span>), and the error comes from a normal error distribution.</p>
<p>When you test your hypothesis, you are focusing on the model coefficients and whether or not they are significantly different than zero. For example, if <span class="math inline">\(\beta_1 \approx 0\)</span>, the Prey covariate would fall out of the model, and you would conclude there is little evidence Prey explains variability in WtChange.</p>
<p>So, to test your research hypothesis, you need to determine if each of the coefficients are significantly different than zero. This can be done using (at least) two methods: The first method estimates the probability (p-value) your coefficient would be estimated at the value it is even though the “real” slope is 0. The second method involves comparing models with different covariate combinations (model selection). In this class, we will test our research hypothesis through model selection but let’s first discuss why we aren’t using the p-value method.</p>
<section id="hypothesis-testing-using-p-values-1" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing-using-p-values-1">Hypothesis testing using p-values</h2>
<p>You’ll remember from last week that you can use <code>summary()</code> on your starting model object to see the coefficients that were fit, along with their uncertainty (standard error) in the coefficients and a p-value. Let’s take another look with an example:</p>
<p><img src="./summaryMod.png" width="400px"></p>
<p>In the example, we are trying to explain variability in WtChange (g) with Prey <span class="math inline">\(num \cdot m^{-3}\)</span> by fitting a model to the hypothesis <code>WtChange ~ Prey + 1</code><a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> with a normal error distribution assumption and linear shape assumption. The coefficients table shows us that the Intercept was estimated as -10.2 +/- 3.6 g and the slope associated with Prey is 0.12 +/- 0.04 <span class="math inline">\(g \cdot m^{3}\cdot num{-1}\)</span>.</p>
<p>For each coefficient, you can see t-statistic (called <code>t value</code> in the table) and p-value (called <code>Pr(&gt;|t\)</code> in the table). The t-statistic allows you to test the hypothesis that the coefficient is not different than zero. The t-statistic is the value of the coefficient divided by the standard error (e.g.&nbsp;for the intercept in the example, -10.2/3.6 = -2.8). The t-statistic is compared to a Student t Distribution to get the probability that we get the estimated coefficient value even though the coefficient is zero. This probability is the p-value. When p-values are very small (P &lt;&lt; 0.05), we are confident that the coefficients we are estimating are likely different than zero, and that the covariate associated with the coefficient can be included in our model (i.e.&nbsp;the covariate is explaining a significant amount of our response variability).</p>
<section id="limitations-of-p-values-1" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-p-values-1">Limitations of p-values</h3>
<p>A problem with this method of testing your research hypothesis comes when you have more than one covariate in your hypothesis. As with Assumption #1 above, any correlation among your covariates will mean that you can not trust your coefficient estimates. This means that you can’t use the p-values as a way to determine which coefficients are significant when you have correlated covariates. Said another way, your assessment of whether a covariate is useful in your model will change if you have correlated covariates. And correlated covariates are very common.</p>
</section>
</section>
<section id="hypothesis-testing-using-model-selection-1" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing-using-model-selection-1">Hypothesis testing using model selection</h2>
<p>As mentioned above, an alternative method of testing your hypothesis is through model selection. Compare the following two models:</p>
<ol type="1">
<li><p><em>WtChange</em> <span class="math inline">\(\sim \beta_1\cdot Prey + \beta_0 + error\)</span></p></li>
<li><p><em>WtChange</em> <span class="math inline">\(\sim \beta_0 + error\)</span></p></li>
</ol>
<p>Note that model 2 is obtained by making <span class="math inline">\(\beta_1 = 0\)</span>. If you determined which of these two models better fits our data, you will know if <span class="math inline">\(\beta_1\)</span> is likely to be 0 and, thus, whether or not Prey can explain variation in your response.</p>
<section id="the-candidate-model-set" class="level3">
<h3 class="anchored" data-anchor-id="the-candidate-model-set">The candidate model set</h3>
<p>We expand this idea from two models to all models in your “candidate model set”. This set are the models representing all possible covariate combinations. So if your hypothesis is</p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span></p>
<p>your candidate model set is</p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Cov1 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim Cov2 + 1\)</span></p>
<p><span class="math inline">\(Resp \sim 1\)</span></p>
<p>Model selection for hypothesis testing therefore involves:</p>
<ul>
<li><p>First, each model in your candidate model set is fit to your data.</p></li>
<li><p>Then, each model is “graded” based on how well it is fit to the data and how complicated it is (more below).</p></li>
<li><p>Finally, the models are ranked and you can choose the best-specified model.</p></li>
</ul>
</section>
<section id="information-criterion" class="level3">
<h3 class="anchored" data-anchor-id="information-criterion">Information criterion</h3>
<p>Each model in the candidate model set is graded based on an estimate of the model’s “cost” vs.&nbsp;“benefit”. The model’s cost is how many parameters the model has where you have preference for a simpler model (less parameters) <a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>, and the benefit is how well the model fits your data. The benefit estimate relates to the likelihood measure that was used to fit your model and estimate your coefficients.</p>
<p>The cost-benefit information is combined to give each model a grade through a metric called “Information Criterion”. For example, Akaike Information Criterion<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> is estimated as:</p>
<p><span class="math inline">\(AIC = 2\cdot k - 2 \cdot ln(L)\)</span></p>
<p>where <span class="math inline">\(k\)</span> is the cost of the model (number of parameters), and <span class="math inline">\(L\)</span> is the maximum likelihood estimate made when the model was fit.<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></p>
<p>In all cases, the model with the lowest AIC is our “best-specified” model, though we will discuss what should be done if you have two or more models that are equally “good” (i.e.&nbsp;within 2 AIC of the lowest AIC). Note that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.</p>
</section>
<section id="choosing-you-best-specified-model" class="level3">
<h3 class="anchored" data-anchor-id="choosing-you-best-specified-model">Choosing you best-specified model</h3>
<!--- vs. backward selection, etc --->
<p>For example, if your starting model is</p>
<p><span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span></p>
<p>your candidate model set is:</p>
<p>1: <span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span></p>
<p>2: <span class="math inline">\(Resp \sim Cov1 + Cov2 + 1\)</span></p>
<p>3: <span class="math inline">\(Resp \sim Cov1 + 1\)</span></p>
<p>4: <span class="math inline">\(Resp \sim Cov2 + 1\)</span></p>
<p>5: <span class="math inline">\(Resp \sim 1\)</span>.</p>
<p>You will fit each of these models and estimate their AIC, for example:</p>
<p>1: <span class="math inline">\(Resp \sim Cov1 + Cov2 + Cov1:Cov2 + 1\)</span> (AIC = 60)</p>
<p>2: <span class="math inline">\(Resp \sim Cov1 + Cov2 + 1\)</span> (AIC = 78)</p>
<p>3: <span class="math inline">\(Resp \sim Cov1 + 1\)</span> (AIC = 20)</p>
<p>4: <span class="math inline">\(Resp \sim Cov2 + 1\)</span> (AIC = 140)</p>
<p>5: <span class="math inline">\(Resp \sim 1\)</span> (AIC = 234).</p>
<p>You then use this information to pick your best-specified model as the model with the lowest AIC<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>. Your best-specified model tells you how to interpret the evidence for your hypothesis. With the example above, you would conclude that the 3rd model (<span class="math inline">\(Resp \sim Cov1 + 1\)</span>) is your best-specified model (it has by far the lowest AIC), indicating that <span class="math inline">\(Cov1\)</span> explains variablity in <span class="math inline">\(Resp\)</span>, but that there is no evidence that <span class="math inline">\(Cov2\)</span> explains variability in <span class="math inline">\(Resp\)</span> (as <span class="math inline">\(Cov2\)</span> doesn’t appear in your best-specified model).</p>
<p>Model selection like this can be easily done using the <code>dredge()</code> function in the MuMIn package. We’ll practice this in class.</p>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>inference is the conclusion you make based on reasoning and evidence<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>vs.&nbsp;categorical<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>if you’re confused why we are choosing these assumptions, read the course notes section on Starting Model<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>these estimates are called parameters<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>note that some write this as “<em>P</em> value” and some as “<em>p</em> value” and some as “<em>p</em>-value”. There is no one rule. Just pick one and make it consistent through your text. I’ll try to do that here.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>field<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>see more in your notes on Data curation and collection<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>See your notes on Model Validation for more on predictor collinearity<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>because it is generally applicable<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>i.e.&nbsp;the effect of <code>Prey</code> on <code>WtChange</code><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>for example, a P value of 0.006 means that there is a 0.6% chance we would estimate the effect of <code>Prey</code> to be 0.12 <span class="math inline">\(g \cdot m^{3}\cdot num{-1}\)</span> when it was in fact 0<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>we’ll come back to this in the Reporting section<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>See your notes on Model Validation for more on predictor collinearity<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>these are also called “nested” models as each model is “nested” in one of the other models when it only differs by one predictor. “Nested” is also used in experimental design to mean something totally different, so we will avoid using the term here.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>see “The Principle of Parsimony” below<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>see the section on Starting Model<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>note the spelling and capitalization of this package name!<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>indeed, your null model <em>only</em> contains an intercept<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>and it is the same number given in the <code>summary()</code> output above. More on this coming up in the Reporting section!<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>Note that lower is always better with information criterion, though the magnitude of the AIC value will change from case to case.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>recall that the <code>+ 1</code> is to indicate that there is an intercept in our model. The <code>+1</code> can be left out of the model formula and R will still estimate an intercept, but I will write it here for clarity<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>see section 9.3 and the “Principle of Parsimony”<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>Section 9.17<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>There are other types of information criteria such as Bayesian Information Criteria (BIC, where the cost is penalized harsher, favouring a simpler model), and the corrected Akaike Information Criterion (AICc, where the metric is optimized for small sample sizes).<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>models within 2 of the lowest AIC are all chosen as the best-specified model<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
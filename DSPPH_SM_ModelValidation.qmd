---
title: "Statistical Modelling: Validation"
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```


```{r include=FALSE}

install.packages("png")
install.packages("grid")
install.packages("formatR")
install.packages("ggplot2")

```


```{r echo=FALSE}
library(png)
library(grid)
library(formatR)
library(ggplot2)
```

::: {.callout-note collapse="false"} 
## In this section you will learn:

- why model validation is necessary

- how to determine if your starting model can be used to test your hypothesis

- what to do if your model is misspecified

:::


# Validating your starting model

## Why do you need to validate your starting model?
As we discussed before "All models are wrong, but some are useful".  So how do you tell if your [starting model](DSPPH_SM_StartingModel.qmd) is a useful one; that is, one that can be used to test your hypothesis? 

As discussed in the last section, when you choose your starting model you make an educated guess as to what a useful starting model might be, but you can only validate that it *is* a useful starting model after you have fit the model to your data.

A useful model is one that reflects the mechanistic understanding of your research hypothesis (the deterministic part of your model - your shape assumption) as well as the nature of your observations (the stochastic part of your model - your error distribution assumption)^[see course notes from last week if this is unclear].  

In addition, a useful model is one that conforms to the assumptions of the method you use to test your model (e.g. GLM). The assumptions of the GLM include that your predictors are not correlated with one another, and that your observations are independent of one another.  

In this section, we will explore each of these assumptions by considering if your:

1) predictors are correlated with one another,

2) observations are independent of one another,

3) error distribution assumption was adequate, and

4) shape distribution assumption was adequate.

By considering these four points, you can determine if your model is "well-specified" to test your hypothesis.

In this section, we will go over tools that will help you determine if your model is well-specified and what to do if you find yourself with a misspecified starting model.

::: {.callout-tip collapse="true" title='"A well-specified model"'} 

Note that you are trying to find "**a** well-specified model".  This terminology reflects the fact that more than one model^[but not all models] is likely appropriate to test your hypothesis.  

:::


# The role of residuals in validating your model

For your starting model to be valid, it needs to capture the variation in your response explained by the predictors, and it needs to follow the error distribution assumption on which the math for fitting the model was based.  We can investigate these characteristics by exploring the model residuals.

Recall that a residual can be defined as^[notice I say "can be defined as...".  This is because there are many different definitions of a residual that have been made to deal with different model structures. We are going to use a very useful one - the scaled residuals - that will let you explore residuals for a wide range of models.  More on this below!] the difference between the model fitted value and your response observation:

![](./residPlotReview.png){width=90%}

When your model is a useful one (a valid one for testing your hypothesis), these residuals will:

1) follow the expected error distribution assumption, and

2) be evenly distributed relative to your model fit and predictors.^[including those predictors inside your model, and possible predictors not included in your model. More on this to come!] 

In fact, you can only validate the assumptions you made when picking your starting model after the model is fit because violations show up in the model residuals.  

We will be looking at two plots to help us investigate the model residuals.  

<img src="./exQQPlotData.png" align="right" width="150px"/>

<img src="./exQQPlot.png" align="right" width="150px"/>


The first is a quantile-quantile (QQ) plot.  A reminder from your notes on Data Distributions that a quantile breaks your observations into groups, e.g. the 25% quantile is the value of the variable where 25% of the values are below this value. A quantile-quantile plot compares the position of the quantiles in the expected and observed distribution to see if they come from the same distribution.  If they come from the same distribution, the quantiles will match, the points will lie along a 1:1 (red) line on the QQ plot, and your model is valid (like the example on the right).

<img src="./exResidualsPlotData.png" align="right" width="150px"/>


<img src="./exResidualsPlot.png" align="right" width="150px"/>



The second is a residual plot.  In the residual plot, you will make plots to compare the residuals (on the y-axis) vs. the model fit and each predictor^[including those predictors inside your model, and possible predictors not included in your model. More on this to come!]  (on the x-axis).  This will let you see if there are any patterns in the plot which could indicate a violation of your model assumptions.  If your model is valid, the points will form a uniform cloud of points in your residual plot (like the example on the right).

You have been looking at residuals as simple differences between your model fit and the observation of your response (as pictured above).  The information that you can get from this type of residual will vary with the structure of your model... this can make residual inspection frustrating, as you would have to learn a new method of considering your residuals for each new type of model.

Instead, you will be using a different type of residual to validate your starting model - one that can be interpreted in similar ways whatever your model structure.  It is called a scaled residual.  

## A useful residual - the scaled residual

As mentioned above, inspecting scaled residuals gives us a method of validating our model that is generally applicable - GREAT - but it also is a method that is intuitive: The scaled residual method checks to see if your model is useful (valid) by seeing if it can even produce data that looks like the data you used to fit your model (i.e. your observations).  A model that is well-specified will be able to simulate data that looks like the data used to fix it.

We will estimate and explore scaled residuals using functions in the [DHARMa package](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html).  


::: {.callout-tip collapse="true" title="DHARMa's scaled residuals"} 
<img src="./scaledResiduals.png" align="right" width="400px"/>


The DHARMa package uses a simulation-based approach to estimate scaled residuals.  These scaled residuals are standardized to a uniform distribution regardless of the model structure.  

Here's how it works:

- DHARMa uses your starting model to simulate new response data for each observation (each row in your data frame).  The default is that it simulates 250 new data sets from your model^[this can be adjusted if needed].

- DHARMa uses the simulated values at each observation to calculate the empirical cumulative density function (ECDF) of the simulated values at that observation.

- The scaled residual for observed data point *i* is then defined as the value of the ECDF at the value of the observed data *i* (see figure to the right).

- Estimated this way, if your model is wellspecified, the scaled residuals will always follow a uniform distribution, regardless of your starting model structure.  Put another way: if the observed data were created from the same data-generating process of your starting model, all values of the cumulative distribution should appear with equal probability and the DHARMa residuals will be distributed uniformly.

You can explore more about the [DHARMa package here](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html).  

:::

# A statistical modelling example

```{r echo = FALSE}
n=190
ss<-sample(c(1:1000), 1)
set.seed(36)
Cont1<-sample(c(30:65), n, replace = TRUE)
Cont2 <- 0.3*Cont1 + rnorm(n, 0, 2)
#Cat<-as.factor(c(rep("Control", times=n/2), rep("Treatment", times=n/2)))
Cat<-as.factor(sample(c("Control", "Treatment"), size=n, replace=TRUE))
Other <-as.factor(sample(c("G328", "G128", "G333", "G23", "G428"), n, replace = TRUE))
uResp<-exp(0.0006*Cont1 +  as.numeric(Cat)*Cont1*0.0108+3)
Resp<-rgamma(n, shape=uResp, scale=0.5)
myDat<-data.frame(Cont1=Cont1, Cont2 = Cont2, Cat=Cat, Resp=Resp, Other=Other)


```


It will help our discussions to have an example of a starting model.  For our example, consider the research hypothesis that your response variable `Resp` is explained by a categorical predictor (`Cat`), two continuous predictors (`Cont1` and `Cont2`) as well as the interactions among all predictors.  You communicate this as:   

`Resp ~ Cat + Cont1 + Cont2 + Cat:Cont1 + Cat:Cont2 + Cont1:Cont2 + Cat:Cont1:Cont2 + 1`

In this example, `Resp` is a positive, continuous variable.

Let us assume you tested this hypothesis by fitting a model with a Gamma error distribution assumption (to reflect the nature of your response variable) and linear shape assumption (to reflect the relationship between each predictor and your response) using a GLM:

```{r}

str(myDat) # a look at our data

startMod<-glm(formula = Resp ~ Cat + Cont1 + Cont2 + Cat:Cont1 + Cat:Cont2 + Cont1:Cont2 + Cat:Cont1:Cont2 + 1, # hypothesis
               data = myDat, # data
               family = Gamma(link="inverse")) # error distribution assumption

summary(startMod) # a look into our model object

```

Let us start by estimating the residuals from this starting model using the scaled residual method mentioned above.  


```{r include=FALSE}

install.packages("DHARMa")

```

```{r}

library(DHARMa) # load package

simOut <- simulateResiduals(fittedModel = startMod, n = 250) # simulate data from our model n times and calculate residuals

myDat$Resid <- simOut$scaledResiduals # add residuals to data frame

str(myDat) # check the structure of the data
```
The residuals have been added to the data frame as `myDat$Resid`.  We will take a look at these residuals as we walk through the model validation below.


# Considering predictor collinearity

**What it is:**
  
  NOTE: predictor collinearity^[sometimes also called multicollinearity] is only a problem if you have multiple predictors in your hypothesis.

Your model assumes that your predictors are not correlated with one another, and that each predictor explains a unique part of the variation in your response.  When this assumption is violated, the estimates of the coefficients^[remember, these are the modelled effects of your predictors on your response] become more uncertain and you are unable to test your hypothesis in a robust way. 

Let us use an extreme case to see why this might be a problem:  
  
  >Imagine you are interested in explaining variability in plant growth (your response is plant growth).  You think temperature and water may influence plant growth, so you set up an experiment where you will grow plants under high and low temperature and high and low water (your predictors are temperature and water treatments).  But in your experimental set-up you only include two treatments: one where the plants are grown with high temperature and high water, and another where plants are grown with low temperature and low water.  In your experiment, your predictors are correlated with one another^[your predictors are so correlated, in fact, we say they are  "aliased" with one another].  When you try to test your hypothesis, you will not be able to say if the growth differences between treatments is due to differences in temperature or water. 

This is an extreme case (where your predictors are 100% correlated with one another), and one that can easily be avoided with a better experimental design.  But, predictors that are at least partially correlated with one another are very common in observational/field studies (e.g. things like salinity, temperature and depth in marine studies).
<br clear="right"/>
  
  Predictor collinearity causes a problem with our interpretation of our model because it increases the error around the coefficients (modelled effects).  This error can be so large, that we are unable to assess the effect of a predictor in explaining the response.  This increase in error due to predictor collinearity is called "variance inflation"^[the error (variance) around the coefficient estimates is getting bigger (inflation)].

**How you know if you have a problem:**
  
  You may have a hunch that you will have a problem with predictor collinearity before you even fit a model.  You can identify potential problems by looking at correlations of your predictors with one another.   

Here is a helpful bit of code for your investigations, applied to our example model:
  

```{r include=FALSE}

install.packages("GGally")

```

```{r}
library(GGally) # loading GGally package

ggpairs(data=myDat) # make a plot of your data - each column vs. another - often called a "pairs plot"

```







